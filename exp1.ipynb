{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import app, get_arxiv_paper_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://arxiv.org/pdf/2310.04406\"\n",
    "response = await app.ainvoke({\"paper_url\": url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = {'citations': [{'authors': ['T. Silver',\n",
    "    'A. Huang',\n",
    "    'C. J. Maddison',\n",
    "    'A. Guez',\n",
    "    'L. Sifre',\n",
    "    'G. van den Driessche',\n",
    "    'J. Schrittwieser',\n",
    "    'I. Antonoglou',\n",
    "    'V. Panneershelvam',\n",
    "    'M. Lanctot',\n",
    "    'S. Dieleman',\n",
    "    'D. Grewe',\n",
    "    'J. Nham',\n",
    "    'N. Kalchbrenner',\n",
    "    'I. Sutskever',\n",
    "    'T. P. Lillicrap',\n",
    "    'M. Leach',\n",
    "    'K. Kavukcuoglu',\n",
    "    'T. Graepel',\n",
    "    'D. Hassabis'],\n",
    "   'title': 'Mastering the game of Go with deep neural networks and tree search',\n",
    "   'year': 2016},\n",
    "  {'authors': ['D. Silver',\n",
    "    'A. Huang',\n",
    "    'C. J. Maddison',\n",
    "    'A. Guez',\n",
    "    'L. Sifre',\n",
    "    'G. van den Driessche',\n",
    "    'J. Schrittwieser',\n",
    "    'I. Antonoglou',\n",
    "    'V. Panneershelvam',\n",
    "    'M. Lanctot',\n",
    "    'S. Dieleman',\n",
    "    'D. Grewe',\n",
    "    'J. Nham',\n",
    "    'N. Kalchbrenner',\n",
    "    'I. Sutskever',\n",
    "    'T. P. Lillicrap',\n",
    "    'M. Leach',\n",
    "    'K. Kavukcuoglu',\n",
    "    'T. Graepel',\n",
    "    'D. Hassabis'],\n",
    "   'title': 'Mastering chess and Shogi by self-play with a general reinforcement learning algorithm',\n",
    "   'year': 2017},\n",
    "  {'authors': ['M. Campbell', 'A. J. Hoane Jr', 'F. Hsu'],\n",
    "   'title': 'Deep blue',\n",
    "   'year': 2002},\n",
    "  {'authors': ['B. Chen',\n",
    "    'F. Zhang',\n",
    "    'A. Nguyen',\n",
    "    'D. Zan',\n",
    "    'Z. Lin',\n",
    "    'J. Lou',\n",
    "    'W. Chen'],\n",
    "   'title': 'CodeT: Code generation with generated tests',\n",
    "   'year': 2023},\n",
    "  {'authors': ['M. Chen',\n",
    "    'J. Tworek',\n",
    "    'H. Jun',\n",
    "    'Q. Yuan',\n",
    "    'H. Ponde',\n",
    "    'J. Kaplan',\n",
    "    'H. Edwards',\n",
    "    'Y. Burda',\n",
    "    'N. Joseph',\n",
    "    'G. Brockman',\n",
    "    'A. Ray',\n",
    "    'R. Puri',\n",
    "    'G. Krueger',\n",
    "    'M. Petrov',\n",
    "    'H. Khlaaf',\n",
    "    'G. Sastry',\n",
    "    'P. Mishkin',\n",
    "    'B. Chan',\n",
    "    'S. Gray',\n",
    "    'N. Ryder',\n",
    "    'M. Bavarian',\n",
    "    'C. Berner',\n",
    "    'S. McCandlish',\n",
    "    'A. Radford',\n",
    "    'I. Sutskever',\n",
    "    'W. Zaremba'],\n",
    "   'title': 'Evaluating large language models trained on code',\n",
    "   'year': 2021},\n",
    "  {'authors': ['W. Chen', 'X. Ma', 'X. Wang', 'W. W. Cohen'],\n",
    "   'title': 'Program of thoughts prompting: disentangling computation from reasoning for numerical reasoning tasks',\n",
    "   'year': 2023},\n",
    "  {'authors': ['A. Chowdhery',\n",
    "    'S. Narang',\n",
    "    'J. Devlin',\n",
    "    'M. Bosma',\n",
    "    'G. Mishra',\n",
    "    'A. Roberts',\n",
    "    'P. Barham',\n",
    "    'H. W. Chung',\n",
    "    'C. Sutton',\n",
    "    'S. Gehrmann',\n",
    "    'P. Schuh',\n",
    "    'K. Shi',\n",
    "    'S. Tsvyashchenko',\n",
    "    'J. Maynez',\n",
    "    'A. Rao',\n",
    "    'P. Barnes',\n",
    "    'Y. Tay',\n",
    "    'N. Shazeer',\n",
    "    'V. Prabhakaran',\n",
    "    'N. Du',\n",
    "    'B. Hutchinson',\n",
    "    'R. Pope',\n",
    "    'J. Bradbury',\n",
    "    'J. Austin',\n",
    "    'M. Isard',\n",
    "    'G. Gur-Ari',\n",
    "    'P. Yin',\n",
    "    'T. Duke',\n",
    "    'A. Lewkowycz',\n",
    "    'S. Zhang',\n",
    "    'Y. Bengio',\n",
    "    'W. W. Cohen',\n",
    "    'R. Salakhutdinov',\n",
    "    'C. D. Manning'],\n",
    "   'title': 'PaLM: Scaling language modeling with pathways',\n",
    "   'year': 2023},\n",
    "  {'authors': ['K. Cobbe',\n",
    "    'V. Kosaraju',\n",
    "    'M. Bavarian',\n",
    "    'M. Chen',\n",
    "    'H. Jun',\n",
    "    'L. Kaiser',\n",
    "    'M. Plappert',\n",
    "    'J. Tworek',\n",
    "    'J. Hilton',\n",
    "    'R. Nakano',\n",
    "    'C. Hesse',\n",
    "    'J. Schulman'],\n",
    "   'title': 'Training verifiers to solve math word problems',\n",
    "   'year': 2021},\n",
    "  {'authors': ['X. Deng',\n",
    "    'Y. Gu',\n",
    "    'B. Zheng',\n",
    "    'S. Chen',\n",
    "    'S. Stevens',\n",
    "    'B. Wang',\n",
    "    'H. Sun',\n",
    "    'Y. Su'],\n",
    "   'title': 'Mind2Web: Towards a generalist agent for the web',\n",
    "   'year': 2023},\n",
    "  {'authors': ['D. Driess',\n",
    "    'F. Xia',\n",
    "    'M. S. M. Sajjadi',\n",
    "    'C. Lynch',\n",
    "    'A. Chowdhery',\n",
    "    'B. Ichter',\n",
    "    'A. Wahid',\n",
    "    'J. Tompson',\n",
    "    'Q. Vuong',\n",
    "    'T. Yu',\n",
    "    'W. Huang',\n",
    "    'Y. Chebotar',\n",
    "    'P. Sermanet',\n",
    "    'D. Duckworth',\n",
    "    'S. Levine',\n",
    "    'V. Vanhoucke',\n",
    "    'K. Hausman',\n",
    "    'M. Toussaint',\n",
    "    'K. Greff',\n",
    "    'A. Zeng',\n",
    "    'I. Mordatch',\n",
    "    'P. Florence'],\n",
    "   'title': 'PaLM-E: An embodied multimodal language model',\n",
    "   'year': 2023},\n",
    "  {'authors': ['Y. Du',\n",
    "    'M. Yang',\n",
    "    'B. Dai',\n",
    "    'H. Dai',\n",
    "    'O. Nachum',\n",
    "    'J. B. Tenenbaum',\n",
    "    'D. Schuurmans',\n",
    "    'P. Abbeel'],\n",
    "   'title': 'Learning universal policies via text-guided video generation',\n",
    "   'year': 2023},\n",
    "  {'authors': ['J. S. B. T. Evans'],\n",
    "   'title': 'Intuition and reasoning: A dual-process perspective',\n",
    "   'year': 2010},\n",
    "  {'authors': ['L. Fan',\n",
    "    'G. Wang',\n",
    "    'Y. Jiang',\n",
    "    'A. Mandlekar',\n",
    "    'Y. Zhu',\n",
    "    'H. Zhang',\n",
    "    'D. Huang',\n",
    "    'A. Anandkumar'],\n",
    "   'title': 'MineDojo: Building open-ended embodied agents with internet-scale knowledge',\n",
    "   'year': 2022},\n",
    "  {'authors': ['H. Furuta',\n",
    "    'O. Nachum',\n",
    "    'K. Lee',\n",
    "    'Y. Matsuo',\n",
    "    'S. S. Gu',\n",
    "    'I. Gur'],\n",
    "   'title': 'Multimodal web navigation with instruction-finetuned foundation models',\n",
    "   'year': 2024},\n",
    "  {'authors': ['L. Gao',\n",
    "    'A. Madaan',\n",
    "    'S. Zhou',\n",
    "    'U. Alon',\n",
    "    'P. Liu',\n",
    "    'Y. Yang',\n",
    "    'J. Callan',\n",
    "    'G. Neubig'],\n",
    "   'title': 'PAL: Program-aided language models',\n",
    "   'year': 2023},\n",
    "  {'authors': ['J. Guo', 'S. Lu', 'H. Cai', 'W. Zhang', 'Y. Yu', 'J. Wang'],\n",
    "   'title': 'Long text generation via adversarial training with leaked information',\n",
    "   'year': 2018},\n",
    "  {'authors': ['W. H. Guss',\n",
    "    'B. Houghton',\n",
    "    'N. Topin',\n",
    "    'P. Wang',\n",
    "    'C. Codel',\n",
    "    'M. Veloso',\n",
    "    'R. Salakhutdinov'],\n",
    "   'title': 'MineRL: A large-scale dataset of Minecraft demonstrations',\n",
    "   'year': 2019},\n",
    "  {'authors': ['D. Hafner',\n",
    "    'T. Lillicrap',\n",
    "    'I. Fischer',\n",
    "    'R. Villegas',\n",
    "    'D. Ha',\n",
    "    'H. Lee',\n",
    "    'J. Davidson'],\n",
    "   'title': 'Learning latent dynamics for planning from pixels',\n",
    "   'year': 2019},\n",
    "  {'authors': ['D. Hafner', 'J. Pasukonis', 'J. Ba', 'T. Lillicrap'],\n",
    "   'title': 'Mastering diverse domains through world models',\n",
    "   'year': 2023},\n",
    "  {'authors': ['S. Hao',\n",
    "    'Y. Gu',\n",
    "    'H. Ma',\n",
    "    'J. J. Hong',\n",
    "    'Z. Wang',\n",
    "    'D. Z. Wang',\n",
    "    'Z. Hu'],\n",
    "   'title': 'Reasoning with language model is planning with world model',\n",
    "   'year': 2023},\n",
    "  {'authors': ['J. Huang',\n",
    "    'X. Chen',\n",
    "    'S. Mishra',\n",
    "    'H. S. Zheng',\n",
    "    'A. W. Yu',\n",
    "    'X. Song',\n",
    "    'D. Zhou'],\n",
    "   'title': 'Large language models cannot self-correct reasoning yet',\n",
    "   'year': 2024},\n",
    "  {'authors': ['W. Huang',\n",
    "    'F. Xia',\n",
    "    'T. Xiao',\n",
    "    'H. Chan',\n",
    "    'J. Liang',\n",
    "    'P. R. Florence',\n",
    "    'A. Zeng',\n",
    "    'J. Tompson',\n",
    "    'I. Mordatch',\n",
    "    'Y. Chebotar',\n",
    "    'P. Sermanet',\n",
    "    'N. Brown',\n",
    "    'T. Jackson',\n",
    "    'L. Luu',\n",
    "    'S. Levine',\n",
    "    'K. Hausman',\n",
    "    'M. Toussaint',\n",
    "    'K. Greff',\n",
    "    'A. Zeng',\n",
    "    'I. Mordatch',\n",
    "    'P. Florence'],\n",
    "   'title': 'Inner monologue: Embodied reasoning through planning with language models',\n",
    "   'year': 2022},\n",
    "  {'authors': ['L. Kocsis', 'C. Szepesvari'],\n",
    "   'title': 'Bandit based monte-carlo planning',\n",
    "   'year': 2006},\n",
    "  {'authors': ['S. M. LaValle'],\n",
    "   'title': 'Rapidly-exploring random trees: A new tool for path planning',\n",
    "   'year': 1998},\n",
    "  {'authors': ['E. Z. Liu', 'K. Guu', 'P. Pasupat', 'T. Shi', 'P. Liang'],\n",
    "   'title': 'Reinforcement learning on web interfaces using workflow-guided exploration',\n",
    "   'year': 2018},\n",
    "  {'authors': ['X. Liu',\n",
    "    'H. Yu',\n",
    "    'H. Zhang',\n",
    "    'Y. Xu',\n",
    "    'X. Lei',\n",
    "    'H. Lai',\n",
    "    'Y. Gu',\n",
    "    'H. Ding',\n",
    "    'K. Men',\n",
    "    'K. Yang',\n",
    "    'S. Zhang',\n",
    "    'X. Deng',\n",
    "    'A. Zeng',\n",
    "    'Z. Du',\n",
    "    'C. Zhang',\n",
    "    'S. Shen',\n",
    "    'T. Zhang',\n",
    "    'Y. Su',\n",
    "    'H. Sun',\n",
    "    'M. Huang',\n",
    "    'Y. Dong',\n",
    "    'J. Tang'],\n",
    "   'title': 'AgentBench: Evaluating LLMs as agents',\n",
    "   'year': 2024},\n",
    "  {'authors': ['Z. Liu',\n",
    "    'H. Hu',\n",
    "    'S. Zhang',\n",
    "    'H. Guo',\n",
    "    'S. Ke',\n",
    "    'B. Liu',\n",
    "    'Z. Wang'],\n",
    "   'title': 'Reason for future, act for now: A principled framework for autonomous LLM agents with provable sample efficiency',\n",
    "   'year': 2023},\n",
    "  {'authors': ['A. Madaan',\n",
    "    'N. Tandon',\n",
    "    'P. Gupta',\n",
    "    'S. Hallinan',\n",
    "    'L. Gao',\n",
    "    'S. Wiegreffe',\n",
    "    'U. Alon',\n",
    "    'N. Dziri',\n",
    "    'S. Prabhumoye',\n",
    "    'Y. Yang',\n",
    "    'S. Gupta',\n",
    "    'B. P. Majumder',\n",
    "    'K. Hermann',\n",
    "    'S. Welleck',\n",
    "    'A. Yazdanbakhsh',\n",
    "    'P. Clark'],\n",
    "   'title': 'Self-refine: Iterative refinement with self-feedback',\n",
    "   'year': 2023},\n",
    "  {'authors': ['R. Nallapati',\n",
    "    'B. Zhou',\n",
    "    'C. dos Santos',\n",
    "    'C. Gulcehre',\n",
    "    'B. Xiang'],\n",
    "   'title': 'Abstractive text summarization using sequence-to-sequence RNNs and beyond',\n",
    "   'year': 2016},\n",
    "  {'authors': ['OpenAI'], 'title': 'GPT-4 technical report', 'year': 2023},\n",
    "  {'authors': ['Y. Qin',\n",
    "    'S. Liang',\n",
    "    'Y. Ye',\n",
    "    'K. Zhu',\n",
    "    'L. Yan',\n",
    "    'Y. Lu',\n",
    "    'Y. Lin',\n",
    "    'X. Cong',\n",
    "    'X. Tang',\n",
    "    'B. Qian',\n",
    "    'S. Zhao',\n",
    "    'R. Tian',\n",
    "    'R. Xie',\n",
    "    'J. Zhou',\n",
    "    'M. Gerstein',\n",
    "    'D. Li',\n",
    "    'Z. Liu',\n",
    "    'M. Sun'],\n",
    "   'title': 'ToolLLM: Facilitating large language models to master 16000+ real-world APIs',\n",
    "   'year': 2024},\n",
    "  {'authors': ['A. Saparov', 'H. He'],\n",
    "   'title': 'Language models are greedy reasoners: A systematic formal analysis of chain-of-thought',\n",
    "   'year': 2023},\n",
    "  {'authors': ['T. Schick',\n",
    "    'J. Dwivedi-Yu',\n",
    "    'R. Dess`ı',\n",
    "    'R. Raileanu',\n",
    "    'M. Lomeli',\n",
    "    'L. Zettlemoyer',\n",
    "    'N. Cancedda',\n",
    "    'T. Scialom'],\n",
    "   'title': 'Toolformer: Language models can teach themselves to use tools',\n",
    "   'year': 2023},\n",
    "  {'authors': ['Y. Shen', 'K. Song', 'X. Tan', 'D. Li', 'W. Lu', 'Y. Zhuang'],\n",
    "   'title': 'HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face',\n",
    "   'year': 2023},\n",
    "  {'authors': ['N. Shinn',\n",
    "    'F. Cassano',\n",
    "    'B. Labash',\n",
    "    'A. Gopinath',\n",
    "    'K. Narasimhan',\n",
    "    'S. Yao'],\n",
    "   'title': 'Reflexion: Language agents with verbal reinforcement learning',\n",
    "   'year': 2023},\n",
    "  {'authors': ['M. Shridhar',\n",
    "    'X. Yuan',\n",
    "    'M. Cotˆ e',\n",
    "    'Y. Bisk',\n",
    "    'A. Trischler',\n",
    "    'M. Hausknecht'],\n",
    "   'title': 'ALFWorld: Aligning text and embodied environments for interactive learning',\n",
    "   'year': 2020},\n",
    "  {'authors': ['D. Silver',\n",
    "    'A. Huang',\n",
    "    'C. J. Maddison',\n",
    "    'A. Guez',\n",
    "    'L. Sifre',\n",
    "    'G. van den Driessche',\n",
    "    'J. Schrittwieser',\n",
    "    'I. Antonoglou',\n",
    "    'V. Panneershelvam',\n",
    "    'M. Lanctot',\n",
    "    'S. Dieleman',\n",
    "    'D. Grewe',\n",
    "    'J. Nham',\n",
    "    'N. Kalchbrenner',\n",
    "    'I. Sutskever',\n",
    "    'T. P. Lillicrap',\n",
    "    'M. Leach',\n",
    "    'K. Kavukcuoglu',\n",
    "    'T. Graepel',\n",
    "    'D. Hassabis'],\n",
    "   'title': 'Mastering the game of Go with deep neural networks and tree search',\n",
    "   'year': 2016},\n",
    "  {'authors': ['D. Silver',\n",
    "    'A. Huang',\n",
    "    'C. J. Maddison',\n",
    "    'A. Guez',\n",
    "    'L. Sifre',\n",
    "    'G. van den Driessche',\n",
    "    'J. Schrittwieser',\n",
    "    'I. Antonoglou',\n",
    "    'V. Panneershelvam',\n",
    "    'M. Lanctot',\n",
    "    'S. Dieleman',\n",
    "    'D. Grewe',\n",
    "    'J. Nham',\n",
    "    'N. Kalchbrenner',\n",
    "    'I. Sutskever',\n",
    "    'T. P. Lillicrap',\n",
    "    'M. Leach',\n",
    "    'K. Kavukcuoglu',\n",
    "    'T. Graepel',\n",
    "    'D. Hassabis'],\n",
    "   'title': 'Mastering chess and Shogi by self-play with a general reinforcement learning algorithm',\n",
    "   'year': 2017},\n",
    "  {'authors': ['S. A. Sloman'],\n",
    "   'title': 'The empirical case for two systems of reasoning',\n",
    "   'year': 1996},\n",
    "  {'authors': ['H. Sun', 'Y. Zhuang', 'L. Kong', 'B. Dai', 'C. Zhang'],\n",
    "   'title': 'AdaPlanner: Adaptive planning from feedback with language models',\n",
    "   'year': 2023},\n",
    "  {'authors': ['D. Sur`ıs', 'S. Menon', 'C. Vondrick'],\n",
    "   'title': 'ViperGPT: Visual inference via Python execution for reasoning',\n",
    "   'year': 2023},\n",
    "  {'authors': ['M. Swiechowski', 'K. Godlewski', 'B. Sawicki', 'J. Ma’ndziuk'],\n",
    "   'title': 'Monte Carlo tree search: A review of recent modifications and applications',\n",
    "   'year': 2021},\n",
    "  {'authors': ['H. Touvron',\n",
    "    'L. Martin',\n",
    "    'K. R. Stone',\n",
    "    'P. Albert',\n",
    "    'A. Almahairi',\n",
    "    'Y. Babaei',\n",
    "    'N. Bashlykov',\n",
    "    'S. Batra',\n",
    "    'P. Bhargava',\n",
    "    'P. Bosma',\n",
    "    'G. Chen',\n",
    "    'C. Canton-Ferrer',\n",
    "    'M. Chen',\n",
    "    'G. Cucurull',\n",
    "    'D. Esiobu',\n",
    "    'J. Fernandes',\n",
    "    'J. Fu',\n",
    "    'W. Fu',\n",
    "    'B. Fuller',\n",
    "    'C. Gao',\n",
    "    'V. Goswami',\n",
    "    'N. Goyal',\n",
    "    'A. S. Hartshorn',\n",
    "    'S. Hosseini',\n",
    "    'R. Hou',\n",
    "    'H. Inan',\n",
    "    'M. Kardas',\n",
    "    'V. Kerkez',\n",
    "    'M. Khabsa',\n",
    "    'I. Kloumann',\n",
    "    'A. V. Korenev',\n",
    "    'P. S. Koura',\n",
    "    'M. Lachaux',\n",
    "    'T. Lavril',\n",
    "    'J. Lee',\n",
    "    'D. Liskovich',\n",
    "    'Y. Lu',\n",
    "    'Y. Mao',\n",
    "    'X. Martinet',\n",
    "    'T. Mihaylov',\n",
    "    'P. Mishra',\n",
    "    'I. Molybog',\n",
    "    'Y. Nie',\n",
    "    'A. Poulton',\n",
    "    'J. Reizenstein',\n",
    "    'R. Rungta',\n",
    "    'K. Saladi',\n",
    "    'A. Schelten',\n",
    "    'R. Silva',\n",
    "    'E. M. Smith',\n",
    "    'R. Subramanian',\n",
    "    'X. Tan',\n",
    "    'B. Tang',\n",
    "    'R. Taylor',\n",
    "    'A. Williams',\n",
    "    'J. Xiang',\n",
    "    'P. Xu',\n",
    "    'Z. Yan',\n",
    "    'I. Zarov',\n",
    "    'Y. Zhang',\n",
    "    'A. Fan',\n",
    "    'M. Kambadur',\n",
    "    'S. Narang',\n",
    "    'A. Rodriguez',\n",
    "    'R. Stojnic',\n",
    "    'S. Edunov',\n",
    "    'T. Scialom'],\n",
    "   'title': 'Llama 2: Open foundation and fine-tuned chat models',\n",
    "   'year': 2023},\n",
    "  {'authors': ['T. Vodopivec', 'S. Samothrakis', 'B. Ster'],\n",
    "   'title': 'On Monte Carlo tree search and reinforcement learning',\n",
    "   'year': 2017},\n",
    "  {'authors': ['G. Wang',\n",
    "    'Y. Xie',\n",
    "    'Y. Jiang',\n",
    "    'A. Mandlekar',\n",
    "    'C. Xiao',\n",
    "    'Y. Zhu',\n",
    "    'L. Fan',\n",
    "    'A. Anandkumar'],\n",
    "   'title': 'Voyager: An open-ended embodied agent with large language models',\n",
    "   'year': 2023},\n",
    "  {'authors': ['X. Wang',\n",
    "    'J. Wei',\n",
    "    'D. Schuurmans',\n",
    "    'Q. Le',\n",
    "    'E. Chi',\n",
    "    'D. Zhou'],\n",
    "   'title': 'Self-consistency improves chain of thought reasoning in language models',\n",
    "   'year': 2022},\n",
    "  {'authors': ['J. Wei',\n",
    "    'X. Wang',\n",
    "    'D. Schuurmans',\n",
    "    'M. Bosma',\n",
    "    'E. Chi',\n",
    "    'Q. Le',\n",
    "    'D. Zhou'],\n",
    "   'title': 'Chain of thought prompting elicits reasoning in large language models',\n",
    "   'year': 2022},\n",
    "  {'authors': ['M. Wooldridge', 'N. R. Jennings'],\n",
    "   'title': 'Intelligent agents: Theory and practice',\n",
    "   'year': 1995},\n",
    "  {'authors': ['P. Wu',\n",
    "    'A. Escontrela',\n",
    "    'D. Hafner',\n",
    "    'P. Abbeel',\n",
    "    'K. Goldberg'],\n",
    "   'title': 'Daydreamer: World models for physical robot learning',\n",
    "   'year': 2023},\n",
    "  {'authors': ['Y. Xie',\n",
    "    'K. Kawaguchi',\n",
    "    'Y. Zhao',\n",
    "    'X. Zhao',\n",
    "    'M. Kan',\n",
    "    'J. He',\n",
    "    'Q. Xie'],\n",
    "   'title': 'Decomposition enhances reasoning via self-evaluation guided decoding',\n",
    "   'year': 2023},\n",
    "  {'authors': ['Z. Yang',\n",
    "    'P. Qi',\n",
    "    'S. Zhang',\n",
    "    'Y. Bengio',\n",
    "    'W. W. Cohen',\n",
    "    'R. Salakhutdinov',\n",
    "    'C. D. Manning'],\n",
    "   'title': 'HotpotQA: A dataset for diverse, explainable multi-hop question answering',\n",
    "   'year': 2018},\n",
    "  {'authors': ['S. Yao', 'H. Chen', 'J. Yang', 'K. R. Narasimhan'],\n",
    "   'title': 'WebShop: Towards scalable real-world web interaction with grounded language agents',\n",
    "   'year': 2022},\n",
    "  {'authors': ['S. Yao',\n",
    "    'D. Yu',\n",
    "    'J. Zhao',\n",
    "    'I. Shafran',\n",
    "    'T. L. Griffiths',\n",
    "    'Y. Cao',\n",
    "    'K. Narasimhan'],\n",
    "   'title': 'Tree of thoughts: Deliberate problem solving with large language models',\n",
    "   'year': 2023},\n",
    "  {'authors': ['S. Yao',\n",
    "    'J. Zhao',\n",
    "    'D. Yu',\n",
    "    'N. Du',\n",
    "    'I. Shafran',\n",
    "    'K. Narasimhan',\n",
    "    'Y. Cao'],\n",
    "   'title': 'ReAct: Synergizing reasoning and acting in language models',\n",
    "   'year': 2023},\n",
    "  {'authors': ['W. Ye', 'S. Liu', 'T. Kurutach', 'P. Abbeel', 'Y. Gao'],\n",
    "   'title': 'Mastering Atari games with limited data',\n",
    "   'year': 2021},\n",
    "  {'authors': ['D. Zhou',\n",
    "    'N. Scharli',\n",
    "    'L. Hou',\n",
    "    'J. Wei',\n",
    "    'N. Scales',\n",
    "    'X. Wang',\n",
    "    'D. Schuurmans',\n",
    "    'O. Bousquet',\n",
    "    'Q. Le',\n",
    "    'E. Chi'],\n",
    "   'title': 'Least-to-most prompting enables complex reasoning in large language models',\n",
    "   'year': 2022},\n",
    "  {'authors': ['Y. Zhuang',\n",
    "    'X. Chen',\n",
    "    'T. Yu',\n",
    "    'S. Mitra',\n",
    "    'V. Bursztyn',\n",
    "    'R. A. Rossi',\n",
    "    'S. Sarkhel',\n",
    "    'C. Zhang'],\n",
    "   'title': 'ToolChain*: Efficient action space navigation in large language models with A* search',\n",
    "   'year': 2023}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_arxiv_paper_details(title, authors=None, year=None):\n",
    "#     base_url = \"http://export.arxiv.org/api/query\"\n",
    "\n",
    "#     # Construct the query\n",
    "#     query_parts = [f'ti:\"{quote(title)}\"']\n",
    "#     if authors:\n",
    "#         author_query = \" AND \".join(f'au:\"{quote(author)}\"' for author in authors)\n",
    "#         query_parts.append(f\"({author_query})\")\n",
    "#     if year:\n",
    "#         query_parts.append(f\"submittedDate:[{year}0101 TO {year}1231]\")\n",
    "\n",
    "#     query = \" AND \".join(query_parts)\n",
    "\n",
    "#     response = requests.get(f\"{base_url}?search_query={query}\")\n",
    "\n",
    "#     if response.status_code == 200:\n",
    "#         root = ElementTree.fromstring(response.content)\n",
    "#         for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
    "#             paper_title = entry.find(\"{http://www.w3.org/2005/Atom}title\").text\n",
    "#             authors = [\n",
    "#                 author.find(\"{http://www.w3.org/2005/Atom}name\").text\n",
    "#                 for author in entry.findall(\"{http://www.w3.org/2005/Atom}author\")\n",
    "#             ]\n",
    "#             abstract = entry.find(\"{http://www.w3.org/2005/Atom}summary\").text\n",
    "#             published_date = entry.find(\"{http://www.w3.org/2005/Atom}published\").text\n",
    "#             year = published_date.split(\"-\")[0]\n",
    "#             return {\n",
    "#                 \"title\": paper_title,\n",
    "#                 \"authors\": authors,\n",
    "#                 \"abstract\": abstract,\n",
    "#                 \"year\": year,\n",
    "#             }\n",
    "#     else:\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: CodeT: Code Generation with Generated Tests\n",
      "Authors: Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, Weizhu Chen\n",
      "Year: 2022\n",
      "Abstract:   The task of generating code solutions for a given programming problem can\n",
      "benefit from the use of pre-trained language models such as Codex, which can\n",
      "produce multiple diverse samples. However, a major challenge for this task is\n",
      "to select the most appropriate solution from the multiple samples generated by\n",
      "the pre-trained language models. A natural way to evaluate the quality and\n",
      "correctness of a code solution is to run it against a set of test cases, but\n",
      "the manual creation of such test cases is often costly and time-consuming. In\n",
      "this paper, we propose a novel method, CodeT, that leverages the same\n",
      "pre-trained language models to automatically generate test cases for the code\n",
      "samples, thus reducing the human effort and increasing the coverage of the test\n",
      "scenarios. CodeT then executes the code samples using the generated test cases,\n",
      "and performs a dual execution agreement, which considers both the consistency\n",
      "of the outputs against the generated test cases and the agreement of the\n",
      "outputs with other code samples. We conduct comprehensive experiments on four\n",
      "benchmarks, HumanEval, MBPP, APPS and CodeContests, using five different\n",
      "pre-trained language models with varying sizes and capabilities. Our results\n",
      "show that CodeT can significantly improve the performance of code solution\n",
      "selection over previous methods, achieving remarkable and consistent gains\n",
      "across different models and benchmarks. For instance, CodeT improves the pass@1\n",
      "metric on HumanEval to 65.8%, which represents an absolute improvement of 18.8%\n",
      "over the code-davinci-002 model, and an absolute improvement of more than 20%\n",
      "over the previous state-of-the-art results.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# title = \"Do as I can, not as I say: Grounding language in robotic affordances.\"\n",
    "# # authors = [\"Ahn\"]\n",
    "# year = 2022\n",
    "\n",
    "title = 'CodeT: Code generation with generated tests'\n",
    "# authors = ['M. Campbell', 'A. J. Hoane Jr', 'F. Hsu']\n",
    "# year = 2023\n",
    "\n",
    "paper_details = await get_arxiv_paper_details(title)\n",
    "if paper_details:\n",
    "    print(f\"Title: {paper_details['title']}\")\n",
    "    print(f\"Authors: {', '.join(paper_details['authors'])}\")\n",
    "    print(f\"Year: {paper_details['year']}\")\n",
    "    print(f\"Abstract: {paper_details['abstract']}\")\n",
    "else:\n",
    "    print(\"Paper not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper not found: Mastering the game of Go with deep neural networks and tree search\n",
      "Paper not found: Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\n",
      "Paper not found: Learning universal policies via text-guided video generation\n",
      "Paper not found: Intuition and reasoning: A dual-process perspective\n",
      "Paper not found: MineDojo: Building open-ended embodied agents with internet-scale knowledge\n",
      "Paper not found: Multimodal web navigation with instruction-finetuned foundation models\n",
      "Paper not found: PAL: Program-aided language models\n",
      "Paper not found: MineRL: A large-scale dataset of Minecraft demonstrations\n",
      "Paper not found: Large language models cannot self-correct reasoning yet\n",
      "Paper not found: Bandit based monte-carlo planning\n",
      "Paper not found: Rapidly-exploring random trees: A new tool for path planning\n",
      "Paper not found: Reinforcement learning on web interfaces using workflow-guided exploration\n",
      "Paper not found: Self-refine: Iterative refinement with self-feedback\n",
      "Paper not found: Abstractive text summarization using sequence-to-sequence RNNs and beyond\n",
      "Paper not found: ToolLLM: Facilitating large language models to master 16000+ real-world APIs\n",
      "Paper not found: Language models are greedy reasoners: A systematic formal analysis of chain-of-thought\n",
      "Paper not found: Mastering the game of Go with deep neural networks and tree search\n",
      "Paper not found: Mastering chess and Shogi by self-play with a general reinforcement learning algorithm\n",
      "Paper not found: The empirical case for two systems of reasoning\n",
      "Paper not found: Llama 2: Open foundation and fine-tuned chat models\n",
      "Paper not found: On Monte Carlo tree search and reinforcement learning\n",
      "Paper not found: Voyager: An open-ended embodied agent with large language models\n",
      "Paper not found: Self-consistency improves chain of thought reasoning in language models\n",
      "Paper not found: Intelligent agents: Theory and practice\n",
      "Paper not found: Decomposition enhances reasoning via self-evaluation guided decoding\n",
      "Paper not found: HotpotQA: A dataset for diverse, explainable multi-hop question answering\n",
      "Paper not found: WebShop: Towards scalable real-world web interaction with grounded language agents\n",
      "Paper not found: Least-to-most prompting enables complex reasoning in large language models\n",
      "Title: Sailing the Deep Blue Sea of Decaying Burgers Turbulence\n",
      "Authors: Michel Bauer, Denis Bernard\n",
      "Year: 1998\n",
      "Abstract:   We study Lagrangian trajectories and scalar transport statistics in decaying\n",
      "Burgers turbulence. We choose velocity fields, solutions of the inviscid\n",
      "Burgers equation, whose probability distribution...\n",
      "\n",
      "\n",
      "Title: CodeT: Code Generation with Generated Tests\n",
      "Authors: Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, Weizhu Chen\n",
      "Year: 2022\n",
      "Abstract:   The task of generating code solutions for a given programming problem can\n",
      "benefit from the use of pre-trained language models such as Codex, which can\n",
      "produce multiple diverse samples. However, a ma...\n",
      "\n",
      "\n",
      "Title: Evaluating Large Language Models Trained on Code\n",
      "Authors: Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba\n",
      "Year: 2021\n",
      "Abstract:   We introduce Codex, a GPT language model fine-tuned on publicly available\n",
      "code from GitHub, and study its Python code-writing capabilities. A distinct\n",
      "production version of Codex powers GitHub Copil...\n",
      "\n",
      "\n",
      "Title: Program of Thoughts Prompting: Disentangling Computation from Reasoning\n",
      "  for Numerical Reasoning Tasks\n",
      "Authors: Wenhu Chen, Xueguang Ma, Xinyi Wang, William W. Cohen\n",
      "Year: 2022\n",
      "Abstract:   Recently, there has been significant progress in teaching language models to\n",
      "perform step-by-step reasoning to solve complex numerical reasoning tasks.\n",
      "Chain-of-thoughts prompting (CoT) is by far th...\n",
      "\n",
      "\n",
      "Title: PaLM: Scaling Language Modeling with Pathways\n",
      "Authors: Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel\n",
      "Year: 2022\n",
      "Abstract:   Large language models have been shown to achieve remarkable performance\n",
      "across a variety of natural language tasks using few-shot learning, which\n",
      "drastically reduces the number of task-specific trai...\n",
      "\n",
      "\n",
      "Title: Training Verifiers to Solve Math Word Problems\n",
      "Authors: Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman\n",
      "Year: 2021\n",
      "Abstract:   State-of-the-art language models can match human performance on many tasks,\n",
      "but they still struggle to robustly perform multi-step mathematical reasoning.\n",
      "To diagnose the failures of current models ...\n",
      "\n",
      "\n",
      "Title: Mind2Web: Towards a Generalist Agent for the Web\n",
      "Authors: Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su\n",
      "Year: 2023\n",
      "Abstract:   We introduce Mind2Web, the first dataset for developing and evaluating\n",
      "generalist agents for the web that can follow language instructions to complete\n",
      "complex tasks on any website. Existing datasets...\n",
      "\n",
      "\n",
      "Title: PaLM-E: An Embodied Multimodal Language Model\n",
      "Authors: Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Pete Florence\n",
      "Year: 2023\n",
      "Abstract:   Large language models excel at a wide range of complex tasks. However,\n",
      "enabling general inference in the real world, e.g., for robotics problems,\n",
      "raises the challenge of grounding. We propose embodi...\n",
      "\n",
      "\n",
      "Title: Long Text Generation via Adversarial Training with Leaked Information\n",
      "Authors: Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, Jun Wang\n",
      "Year: 2017\n",
      "Abstract:   Automatically generating coherent and semantically meaningful text has many\n",
      "applications in machine translation, dialogue systems, image captioning, etc.\n",
      "Recently, by combining with policy gradient,...\n",
      "\n",
      "\n",
      "Title: Learning Latent Dynamics for Planning from Pixels\n",
      "Authors: Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, James Davidson\n",
      "Year: 2018\n",
      "Abstract:   Planning has been very successful for control tasks with known environment\n",
      "dynamics. To leverage planning in unknown environments, the agent needs to\n",
      "learn the dynamics from interactions with the wo...\n",
      "\n",
      "\n",
      "Title: Mastering Diverse Domains through World Models\n",
      "Authors: Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap\n",
      "Year: 2023\n",
      "Abstract:   Developing a general algorithm that learns to solve tasks across a wide range\n",
      "of applications has been a fundamental challenge in artificial intelligence.\n",
      "Although current reinforcement learning alg...\n",
      "\n",
      "\n",
      "Title: Reasoning with Language Model is Planning with World Model\n",
      "Authors: Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu\n",
      "Year: 2023\n",
      "Abstract:   Large language models (LLMs) have shown remarkable reasoning capabilities,\n",
      "especially when prompted to generate intermediate reasoning steps (e.g.,\n",
      "Chain-of-Thought, CoT). However, LLMs can still st...\n",
      "\n",
      "\n",
      "Title: Inner Monologue: Embodied Reasoning through Planning with Language\n",
      "  Models\n",
      "Authors: Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, Brian Ichter\n",
      "Year: 2022\n",
      "Abstract:   Recent works have shown how the reasoning capabilities of Large Language\n",
      "Models (LLMs) can be applied to domains beyond natural language processing,\n",
      "such as planning and interaction for robots. Thes...\n",
      "\n",
      "\n",
      "Title: AgentBench: Evaluating LLMs as Agents\n",
      "Authors: Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang\n",
      "Year: 2023\n",
      "Abstract:   Large Language Models (LLMs) are becoming increasingly smart and autonomous,\n",
      "targeting real-world pragmatic missions beyond traditional NLP tasks. As a\n",
      "result, there has been an urgent need to evalu...\n",
      "\n",
      "\n",
      "Title: Reason for Future, Act for Now: A Principled Framework for Autonomous\n",
      "  LLM Agents with Provable Sample Efficiency\n",
      "Authors: Zhihan Liu, Hao Hu, Shenao Zhang, Hongyi Guo, Shuqi Ke, Boyi Liu, Zhaoran Wang\n",
      "Year: 2023\n",
      "Abstract:   Large language models (LLMs) demonstrate impressive reasoning abilities, but\n",
      "translating reasoning into actions in the real world remains challenging. In\n",
      "particular, it remains unclear how to comple...\n",
      "\n",
      "\n",
      "Title: GPT-4 Technical Report\n",
      "Authors:  OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto,  Michael,  Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, Barret Zoph\n",
      "Year: 2023\n",
      "Abstract:   We report the development of GPT-4, a large-scale, multimodal model which can\n",
      "accept image and text inputs and produce text outputs. While less capable than\n",
      "humans in many real-world scenarios, GPT-...\n",
      "\n",
      "\n",
      "Title: Toolformer: Language Models Can Teach Themselves to Use Tools\n",
      "Authors: Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom\n",
      "Year: 2023\n",
      "Abstract:   Language models (LMs) exhibit remarkable abilities to solve new tasks from\n",
      "just a few examples or textual instructions, especially at scale. They also,\n",
      "paradoxically, struggle with basic functionali...\n",
      "\n",
      "\n",
      "Title: HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging\n",
      "  Face\n",
      "Authors: Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang\n",
      "Year: 2023\n",
      "Abstract:   Solving complicated AI tasks with different domains and modalities is a key\n",
      "step toward artificial general intelligence. While there are numerous AI models\n",
      "available for various domains and modaliti...\n",
      "\n",
      "\n",
      "Title: Reflexion: Language Agents with Verbal Reinforcement Learning\n",
      "Authors: Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao\n",
      "Year: 2023\n",
      "Abstract:   Large language models (LLMs) have been increasingly used to interact with\n",
      "external environments (e.g., games, compilers, APIs) as goal-driven agents.\n",
      "However, it remains challenging for these langua...\n",
      "\n",
      "\n",
      "Title: ALFWorld: Aligning Text and Embodied Environments for Interactive\n",
      "  Learning\n",
      "Authors: Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht\n",
      "Year: 2020\n",
      "Abstract:   Given a simple request like Put a washed apple in the kitchen fridge, humans\n",
      "can reason in purely abstract terms by imagining action sequences and scoring\n",
      "their likelihood of success, prototypicalit...\n",
      "\n",
      "\n",
      "Title: AdaPlanner: Adaptive Planning from Feedback with Language Models\n",
      "Authors: Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang\n",
      "Year: 2023\n",
      "Abstract:   Large language models (LLMs) have recently demonstrated the potential in\n",
      "acting as autonomous agents for sequential decision-making tasks. However, most\n",
      "existing methods either take actions greedily...\n",
      "\n",
      "\n",
      "Title: ViperGPT: Visual Inference via Python Execution for Reasoning\n",
      "Authors: Dídac Surís, Sachit Menon, Carl Vondrick\n",
      "Year: 2023\n",
      "Abstract:   Answering visual queries is a complex task that requires both visual\n",
      "processing and reasoning. End-to-end models, the dominant approach for this\n",
      "task, do not explicitly differentiate between the two...\n",
      "\n",
      "\n",
      "Title: Monte Carlo Tree Search: A Review of Recent Modifications and\n",
      "  Applications\n",
      "Authors: Maciej Świechowski, Konrad Godlewski, Bartosz Sawicki, Jacek Mańdziuk\n",
      "Year: 2021\n",
      "Abstract:   Monte Carlo Tree Search (MCTS) is a powerful approach to designing\n",
      "game-playing bots or solving sequential decision problems. The method relies on\n",
      "intelligent tree search that balances exploration a...\n",
      "\n",
      "\n",
      "Title: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n",
      "Authors: Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou\n",
      "Year: 2022\n",
      "Abstract:   We explore how generating a chain of thought -- a series of intermediate\n",
      "reasoning steps -- significantly improves the ability of large language models\n",
      "to perform complex reasoning. In particular, w...\n",
      "\n",
      "\n",
      "Title: DayDreamer: World Models for Physical Robot Learning\n",
      "Authors: Philipp Wu, Alejandro Escontrela, Danijar Hafner, Ken Goldberg, Pieter Abbeel\n",
      "Year: 2022\n",
      "Abstract:   To solve tasks in complex environments, robots need to learn from experience.\n",
      "Deep reinforcement learning is a common approach to robot learning but requires\n",
      "a large amount of trial and error to lea...\n",
      "\n",
      "\n",
      "Title: Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n",
      "Authors: Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan\n",
      "Year: 2023\n",
      "Abstract:   Language models are increasingly being deployed for general problem solving\n",
      "across a wide range of tasks, but are still confined to token-level,\n",
      "left-to-right decision-making processes during infere...\n",
      "\n",
      "\n",
      "Title: ReAct: Synergizing Reasoning and Acting in Language Models\n",
      "Authors: Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao\n",
      "Year: 2022\n",
      "Abstract:   While large language models (LLMs) have demonstrated impressive capabilities\n",
      "across tasks in language understanding and interactive decision making, their\n",
      "abilities for reasoning (e.g. chain-of-thou...\n",
      "\n",
      "\n",
      "Title: Mastering Atari Games with Limited Data\n",
      "Authors: Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao\n",
      "Year: 2021\n",
      "Abstract:   Reinforcement learning has achieved great success in many applications.\n",
      "However, sample efficiency remains a key challenge, with prominent methods\n",
      "requiring millions (or even billions) of environmen...\n",
      "\n",
      "\n",
      "Title: ToolChain*: Efficient Action Space Navigation in Large Language Models\n",
      "  with A* Search\n",
      "Authors: Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan A. Rossi, Somdeb Sarkhel, Chao Zhang\n",
      "Year: 2023\n",
      "Abstract:   Large language models (LLMs) have demonstrated powerful decision-making and\n",
      "planning capabilities in solving complicated real-world problems. LLM-based\n",
      "autonomous agents can interact with diverse to...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from agent import app, get_arxiv_paper_details\n",
    "\n",
    "async def process_papers(papers):\n",
    "    results = []\n",
    "    for paper in papers:\n",
    "        title = paper['title']\n",
    "        \n",
    "        paper_details = await get_arxiv_paper_details(title=title)\n",
    "        if paper_details:\n",
    "            results.append(paper_details)\n",
    "        else:\n",
    "            print(f\"Paper not found: {title}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process the papers\n",
    "citations = json['citations']\n",
    "processed_papers = await process_papers(citations)\n",
    "\n",
    "# Print the results\n",
    "for paper in processed_papers:\n",
    "    print(f\"Title: {paper['title']}\")\n",
    "    print(f\"Authors: {', '.join(paper['authors'])}\")\n",
    "    print(f\"Year: {paper['year']}\")\n",
    "    print(f\"Abstract: {paper['abstract'][:200]}...\")  # Print first 200 characters of abstract\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Sailing the Deep Blue Sea of Decaying Burgers Turbulence',\n",
       "  'authors': ['Michel Bauer', 'Denis Bernard'],\n",
       "  'abstract': \"  We study Lagrangian trajectories and scalar transport statistics in decaying\\nBurgers turbulence. We choose velocity fields, solutions of the inviscid\\nBurgers equation, whose probability distributions are specified by Kida's\\nstatistics. They are time-correlated, not time-reversal invariant and not\\nGaussian. We discuss in some details the effect of shocks on trajectories and\\ntransport equations. We derive the inviscid limit of these equations using a\\nformalism of operators localized on shocks. We compute the probability\\ndistribution functions of the trajectories although they do not define Markov\\nprocesses. As physically expected, these trajectories are statistically\\nwell-defined but collapse with probability one at infinite time. We point out\\nthat the advected scalars enjoy inverse energy cascades. We also make a few\\ncomments on the connection between our computations and persistence problems.\\n\",\n",
       "  'year': '1998'},\n",
       " {'title': 'CodeT: Code Generation with Generated Tests',\n",
       "  'authors': ['Bei Chen',\n",
       "   'Fengji Zhang',\n",
       "   'Anh Nguyen',\n",
       "   'Daoguang Zan',\n",
       "   'Zeqi Lin',\n",
       "   'Jian-Guang Lou',\n",
       "   'Weizhu Chen'],\n",
       "  'abstract': '  The task of generating code solutions for a given programming problem can\\nbenefit from the use of pre-trained language models such as Codex, which can\\nproduce multiple diverse samples. However, a major challenge for this task is\\nto select the most appropriate solution from the multiple samples generated by\\nthe pre-trained language models. A natural way to evaluate the quality and\\ncorrectness of a code solution is to run it against a set of test cases, but\\nthe manual creation of such test cases is often costly and time-consuming. In\\nthis paper, we propose a novel method, CodeT, that leverages the same\\npre-trained language models to automatically generate test cases for the code\\nsamples, thus reducing the human effort and increasing the coverage of the test\\nscenarios. CodeT then executes the code samples using the generated test cases,\\nand performs a dual execution agreement, which considers both the consistency\\nof the outputs against the generated test cases and the agreement of the\\noutputs with other code samples. We conduct comprehensive experiments on four\\nbenchmarks, HumanEval, MBPP, APPS and CodeContests, using five different\\npre-trained language models with varying sizes and capabilities. Our results\\nshow that CodeT can significantly improve the performance of code solution\\nselection over previous methods, achieving remarkable and consistent gains\\nacross different models and benchmarks. For instance, CodeT improves the pass@1\\nmetric on HumanEval to 65.8%, which represents an absolute improvement of 18.8%\\nover the code-davinci-002 model, and an absolute improvement of more than 20%\\nover the previous state-of-the-art results.\\n',\n",
       "  'year': '2022'},\n",
       " {'title': 'Evaluating Large Language Models Trained on Code',\n",
       "  'authors': ['Mark Chen',\n",
       "   'Jerry Tworek',\n",
       "   'Heewoo Jun',\n",
       "   'Qiming Yuan',\n",
       "   'Henrique Ponde de Oliveira Pinto',\n",
       "   'Jared Kaplan',\n",
       "   'Harri Edwards',\n",
       "   'Yuri Burda',\n",
       "   'Nicholas Joseph',\n",
       "   'Greg Brockman',\n",
       "   'Alex Ray',\n",
       "   'Raul Puri',\n",
       "   'Gretchen Krueger',\n",
       "   'Michael Petrov',\n",
       "   'Heidy Khlaaf',\n",
       "   'Girish Sastry',\n",
       "   'Pamela Mishkin',\n",
       "   'Brooke Chan',\n",
       "   'Scott Gray',\n",
       "   'Nick Ryder',\n",
       "   'Mikhail Pavlov',\n",
       "   'Alethea Power',\n",
       "   'Lukasz Kaiser',\n",
       "   'Mohammad Bavarian',\n",
       "   'Clemens Winter',\n",
       "   'Philippe Tillet',\n",
       "   'Felipe Petroski Such',\n",
       "   'Dave Cummings',\n",
       "   'Matthias Plappert',\n",
       "   'Fotios Chantzis',\n",
       "   'Elizabeth Barnes',\n",
       "   'Ariel Herbert-Voss',\n",
       "   'William Hebgen Guss',\n",
       "   'Alex Nichol',\n",
       "   'Alex Paino',\n",
       "   'Nikolas Tezak',\n",
       "   'Jie Tang',\n",
       "   'Igor Babuschkin',\n",
       "   'Suchir Balaji',\n",
       "   'Shantanu Jain',\n",
       "   'William Saunders',\n",
       "   'Christopher Hesse',\n",
       "   'Andrew N. Carr',\n",
       "   'Jan Leike',\n",
       "   'Josh Achiam',\n",
       "   'Vedant Misra',\n",
       "   'Evan Morikawa',\n",
       "   'Alec Radford',\n",
       "   'Matthew Knight',\n",
       "   'Miles Brundage',\n",
       "   'Mira Murati',\n",
       "   'Katie Mayer',\n",
       "   'Peter Welinder',\n",
       "   'Bob McGrew',\n",
       "   'Dario Amodei',\n",
       "   'Sam McCandlish',\n",
       "   'Ilya Sutskever',\n",
       "   'Wojciech Zaremba'],\n",
       "  'abstract': '  We introduce Codex, a GPT language model fine-tuned on publicly available\\ncode from GitHub, and study its Python code-writing capabilities. A distinct\\nproduction version of Codex powers GitHub Copilot. On HumanEval, a new\\nevaluation set we release to measure functional correctness for synthesizing\\nprograms from docstrings, our model solves 28.8% of the problems, while GPT-3\\nsolves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling\\nfrom the model is a surprisingly effective strategy for producing working\\nsolutions to difficult prompts. Using this method, we solve 70.2% of our\\nproblems with 100 samples per problem. Careful investigation of our model\\nreveals its limitations, including difficulty with docstrings describing long\\nchains of operations and with binding operations to variables. Finally, we\\ndiscuss the potential broader impacts of deploying powerful code generation\\ntechnologies, covering safety, security, and economics.\\n',\n",
       "  'year': '2021'},\n",
       " {'title': 'Program of Thoughts Prompting: Disentangling Computation from Reasoning\\n  for Numerical Reasoning Tasks',\n",
       "  'authors': ['Wenhu Chen', 'Xueguang Ma', 'Xinyi Wang', 'William W. Cohen'],\n",
       "  'abstract': \"  Recently, there has been significant progress in teaching language models to\\nperform step-by-step reasoning to solve complex numerical reasoning tasks.\\nChain-of-thoughts prompting (CoT) is by far the state-of-art method for these\\ntasks. CoT uses language models to perform both reasoning and computation in\\nthe multi-step `thought' process. To disentangle computation from reasoning, we\\npropose `Program of Thoughts' (PoT), which uses language models (mainly Codex)\\nto express the reasoning process as a program. The computation is relegated to\\nan external computer, which executes the generated programs to derive the\\nanswer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP,\\nTabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA)\\nfor both few-shot and zero-shot setups. Under both few-shot and zero-shot\\nsettings, PoT can show an average performance gain over CoT by around 12\\\\%\\nacross all the evaluated datasets. By combining PoT with self-consistency\\ndecoding, we can achieve SoTA performance on all math problem datasets and\\nnear-SoTA performance on financial datasets. All of our data and code are\\nreleased in Github https://github.com/wenhuchen/Program-of-Thoughts\\n\",\n",
       "  'year': '2022'},\n",
       " {'title': 'PaLM: Scaling Language Modeling with Pathways',\n",
       "  'authors': ['Aakanksha Chowdhery',\n",
       "   'Sharan Narang',\n",
       "   'Jacob Devlin',\n",
       "   'Maarten Bosma',\n",
       "   'Gaurav Mishra',\n",
       "   'Adam Roberts',\n",
       "   'Paul Barham',\n",
       "   'Hyung Won Chung',\n",
       "   'Charles Sutton',\n",
       "   'Sebastian Gehrmann',\n",
       "   'Parker Schuh',\n",
       "   'Kensen Shi',\n",
       "   'Sasha Tsvyashchenko',\n",
       "   'Joshua Maynez',\n",
       "   'Abhishek Rao',\n",
       "   'Parker Barnes',\n",
       "   'Yi Tay',\n",
       "   'Noam Shazeer',\n",
       "   'Vinodkumar Prabhakaran',\n",
       "   'Emily Reif',\n",
       "   'Nan Du',\n",
       "   'Ben Hutchinson',\n",
       "   'Reiner Pope',\n",
       "   'James Bradbury',\n",
       "   'Jacob Austin',\n",
       "   'Michael Isard',\n",
       "   'Guy Gur-Ari',\n",
       "   'Pengcheng Yin',\n",
       "   'Toju Duke',\n",
       "   'Anselm Levskaya',\n",
       "   'Sanjay Ghemawat',\n",
       "   'Sunipa Dev',\n",
       "   'Henryk Michalewski',\n",
       "   'Xavier Garcia',\n",
       "   'Vedant Misra',\n",
       "   'Kevin Robinson',\n",
       "   'Liam Fedus',\n",
       "   'Denny Zhou',\n",
       "   'Daphne Ippolito',\n",
       "   'David Luan',\n",
       "   'Hyeontaek Lim',\n",
       "   'Barret Zoph',\n",
       "   'Alexander Spiridonov',\n",
       "   'Ryan Sepassi',\n",
       "   'David Dohan',\n",
       "   'Shivani Agrawal',\n",
       "   'Mark Omernick',\n",
       "   'Andrew M. Dai',\n",
       "   'Thanumalayan Sankaranarayana Pillai',\n",
       "   'Marie Pellat',\n",
       "   'Aitor Lewkowycz',\n",
       "   'Erica Moreira',\n",
       "   'Rewon Child',\n",
       "   'Oleksandr Polozov',\n",
       "   'Katherine Lee',\n",
       "   'Zongwei Zhou',\n",
       "   'Xuezhi Wang',\n",
       "   'Brennan Saeta',\n",
       "   'Mark Diaz',\n",
       "   'Orhan Firat',\n",
       "   'Michele Catasta',\n",
       "   'Jason Wei',\n",
       "   'Kathy Meier-Hellstern',\n",
       "   'Douglas Eck',\n",
       "   'Jeff Dean',\n",
       "   'Slav Petrov',\n",
       "   'Noah Fiedel'],\n",
       "  'abstract': '  Large language models have been shown to achieve remarkable performance\\nacross a variety of natural language tasks using few-shot learning, which\\ndrastically reduces the number of task-specific training examples needed to\\nadapt the model to a particular application. To further our understanding of\\nthe impact of scale on few-shot learning, we trained a 540-billion parameter,\\ndensely activated, Transformer language model, which we call Pathways Language\\nModel PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML\\nsystem which enables highly efficient training across multiple TPU Pods. We\\ndemonstrate continued benefits of scaling by achieving state-of-the-art\\nfew-shot learning results on hundreds of language understanding and generation\\nbenchmarks. On a number of these tasks, PaLM 540B achieves breakthrough\\nperformance, outperforming the finetuned state-of-the-art on a suite of\\nmulti-step reasoning tasks, and outperforming average human performance on the\\nrecently released BIG-bench benchmark. A significant number of BIG-bench tasks\\nshowed discontinuous improvements from model scale, meaning that performance\\nsteeply increased as we scaled to our largest model. PaLM also has strong\\ncapabilities in multilingual tasks and source code generation, which we\\ndemonstrate on a wide array of benchmarks. We additionally provide a\\ncomprehensive analysis on bias and toxicity, and study the extent of training\\ndata memorization with respect to model scale. Finally, we discuss the ethical\\nconsiderations related to large language models and discuss potential\\nmitigation strategies.\\n',\n",
       "  'year': '2022'},\n",
       " {'title': 'Training Verifiers to Solve Math Word Problems',\n",
       "  'authors': ['Karl Cobbe',\n",
       "   'Vineet Kosaraju',\n",
       "   'Mohammad Bavarian',\n",
       "   'Mark Chen',\n",
       "   'Heewoo Jun',\n",
       "   'Lukasz Kaiser',\n",
       "   'Matthias Plappert',\n",
       "   'Jerry Tworek',\n",
       "   'Jacob Hilton',\n",
       "   'Reiichiro Nakano',\n",
       "   'Christopher Hesse',\n",
       "   'John Schulman'],\n",
       "  'abstract': '  State-of-the-art language models can match human performance on many tasks,\\nbut they still struggle to robustly perform multi-step mathematical reasoning.\\nTo diagnose the failures of current models and support research, we introduce\\nGSM8K, a dataset of 8.5K high quality linguistically diverse grade school math\\nword problems. We find that even the largest transformer models fail to achieve\\nhigh test performance, despite the conceptual simplicity of this problem\\ndistribution. To increase performance, we propose training verifiers to judge\\nthe correctness of model completions. At test time, we generate many candidate\\nsolutions and select the one ranked highest by the verifier. We demonstrate\\nthat verification significantly improves performance on GSM8K, and we provide\\nstrong empirical evidence that verification scales more effectively with\\nincreased data than a finetuning baseline.\\n',\n",
       "  'year': '2021'},\n",
       " {'title': 'Mind2Web: Towards a Generalist Agent for the Web',\n",
       "  'authors': ['Xiang Deng',\n",
       "   'Yu Gu',\n",
       "   'Boyuan Zheng',\n",
       "   'Shijie Chen',\n",
       "   'Samuel Stevens',\n",
       "   'Boshi Wang',\n",
       "   'Huan Sun',\n",
       "   'Yu Su'],\n",
       "  'abstract': '  We introduce Mind2Web, the first dataset for developing and evaluating\\ngeneralist agents for the web that can follow language instructions to complete\\ncomplex tasks on any website. Existing datasets for web agents either use\\nsimulated websites or only cover a limited set of websites and tasks, thus not\\nsuitable for generalist web agents. With over 2,000 open-ended tasks collected\\nfrom 137 websites spanning 31 domains and crowdsourced action sequences for the\\ntasks, Mind2Web provides three necessary ingredients for building generalist\\nweb agents: 1) diverse domains, websites, and tasks, 2) use of real-world\\nwebsites instead of simulated and simplified ones, and 3) a broad spectrum of\\nuser interaction patterns. Based on Mind2Web, we conduct an initial exploration\\nof using large language models (LLMs) for building generalist web agents. While\\nthe raw HTML of real-world websites are often too large to be fed to LLMs, we\\nshow that first filtering it with a small LM significantly improves the\\neffectiveness and efficiency of LLMs. Our solution demonstrates a decent level\\nof performance, even on websites or entire domains the model has never seen\\nbefore, but there is still a substantial room to improve towards truly\\ngeneralizable agents. We open-source our dataset, model implementation, and\\ntrained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further\\nresearch on building a generalist agent for the web.\\n',\n",
       "  'year': '2023'},\n",
       " {'title': 'PaLM-E: An Embodied Multimodal Language Model',\n",
       "  'authors': ['Danny Driess',\n",
       "   'Fei Xia',\n",
       "   'Mehdi S. M. Sajjadi',\n",
       "   'Corey Lynch',\n",
       "   'Aakanksha Chowdhery',\n",
       "   'Brian Ichter',\n",
       "   'Ayzaan Wahid',\n",
       "   'Jonathan Tompson',\n",
       "   'Quan Vuong',\n",
       "   'Tianhe Yu',\n",
       "   'Wenlong Huang',\n",
       "   'Yevgen Chebotar',\n",
       "   'Pierre Sermanet',\n",
       "   'Daniel Duckworth',\n",
       "   'Sergey Levine',\n",
       "   'Vincent Vanhoucke',\n",
       "   'Karol Hausman',\n",
       "   'Marc Toussaint',\n",
       "   'Klaus Greff',\n",
       "   'Andy Zeng',\n",
       "   'Igor Mordatch',\n",
       "   'Pete Florence'],\n",
       "  'abstract': '  Large language models excel at a wide range of complex tasks. However,\\nenabling general inference in the real world, e.g., for robotics problems,\\nraises the challenge of grounding. We propose embodied language models to\\ndirectly incorporate real-world continuous sensor modalities into language\\nmodels and thereby establish the link between words and percepts. Input to our\\nembodied language model are multi-modal sentences that interleave visual,\\ncontinuous state estimation, and textual input encodings. We train these\\nencodings end-to-end, in conjunction with a pre-trained large language model,\\nfor multiple embodied tasks including sequential robotic manipulation planning,\\nvisual question answering, and captioning. Our evaluations show that PaLM-E, a\\nsingle large embodied multimodal model, can address a variety of embodied\\nreasoning tasks, from a variety of observation modalities, on multiple\\nembodiments, and further, exhibits positive transfer: the model benefits from\\ndiverse joint training across internet-scale language, vision, and\\nvisual-language domains. Our largest model, PaLM-E-562B with 562B parameters,\\nin addition to being trained on robotics tasks, is a visual-language generalist\\nwith state-of-the-art performance on OK-VQA, and retains generalist language\\ncapabilities with increasing scale.\\n',\n",
       "  'year': '2023'},\n",
       " {'title': 'Long Text Generation via Adversarial Training with Leaked Information',\n",
       "  'authors': ['Jiaxian Guo',\n",
       "   'Sidi Lu',\n",
       "   'Han Cai',\n",
       "   'Weinan Zhang',\n",
       "   'Yong Yu',\n",
       "   'Jun Wang'],\n",
       "  'abstract': '  Automatically generating coherent and semantically meaningful text has many\\napplications in machine translation, dialogue systems, image captioning, etc.\\nRecently, by combining with policy gradient, Generative Adversarial Nets (GAN)\\nthat use a discriminative model to guide the training of the generative model\\nas a reinforcement learning policy has shown promising results in text\\ngeneration. However, the scalar guiding signal is only available after the\\nentire text has been generated and lacks intermediate information about text\\nstructure during the generative process. As such, it limits its success when\\nthe length of the generated text samples is long (more than 20 words). In this\\npaper, we propose a new framework, called LeakGAN, to address the problem for\\nlong text generation. We allow the discriminative net to leak its own\\nhigh-level extracted features to the generative net to further help the\\nguidance. The generator incorporates such informative signals into all\\ngeneration steps through an additional Manager module, which takes the\\nextracted features of current generated words and outputs a latent vector to\\nguide the Worker module for next-word generation. Our extensive experiments on\\nsynthetic data and various real-world tasks with Turing test demonstrate that\\nLeakGAN is highly effective in long text generation and also improves the\\nperformance in short text generation scenarios. More importantly, without any\\nsupervision, LeakGAN would be able to implicitly learn sentence structures only\\nthrough the interaction between Manager and Worker.\\n',\n",
       "  'year': '2017'},\n",
       " {'title': 'Learning Latent Dynamics for Planning from Pixels',\n",
       "  'authors': ['Danijar Hafner',\n",
       "   'Timothy Lillicrap',\n",
       "   'Ian Fischer',\n",
       "   'Ruben Villegas',\n",
       "   'David Ha',\n",
       "   'Honglak Lee',\n",
       "   'James Davidson'],\n",
       "  'abstract': '  Planning has been very successful for control tasks with known environment\\ndynamics. To leverage planning in unknown environments, the agent needs to\\nlearn the dynamics from interactions with the world. However, learning dynamics\\nmodels that are accurate enough for planning has been a long-standing\\nchallenge, especially in image-based domains. We propose the Deep Planning\\nNetwork (PlaNet), a purely model-based agent that learns the environment\\ndynamics from images and chooses actions through fast online planning in latent\\nspace. To achieve high performance, the dynamics model must accurately predict\\nthe rewards ahead for multiple time steps. We approach this using a latent\\ndynamics model with both deterministic and stochastic transition components.\\nMoreover, we propose a multi-step variational inference objective that we name\\nlatent overshooting. Using only pixel observations, our agent solves continuous\\ncontrol tasks with contact dynamics, partial observability, and sparse rewards,\\nwhich exceed the difficulty of tasks that were previously solved by planning\\nwith learned models. PlaNet uses substantially fewer episodes and reaches final\\nperformance close to and sometimes higher than strong model-free algorithms.\\n',\n",
       "  'year': '2018'},\n",
       " {'title': 'Mastering Diverse Domains through World Models',\n",
       "  'authors': ['Danijar Hafner',\n",
       "   'Jurgis Pasukonis',\n",
       "   'Jimmy Ba',\n",
       "   'Timothy Lillicrap'],\n",
       "  'abstract': '  Developing a general algorithm that learns to solve tasks across a wide range\\nof applications has been a fundamental challenge in artificial intelligence.\\nAlthough current reinforcement learning algorithms can be readily applied to\\ntasks similar to what they have been developed for, configuring them for new\\napplication domains requires significant human expertise and experimentation.\\nWe present DreamerV3, a general algorithm that outperforms specialized methods\\nacross over 150 diverse tasks, with a single configuration. Dreamer learns a\\nmodel of the environment and improves its behavior by imagining future\\nscenarios. Robustness techniques based on normalization, balancing, and\\ntransformations enable stable learning across domains. Applied out of the box,\\nDreamer is the first algorithm to collect diamonds in Minecraft from scratch\\nwithout human data or curricula. This achievement has been posed as a\\nsignificant challenge in artificial intelligence that requires exploring\\nfarsighted strategies from pixels and sparse rewards in an open world. Our work\\nallows solving challenging control problems without extensive experimentation,\\nmaking reinforcement learning broadly applicable.\\n',\n",
       "  'year': '2023'},\n",
       " {'title': 'Reasoning with Language Model is Planning with World Model',\n",
       "  'authors': ['Shibo Hao',\n",
       "   'Yi Gu',\n",
       "   'Haodi Ma',\n",
       "   'Joshua Jiahua Hong',\n",
       "   'Zhen Wang',\n",
       "   'Daisy Zhe Wang',\n",
       "   'Zhiting Hu'],\n",
       "  'abstract': '  Large language models (LLMs) have shown remarkable reasoning capabilities,\\nespecially when prompted to generate intermediate reasoning steps (e.g.,\\nChain-of-Thought, CoT). However, LLMs can still struggle with problems that are\\neasy for humans, such as generating action plans for executing tasks in a given\\nenvironment, or performing complex math, logical, and commonsense reasoning.\\nThe deficiency stems from the key fact that LLMs lack an internal\\n$\\\\textit{world model}$ to predict the world $\\\\textit{state}$ (e.g., environment\\nstatus, intermediate variable values) and simulate long-term outcomes of\\nactions. This prevents LLMs from performing deliberate planning akin to human\\nbrains, which involves exploring alternative reasoning paths, anticipating\\nfuture states and rewards, and iteratively refining existing reasoning steps.\\nTo overcome the limitations, we propose a new LLM reasoning framework,\\n$\\\\underline{R}$easoning vi$\\\\underline{a}$ $\\\\underline{P}$lanning\\n$\\\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning\\nagent, and incorporates a principled planning algorithm (based on Monto Carlo\\nTree Search) for strategic exploration in the vast reasoning space. During\\nreasoning, the LLM (as agent) incrementally builds a reasoning tree under the\\nguidance of the LLM (as world model) and task-specific rewards, and obtains a\\nhigh-reward reasoning path efficiently with a proper balance between\\nexploration $\\\\textit{vs.}$ exploitation. We apply RAP to a variety of\\nchallenging reasoning problems including plan generation, math reasoning, and\\nlogical inference. Empirical results on these tasks demonstrate the superiority\\nof RAP over various strong baselines, including CoT and least-to-most prompting\\nwith self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33%\\nrelative improvement in a plan generation setting.\\n',\n",
       "  'year': '2023'},\n",
       " {'title': 'Inner Monologue: Embodied Reasoning through Planning with Language\\n  Models',\n",
       "  'authors': ['Wenlong Huang',\n",
       "   'Fei Xia',\n",
       "   'Ted Xiao',\n",
       "   'Harris Chan',\n",
       "   'Jacky Liang',\n",
       "   'Pete Florence',\n",
       "   'Andy Zeng',\n",
       "   'Jonathan Tompson',\n",
       "   'Igor Mordatch',\n",
       "   'Yevgen Chebotar',\n",
       "   'Pierre Sermanet',\n",
       "   'Noah Brown',\n",
       "   'Tomas Jackson',\n",
       "   'Linda Luu',\n",
       "   'Sergey Levine',\n",
       "   'Karol Hausman',\n",
       "   'Brian Ichter'],\n",
       "  'abstract': \"  Recent works have shown how the reasoning capabilities of Large Language\\nModels (LLMs) can be applied to domains beyond natural language processing,\\nsuch as planning and interaction for robots. These embodied problems require an\\nagent to understand many semantic aspects of the world: the repertoire of\\nskills available, how these skills influence the world, and how changes to the\\nworld map back to the language. LLMs planning in embodied environments need to\\nconsider not just what skills to do, but also how and when to do them - answers\\nthat change over time in response to the agent's own choices. In this work, we\\ninvestigate to what extent LLMs used in such embodied contexts can reason over\\nsources of feedback provided through natural language, without any additional\\ntraining. We propose that by leveraging environment feedback, LLMs are able to\\nform an inner monologue that allows them to more richly process and plan in\\nrobotic control scenarios. We investigate a variety of sources of feedback,\\nsuch as success detection, scene description, and human interaction. We find\\nthat closed-loop language feedback significantly improves high-level\\ninstruction completion on three domains, including simulated and real table top\\nrearrangement tasks and long-horizon mobile manipulation tasks in a kitchen\\nenvironment in the real world.\\n\",\n",
       "  'year': '2022'},\n",
       " {'title': 'AgentBench: Evaluating LLMs as Agents',\n",
       "  'authors': ['Xiao Liu',\n",
       "   'Hao Yu',\n",
       "   'Hanchen Zhang',\n",
       "   'Yifan Xu',\n",
       "   'Xuanyu Lei',\n",
       "   'Hanyu Lai',\n",
       "   'Yu Gu',\n",
       "   'Hangliang Ding',\n",
       "   'Kaiwen Men',\n",
       "   'Kejuan Yang',\n",
       "   'Shudan Zhang',\n",
       "   'Xiang Deng',\n",
       "   'Aohan Zeng',\n",
       "   'Zhengxiao Du',\n",
       "   'Chenhui Zhang',\n",
       "   'Sheng Shen',\n",
       "   'Tianjun Zhang',\n",
       "   'Yu Su',\n",
       "   'Huan Sun',\n",
       "   'Minlie Huang',\n",
       "   'Yuxiao Dong',\n",
       "   'Jie Tang'],\n",
       "  'abstract': \"  Large Language Models (LLMs) are becoming increasingly smart and autonomous,\\ntargeting real-world pragmatic missions beyond traditional NLP tasks. As a\\nresult, there has been an urgent need to evaluate LLMs as agents on challenging\\ntasks in interactive environments. We present AgentBench, a multi-dimensional\\nevolving benchmark that currently consists of 8 distinct environments to assess\\nLLM-as-Agent's reasoning and decision-making abilities in a multi-turn\\nopen-ended generation setting. Our extensive test over 27 API-based and\\nopen-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong\\nability of acting as agents in complex environments, there is a significant\\ndisparity in performance between them and OSS competitors. We identify the\\ntypical reasons of failures in environments and LLMs, showing that poor\\nlong-term reasoning, decision-making, and instruction following abilities are\\nthe main obstacles for developing usable LLM agents. Training on code and high\\nquality multi-turn alignment data could improve agent performance. Datasets,\\nenvironments, and an integrated evaluation package for AgentBench are released\\nat \\\\url{https://github.com/THUDM/AgentBench}.\\n\",\n",
       "  'year': '2023'},\n",
       " {'title': 'Reason for Future, Act for Now: A Principled Framework for Autonomous\\n  LLM Agents with Provable Sample Efficiency',\n",
       "  'authors': ['Zhihan Liu',\n",
       "   'Hao Hu',\n",
       "   'Shenao Zhang',\n",
       "   'Hongyi Guo',\n",
       "   'Shuqi Ke',\n",
       "   'Boyi Liu',\n",
       "   'Zhaoran Wang'],\n",
       "  'abstract': '  Large language models (LLMs) demonstrate impressive reasoning abilities, but\\ntranslating reasoning into actions in the real world remains challenging. In\\nparticular, it remains unclear how to complete a given task provably within a\\nminimum number of interactions with the external environment, e.g., through an\\ninternal mechanism of reasoning. To this end, we propose a principled framework\\nwith provable regret guarantees to orchestrate reasoning and acting, which we\\ncall \"reason for future, act for now\" (\\\\texttt{RAFA}). Specifically, we design\\na prompt template for reasoning that learns from the memory buffer and plans a\\nfuture trajectory over a long horizon (\"reason for future\"). At each step, the\\nLLM agent takes the initial action of the planned trajectory (\"act for now\"),\\nstores the collected feedback in the memory buffer, and reinvokes the reasoning\\nroutine to replan the future trajectory from the new state.\\n  The key idea is to cast reasoning in LLMs as learning and planning in\\nBayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt\\nLLMs to form an updated posterior of the unknown environment from the memory\\nbuffer (learning) and generate an optimal trajectory for multiple future steps\\nthat maximizes a value function (planning). The learning and planning\\nsubroutines are performed in an \"in-context\" manner to emulate the actor-critic\\nupdate for MDPs. Our theoretical analysis proves that the novel combination of\\nlong-term reasoning and short-term acting achieves a $\\\\sqrt{T}$ regret. Here,\\n$T$ denotes the number of online interactions. In particular, the regret bound\\nhighlights an intriguing interplay between the prior knowledge obtained through\\npretraining and the uncertainty reduction achieved by reasoning and acting. Our\\nempirical validation shows that it outperforms various existing frameworks and\\nachieves nearly perfect scores on a few benchmarks.\\n',\n",
       "  'year': '2023'},\n",
       " {'title': 'GPT-4 Technical Report',\n",
       "  'authors': [' OpenAI',\n",
       "   'Josh Achiam',\n",
       "   'Steven Adler',\n",
       "   'Sandhini Agarwal',\n",
       "   'Lama Ahmad',\n",
       "   'Ilge Akkaya',\n",
       "   'Florencia Leoni Aleman',\n",
       "   'Diogo Almeida',\n",
       "   'Janko Altenschmidt',\n",
       "   'Sam Altman',\n",
       "   'Shyamal Anadkat',\n",
       "   'Red Avila',\n",
       "   'Igor Babuschkin',\n",
       "   'Suchir Balaji',\n",
       "   'Valerie Balcom',\n",
       "   'Paul Baltescu',\n",
       "   'Haiming Bao',\n",
       "   'Mohammad Bavarian',\n",
       "   'Jeff Belgum',\n",
       "   'Irwan Bello',\n",
       "   'Jake Berdine',\n",
       "   'Gabriel Bernadett-Shapiro',\n",
       "   'Christopher Berner',\n",
       "   'Lenny Bogdonoff',\n",
       "   'Oleg Boiko',\n",
       "   'Madelaine Boyd',\n",
       "   'Anna-Luisa Brakman',\n",
       "   'Greg Brockman',\n",
       "   'Tim Brooks',\n",
       "   'Miles Brundage',\n",
       "   'Kevin Button',\n",
       "   'Trevor Cai',\n",
       "   'Rosie Campbell',\n",
       "   'Andrew Cann',\n",
       "   'Brittany Carey',\n",
       "   'Chelsea Carlson',\n",
       "   'Rory Carmichael',\n",
       "   'Brooke Chan',\n",
       "   'Che Chang',\n",
       "   'Fotis Chantzis',\n",
       "   'Derek Chen',\n",
       "   'Sully Chen',\n",
       "   'Ruby Chen',\n",
       "   'Jason Chen',\n",
       "   'Mark Chen',\n",
       "   'Ben Chess',\n",
       "   'Chester Cho',\n",
       "   'Casey Chu',\n",
       "   'Hyung Won Chung',\n",
       "   'Dave Cummings',\n",
       "   'Jeremiah Currier',\n",
       "   'Yunxing Dai',\n",
       "   'Cory Decareaux',\n",
       "   'Thomas Degry',\n",
       "   'Noah Deutsch',\n",
       "   'Damien Deville',\n",
       "   'Arka Dhar',\n",
       "   'David Dohan',\n",
       "   'Steve Dowling',\n",
       "   'Sheila Dunning',\n",
       "   'Adrien Ecoffet',\n",
       "   'Atty Eleti',\n",
       "   'Tyna Eloundou',\n",
       "   'David Farhi',\n",
       "   'Liam Fedus',\n",
       "   'Niko Felix',\n",
       "   'Simón Posada Fishman',\n",
       "   'Juston Forte',\n",
       "   'Isabella Fulford',\n",
       "   'Leo Gao',\n",
       "   'Elie Georges',\n",
       "   'Christian Gibson',\n",
       "   'Vik Goel',\n",
       "   'Tarun Gogineni',\n",
       "   'Gabriel Goh',\n",
       "   'Rapha Gontijo-Lopes',\n",
       "   'Jonathan Gordon',\n",
       "   'Morgan Grafstein',\n",
       "   'Scott Gray',\n",
       "   'Ryan Greene',\n",
       "   'Joshua Gross',\n",
       "   'Shixiang Shane Gu',\n",
       "   'Yufei Guo',\n",
       "   'Chris Hallacy',\n",
       "   'Jesse Han',\n",
       "   'Jeff Harris',\n",
       "   'Yuchen He',\n",
       "   'Mike Heaton',\n",
       "   'Johannes Heidecke',\n",
       "   'Chris Hesse',\n",
       "   'Alan Hickey',\n",
       "   'Wade Hickey',\n",
       "   'Peter Hoeschele',\n",
       "   'Brandon Houghton',\n",
       "   'Kenny Hsu',\n",
       "   'Shengli Hu',\n",
       "   'Xin Hu',\n",
       "   'Joost Huizinga',\n",
       "   'Shantanu Jain',\n",
       "   'Shawn Jain',\n",
       "   'Joanne Jang',\n",
       "   'Angela Jiang',\n",
       "   'Roger Jiang',\n",
       "   'Haozhun Jin',\n",
       "   'Denny Jin',\n",
       "   'Shino Jomoto',\n",
       "   'Billie Jonn',\n",
       "   'Heewoo Jun',\n",
       "   'Tomer Kaftan',\n",
       "   'Łukasz Kaiser',\n",
       "   'Ali Kamali',\n",
       "   'Ingmar Kanitscheider',\n",
       "   'Nitish Shirish Keskar',\n",
       "   'Tabarak Khan',\n",
       "   'Logan Kilpatrick',\n",
       "   'Jong Wook Kim',\n",
       "   'Christina Kim',\n",
       "   'Yongjik Kim',\n",
       "   'Jan Hendrik Kirchner',\n",
       "   'Jamie Kiros',\n",
       "   'Matt Knight',\n",
       "   'Daniel Kokotajlo',\n",
       "   'Łukasz Kondraciuk',\n",
       "   'Andrew Kondrich',\n",
       "   'Aris Konstantinidis',\n",
       "   'Kyle Kosic',\n",
       "   'Gretchen Krueger',\n",
       "   'Vishal Kuo',\n",
       "   'Michael Lampe',\n",
       "   'Ikai Lan',\n",
       "   'Teddy Lee',\n",
       "   'Jan Leike',\n",
       "   'Jade Leung',\n",
       "   'Daniel Levy',\n",
       "   'Chak Ming Li',\n",
       "   'Rachel Lim',\n",
       "   'Molly Lin',\n",
       "   'Stephanie Lin',\n",
       "   'Mateusz Litwin',\n",
       "   'Theresa Lopez',\n",
       "   'Ryan Lowe',\n",
       "   'Patricia Lue',\n",
       "   'Anna Makanju',\n",
       "   'Kim Malfacini',\n",
       "   'Sam Manning',\n",
       "   'Todor Markov',\n",
       "   'Yaniv Markovski',\n",
       "   'Bianca Martin',\n",
       "   'Katie Mayer',\n",
       "   'Andrew Mayne',\n",
       "   'Bob McGrew',\n",
       "   'Scott Mayer McKinney',\n",
       "   'Christine McLeavey',\n",
       "   'Paul McMillan',\n",
       "   'Jake McNeil',\n",
       "   'David Medina',\n",
       "   'Aalok Mehta',\n",
       "   'Jacob Menick',\n",
       "   'Luke Metz',\n",
       "   'Andrey Mishchenko',\n",
       "   'Pamela Mishkin',\n",
       "   'Vinnie Monaco',\n",
       "   'Evan Morikawa',\n",
       "   'Daniel Mossing',\n",
       "   'Tong Mu',\n",
       "   'Mira Murati',\n",
       "   'Oleg Murk',\n",
       "   'David Mély',\n",
       "   'Ashvin Nair',\n",
       "   'Reiichiro Nakano',\n",
       "   'Rajeev Nayak',\n",
       "   'Arvind Neelakantan',\n",
       "   'Richard Ngo',\n",
       "   'Hyeonwoo Noh',\n",
       "   'Long Ouyang',\n",
       "   \"Cullen O'Keefe\",\n",
       "   'Jakub Pachocki',\n",
       "   'Alex Paino',\n",
       "   'Joe Palermo',\n",
       "   'Ashley Pantuliano',\n",
       "   'Giambattista Parascandolo',\n",
       "   'Joel Parish',\n",
       "   'Emy Parparita',\n",
       "   'Alex Passos',\n",
       "   'Mikhail Pavlov',\n",
       "   'Andrew Peng',\n",
       "   'Adam Perelman',\n",
       "   'Filipe de Avila Belbute Peres',\n",
       "   'Michael Petrov',\n",
       "   'Henrique Ponde de Oliveira Pinto',\n",
       "   ' Michael',\n",
       "   ' Pokorny',\n",
       "   'Michelle Pokrass',\n",
       "   'Vitchyr H. Pong',\n",
       "   'Tolly Powell',\n",
       "   'Alethea Power',\n",
       "   'Boris Power',\n",
       "   'Elizabeth Proehl',\n",
       "   'Raul Puri',\n",
       "   'Alec Radford',\n",
       "   'Jack Rae',\n",
       "   'Aditya Ramesh',\n",
       "   'Cameron Raymond',\n",
       "   'Francis Real',\n",
       "   'Kendra Rimbach',\n",
       "   'Carl Ross',\n",
       "   'Bob Rotsted',\n",
       "   'Henri Roussez',\n",
       "   'Nick Ryder',\n",
       "   'Mario Saltarelli',\n",
       "   'Ted Sanders',\n",
       "   'Shibani Santurkar',\n",
       "   'Girish Sastry',\n",
       "   'Heather Schmidt',\n",
       "   'David Schnurr',\n",
       "   'John Schulman',\n",
       "   'Daniel Selsam',\n",
       "   'Kyla Sheppard',\n",
       "   'Toki Sherbakov',\n",
       "   'Jessica Shieh',\n",
       "   'Sarah Shoker',\n",
       "   'Pranav Shyam',\n",
       "   'Szymon Sidor',\n",
       "   'Eric Sigler',\n",
       "   'Maddie Simens',\n",
       "   'Jordan Sitkin',\n",
       "   'Katarina Slama',\n",
       "   'Ian Sohl',\n",
       "   'Benjamin Sokolowsky',\n",
       "   'Yang Song',\n",
       "   'Natalie Staudacher',\n",
       "   'Felipe Petroski Such',\n",
       "   'Natalie Summers',\n",
       "   'Ilya Sutskever',\n",
       "   'Jie Tang',\n",
       "   'Nikolas Tezak',\n",
       "   'Madeleine B. Thompson',\n",
       "   'Phil Tillet',\n",
       "   'Amin Tootoonchian',\n",
       "   'Elizabeth Tseng',\n",
       "   'Preston Tuggle',\n",
       "   'Nick Turley',\n",
       "   'Jerry Tworek',\n",
       "   'Juan Felipe Cerón Uribe',\n",
       "   'Andrea Vallone',\n",
       "   'Arun Vijayvergiya',\n",
       "   'Chelsea Voss',\n",
       "   'Carroll Wainwright',\n",
       "   'Justin Jay Wang',\n",
       "   'Alvin Wang',\n",
       "   'Ben Wang',\n",
       "   'Jonathan Ward',\n",
       "   'Jason Wei',\n",
       "   'CJ Weinmann',\n",
       "   'Akila Welihinda',\n",
       "   'Peter Welinder',\n",
       "   'Jiayi Weng',\n",
       "   'Lilian Weng',\n",
       "   'Matt Wiethoff',\n",
       "   'Dave Willner',\n",
       "   'Clemens Winter',\n",
       "   'Samuel Wolrich',\n",
       "   'Hannah Wong',\n",
       "   'Lauren Workman',\n",
       "   'Sherwin Wu',\n",
       "   'Jeff Wu',\n",
       "   'Michael Wu',\n",
       "   'Kai Xiao',\n",
       "   'Tao Xu',\n",
       "   'Sarah Yoo',\n",
       "   'Kevin Yu',\n",
       "   'Qiming Yuan',\n",
       "   'Wojciech Zaremba',\n",
       "   'Rowan Zellers',\n",
       "   'Chong Zhang',\n",
       "   'Marvin Zhang',\n",
       "   'Shengjia Zhao',\n",
       "   'Tianhao Zheng',\n",
       "   'Juntang Zhuang',\n",
       "   'William Zhuk',\n",
       "   'Barret Zoph'],\n",
       "  'abstract': \"  We report the development of GPT-4, a large-scale, multimodal model which can\\naccept image and text inputs and produce text outputs. While less capable than\\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\\nvarious professional and academic benchmarks, including passing a simulated bar\\nexam with a score around the top 10% of test takers. GPT-4 is a\\nTransformer-based model pre-trained to predict the next token in a document.\\nThe post-training alignment process results in improved performance on measures\\nof factuality and adherence to desired behavior. A core component of this\\nproject was developing infrastructure and optimization methods that behave\\npredictably across a wide range of scales. This allowed us to accurately\\npredict some aspects of GPT-4's performance based on models trained with no\\nmore than 1/1,000th the compute of GPT-4.\\n\",\n",
       "  'year': '2023'},\n",
       " {'title': 'Toolformer: Language Models Can Teach Themselves to Use Tools',\n",
       "  'authors': ['Timo Schick',\n",
       "   'Jane Dwivedi-Yu',\n",
       "   'Roberto Dessì',\n",
       "   'Roberta Raileanu',\n",
       "   'Maria Lomeli',\n",
       "   'Luke Zettlemoyer',\n",
       "   'Nicola Cancedda',\n",
       "   'Thomas Scialom'],\n",
       "  'abstract': '  Language models (LMs) exhibit remarkable abilities to solve new tasks from\\njust a few examples or textual instructions, especially at scale. They also,\\nparadoxically, struggle with basic functionality, such as arithmetic or factual\\nlookup, where much simpler and smaller models excel. In this paper, we show\\nthat LMs can teach themselves to use external tools via simple APIs and achieve\\nthe best of both worlds. We introduce Toolformer, a model trained to decide\\nwhich APIs to call, when to call them, what arguments to pass, and how to best\\nincorporate the results into future token prediction. This is done in a\\nself-supervised way, requiring nothing more than a handful of demonstrations\\nfor each API. We incorporate a range of tools, including a calculator, a Q\\\\&A\\nsystem, two different search engines, a translation system, and a calendar.\\nToolformer achieves substantially improved zero-shot performance across a\\nvariety of downstream tasks, often competitive with much larger models, without\\nsacrificing its core language modeling abilities.\\n',\n",
       "  'year': '2023'},\n",
       " {'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging\\n  Face',\n",
       "  'authors': ['Yongliang Shen',\n",
       "   'Kaitao Song',\n",
       "   'Xu Tan',\n",
       "   'Dongsheng Li',\n",
       "   'Weiming Lu',\n",
       "   'Yueting Zhuang'],\n",
       "  'abstract': '  Solving complicated AI tasks with different domains and modalities is a key\\nstep toward artificial general intelligence. While there are numerous AI models\\navailable for various domains and modalities, they cannot handle complicated AI\\ntasks autonomously. Considering large language models (LLMs) have exhibited\\nexceptional abilities in language understanding, generation, interaction, and\\nreasoning, we advocate that LLMs could act as a controller to manage existing\\nAI models to solve complicated AI tasks, with language serving as a generic\\ninterface to empower this. Based on this philosophy, we present HuggingGPT, an\\nLLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI\\nmodels in machine learning communities (e.g., Hugging Face) to solve AI tasks.\\nSpecifically, we use ChatGPT to conduct task planning when receiving a user\\nrequest, select models according to their function descriptions available in\\nHugging Face, execute each subtask with the selected AI model, and summarize\\nthe response according to the execution results. By leveraging the strong\\nlanguage capability of ChatGPT and abundant AI models in Hugging Face,\\nHuggingGPT can tackle a wide range of sophisticated AI tasks spanning different\\nmodalities and domains and achieve impressive results in language, vision,\\nspeech, and other challenging tasks, which paves a new way towards the\\nrealization of artificial general intelligence.\\n',\n",
       "  'year': '2023'},\n",
       " {'title': 'Reflexion: Language Agents with Verbal Reinforcement Learning',\n",
       "  'authors': ['Noah Shinn',\n",
       "   'Federico Cassano',\n",
       "   'Edward Berman',\n",
       "   'Ashwin Gopinath',\n",
       "   'Karthik Narasimhan',\n",
       "   'Shunyu Yao'],\n",
       "  'abstract': '  Large language models (LLMs) have been increasingly used to interact with\\nexternal environments (e.g., games, compilers, APIs) as goal-driven agents.\\nHowever, it remains challenging for these language agents to quickly and\\nefficiently learn from trial-and-error as traditional reinforcement learning\\nmethods require extensive training samples and expensive model fine-tuning. We\\npropose Reflexion, a novel framework to reinforce language agents not by\\nupdating weights, but instead through linguistic feedback. Concretely,\\nReflexion agents verbally reflect on task feedback signals, then maintain their\\nown reflective text in an episodic memory buffer to induce better\\ndecision-making in subsequent trials. Reflexion is flexible enough to\\nincorporate various types (scalar values or free-form language) and sources\\n(external or internally simulated) of feedback signals, and obtains significant\\nimprovements over a baseline agent across diverse tasks (sequential\\ndecision-making, coding, language reasoning). For example, Reflexion achieves a\\n91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous\\nstate-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis\\nstudies using different feedback signals, feedback incorporation methods, and\\nagent types, and provide insights into how they affect performance.\\n',\n",
       "  'year': '2023'},\n",
       " {'title': 'ALFWorld: Aligning Text and Embodied Environments for Interactive\\n  Learning',\n",
       "  'authors': ['Mohit Shridhar',\n",
       "   'Xingdi Yuan',\n",
       "   'Marc-Alexandre Côté',\n",
       "   'Yonatan Bisk',\n",
       "   'Adam Trischler',\n",
       "   'Matthew Hausknecht'],\n",
       "  'abstract': \"  Given a simple request like Put a washed apple in the kitchen fridge, humans\\ncan reason in purely abstract terms by imagining action sequences and scoring\\ntheir likelihood of success, prototypicality, and efficiency, all without\\nmoving a muscle. Once we see the kitchen in question, we can update our\\nabstract plans to fit the scene. Embodied agents require the same abilities,\\nbut existing work does not yet provide the infrastructure necessary for both\\nreasoning abstractly and executing concretely. We address this limitation by\\nintroducing ALFWorld, a simulator that enables agents to learn abstract, text\\nbased policies in TextWorld (C\\\\^ot\\\\'e et al., 2018) and then execute goals from\\nthe ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment.\\nALFWorld enables the creation of a new BUTLER agent whose abstract knowledge,\\nlearned in TextWorld, corresponds directly to concrete, visually grounded\\nactions. In turn, as we demonstrate empirically, this fosters better agent\\ngeneralization than training only in the visually grounded environment.\\nBUTLER's simple, modular design factors the problem to allow researchers to\\nfocus on models for improving every piece of the pipeline (language\\nunderstanding, planning, navigation, and visual scene understanding).\\n\",\n",
       "  'year': '2020'},\n",
       " {'title': 'AdaPlanner: Adaptive Planning from Feedback with Language Models',\n",
       "  'authors': ['Haotian Sun',\n",
       "   'Yuchen Zhuang',\n",
       "   'Lingkai Kong',\n",
       "   'Bo Dai',\n",
       "   'Chao Zhang'],\n",
       "  'abstract': '  Large language models (LLMs) have recently demonstrated the potential in\\nacting as autonomous agents for sequential decision-making tasks. However, most\\nexisting methods either take actions greedily without planning or rely on\\nstatic plans that are not adaptable to environmental feedback. Consequently,\\nthe sequential decision-making performance of LLM agents degenerates with\\nproblem complexity and plan horizons increase. We propose a closed-loop\\napproach, AdaPlanner, which allows the LLM agent to refine its self-generated\\nplan adaptively in response to environmental feedback. In AdaPlanner, the LLM\\nagent adaptively refines its plan from feedback with both in-plan and\\nout-of-plan refinement strategies. To mitigate hallucination, we develop a\\ncode-style LLM prompt structure that facilitates plan generation across a\\nvariety of tasks, environments, and agent capabilities. Furthermore, we propose\\na skill discovery mechanism that leverages successful plans as few-shot\\nexemplars, enabling the agent to plan and refine with fewer task\\ndemonstrations. Our experiments in the ALFWorld and MiniWoB++ environments\\ndemonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and\\n4.11% while utilizing 2x and 600x fewer samples, respectively.\\n',\n",
       "  'year': '2023'},\n",
       " {'title': 'ViperGPT: Visual Inference via Python Execution for Reasoning',\n",
       "  'authors': ['Dídac Surís', 'Sachit Menon', 'Carl Vondrick'],\n",
       "  'abstract': '  Answering visual queries is a complex task that requires both visual\\nprocessing and reasoning. End-to-end models, the dominant approach for this\\ntask, do not explicitly differentiate between the two, limiting\\ninterpretability and generalization. Learning modular programs presents a\\npromising alternative, but has proven challenging due to the difficulty of\\nlearning both the programs and modules simultaneously. We introduce ViperGPT, a\\nframework that leverages code-generation models to compose vision-and-language\\nmodels into subroutines to produce a result for any query. ViperGPT utilizes a\\nprovided API to access the available modules, and composes them by generating\\nPython code that is later executed. This simple approach requires no further\\ntraining, and achieves state-of-the-art results across various complex visual\\ntasks.\\n',\n",
       "  'year': '2023'},\n",
       " {'title': 'Monte Carlo Tree Search: A Review of Recent Modifications and\\n  Applications',\n",
       "  'authors': ['Maciej Świechowski',\n",
       "   'Konrad Godlewski',\n",
       "   'Bartosz Sawicki',\n",
       "   'Jacek Mańdziuk'],\n",
       "  'abstract': '  Monte Carlo Tree Search (MCTS) is a powerful approach to designing\\ngame-playing bots or solving sequential decision problems. The method relies on\\nintelligent tree search that balances exploration and exploitation. MCTS\\nperforms random sampling in the form of simulations and stores statistics of\\nactions to make more educated choices in each subsequent iteration. The method\\nhas become a state-of-the-art technique for combinatorial games, however, in\\nmore complex games (e.g. those with high branching factor or real-time ones),\\nas well as in various practical domains (e.g. transportation, scheduling or\\nsecurity) an efficient MCTS application often requires its problem-dependent\\nmodification or integration with other techniques. Such domain-specific\\nmodifications and hybrid approaches are the main focus of this survey. The last\\nmajor MCTS survey has been published in 2012. Contributions that appeared since\\nits release are of particular interest for this review.\\n',\n",
       "  'year': '2021'},\n",
       " {'title': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models',\n",
       "  'authors': ['Jason Wei',\n",
       "   'Xuezhi Wang',\n",
       "   'Dale Schuurmans',\n",
       "   'Maarten Bosma',\n",
       "   'Brian Ichter',\n",
       "   'Fei Xia',\n",
       "   'Ed Chi',\n",
       "   'Quoc Le',\n",
       "   'Denny Zhou'],\n",
       "  'abstract': '  We explore how generating a chain of thought -- a series of intermediate\\nreasoning steps -- significantly improves the ability of large language models\\nto perform complex reasoning. In particular, we show how such reasoning\\nabilities emerge naturally in sufficiently large language models via a simple\\nmethod called chain of thought prompting, where a few chain of thought\\ndemonstrations are provided as exemplars in prompting. Experiments on three\\nlarge language models show that chain of thought prompting improves performance\\non a range of arithmetic, commonsense, and symbolic reasoning tasks. The\\nempirical gains can be striking. For instance, prompting a 540B-parameter\\nlanguage model with just eight chain of thought exemplars achieves state of the\\nart accuracy on the GSM8K benchmark of math word problems, surpassing even\\nfinetuned GPT-3 with a verifier.\\n',\n",
       "  'year': '2022'},\n",
       " {'title': 'DayDreamer: World Models for Physical Robot Learning',\n",
       "  'authors': ['Philipp Wu',\n",
       "   'Alejandro Escontrela',\n",
       "   'Danijar Hafner',\n",
       "   'Ken Goldberg',\n",
       "   'Pieter Abbeel'],\n",
       "  'abstract': '  To solve tasks in complex environments, robots need to learn from experience.\\nDeep reinforcement learning is a common approach to robot learning but requires\\na large amount of trial and error to learn, limiting its deployment in the\\nphysical world. As a consequence, many advances in robot learning rely on\\nsimulators. On the other hand, learning inside of simulators fails to capture\\nthe complexity of the real world, is prone to simulator inaccuracies, and the\\nresulting behaviors do not adapt to changes in the world. The Dreamer algorithm\\nhas recently shown great promise for learning from small amounts of interaction\\nby planning within a learned world model, outperforming pure reinforcement\\nlearning in video games. Learning a world model to predict the outcomes of\\npotential actions enables planning in imagination, reducing the amount of trial\\nand error needed in the real environment. However, it is unknown whether\\nDreamer can facilitate faster learning on physical robots. In this paper, we\\napply Dreamer to 4 robots to learn online and directly in the real world,\\nwithout simulators. Dreamer trains a quadruped robot to roll off its back,\\nstand up, and walk from scratch and without resets in only 1 hour. We then push\\nthe robot and find that Dreamer adapts within 10 minutes to withstand\\nperturbations or quickly roll over and stand back up. On two different robotic\\narms, Dreamer learns to pick and place multiple objects directly from camera\\nimages and sparse rewards, approaching human performance. On a wheeled robot,\\nDreamer learns to navigate to a goal position purely from camera images,\\nautomatically resolving ambiguity about the robot orientation. Using the same\\nhyperparameters across all experiments, we find that Dreamer is capable of\\nonline learning in the real world, establishing a strong baseline. We release\\nour infrastructure for future applications of world models to robot learning.\\n',\n",
       "  'year': '2022'},\n",
       " {'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models',\n",
       "  'authors': ['Shunyu Yao',\n",
       "   'Dian Yu',\n",
       "   'Jeffrey Zhao',\n",
       "   'Izhak Shafran',\n",
       "   'Thomas L. Griffiths',\n",
       "   'Yuan Cao',\n",
       "   'Karthik Narasimhan'],\n",
       "  'abstract': \"  Language models are increasingly being deployed for general problem solving\\nacross a wide range of tasks, but are still confined to token-level,\\nleft-to-right decision-making processes during inference. This means they can\\nfall short in tasks that require exploration, strategic lookahead, or where\\ninitial decisions play a pivotal role. To surmount these challenges, we\\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\\nwhich generalizes over the popular Chain of Thought approach to prompting\\nlanguage models, and enables exploration over coherent units of text (thoughts)\\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\\nperform deliberate decision making by considering multiple different reasoning\\npaths and self-evaluating choices to decide the next course of action, as well\\nas looking ahead or backtracking when necessary to make global choices. Our\\nexperiments show that ToT significantly enhances language models'\\nproblem-solving abilities on three novel tasks requiring non-trivial planning\\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\\nhttps://github.com/princeton-nlp/tree-of-thought-llm.\\n\",\n",
       "  'year': '2023'},\n",
       " {'title': 'ReAct: Synergizing Reasoning and Acting in Language Models',\n",
       "  'authors': ['Shunyu Yao',\n",
       "   'Jeffrey Zhao',\n",
       "   'Dian Yu',\n",
       "   'Nan Du',\n",
       "   'Izhak Shafran',\n",
       "   'Karthik Narasimhan',\n",
       "   'Yuan Cao'],\n",
       "  'abstract': '  While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io\\n',\n",
       "  'year': '2022'},\n",
       " {'title': 'Mastering Atari Games with Limited Data',\n",
       "  'authors': ['Weirui Ye',\n",
       "   'Shaohuai Liu',\n",
       "   'Thanard Kurutach',\n",
       "   'Pieter Abbeel',\n",
       "   'Yang Gao'],\n",
       "  'abstract': \"  Reinforcement learning has achieved great success in many applications.\\nHowever, sample efficiency remains a key challenge, with prominent methods\\nrequiring millions (or even billions) of environment steps to train. Recently,\\nthere has been significant progress in sample efficient image-based RL\\nalgorithms; however, consistent human-level performance on the Atari game\\nbenchmark remains an elusive goal. We propose a sample efficient model-based\\nvisual RL algorithm built on MuZero, which we name EfficientZero. Our method\\nachieves 194.3% mean human performance and 109.0% median performance on the\\nAtari 100k benchmark with only two hours of real-time game experience and\\noutperforms the state SAC in some tasks on the DMControl 100k benchmark. This\\nis the first time an algorithm achieves super-human performance on Atari games\\nwith such little data. EfficientZero's performance is also close to DQN's\\nperformance at 200 million frames while we consume 500 times less data.\\nEfficientZero's low sample complexity and high performance can bring RL closer\\nto real-world applicability. We implement our algorithm in an\\neasy-to-understand manner and it is available at\\nhttps://github.com/YeWR/EfficientZero. We hope it will accelerate the research\\nof MCTS-based RL algorithms in the wider community.\\n\",\n",
       "  'year': '2021'},\n",
       " {'title': 'ToolChain*: Efficient Action Space Navigation in Large Language Models\\n  with A* Search',\n",
       "  'authors': ['Yuchen Zhuang',\n",
       "   'Xiang Chen',\n",
       "   'Tong Yu',\n",
       "   'Saayan Mitra',\n",
       "   'Victor Bursztyn',\n",
       "   'Ryan A. Rossi',\n",
       "   'Somdeb Sarkhel',\n",
       "   'Chao Zhang'],\n",
       "  'abstract': '  Large language models (LLMs) have demonstrated powerful decision-making and\\nplanning capabilities in solving complicated real-world problems. LLM-based\\nautonomous agents can interact with diverse tools (e.g., functional APIs) and\\ngenerate solution plans that execute a series of API function calls in a\\nstep-by-step manner. The multitude of candidate API function calls\\nsignificantly expands the action space, amplifying the critical need for\\nefficient action space navigation. However, existing methods either struggle\\nwith unidirectional exploration in expansive action spaces, trapped into a\\nlocally optimal solution, or suffer from exhaustively traversing all potential\\nactions, causing inefficient navigation. To address these issues, we propose\\nToolChain*, an efficient tree search-based planning algorithm for LLM-based\\nagents. It formulates the entire action space as a decision tree, where each\\nnode represents a possible API function call involved in a solution plan. By\\nincorporating the A* search algorithm with task-specific cost function design,\\nit efficiently prunes high-cost branches that may involve incorrect actions,\\nidentifying the most low-cost valid path as the solution. Extensive experiments\\non multiple tool-use and reasoning tasks demonstrate that ToolChain*\\nefficiently balances exploration and exploitation within an expansive action\\nspace. It outperforms state-of-the-art baselines on planning and reasoning\\ntasks by 3.1% and 3.5% on average while requiring 7.35x and 2.31x less time,\\nrespectively.\\n',\n",
       "  'year': '2023'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
