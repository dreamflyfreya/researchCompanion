paper_content = """## Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\n\n**Andy Zhou** [1 2] **Kai Yan** [1] **Michal Shlapentokh-Rothman** [1] **Haohan Wang** [1] **Yu-Xiong Wang** [1]\n\n\n**Abstract**\n\n\nWhile language models (LMs) have shown potential across a range of decision-making tasks,\ntheir reliance on simple acting processes limits\ntheir broad deployment as autonomous agents. In\nthis paper, we introduce Language Agent Tree\nSearch (LATS) – the first general framework that\n_synergizes the capabilities of LMs in reasoning,_\nacting, and planning. By leveraging the in-context\nlearning ability of LMs, we integrate Monte Carlo\nTree Search into LATS to enable LMs as agents,\nalong with LM-powered value functions and\nself-reflections for proficient exploration and enhanced decision-making. A key feature of our approach is the incorporation of an environment for\nexternal feedback, which offers a more deliberate\nand adaptive problem-solving mechanism that surpasses the constraints of existing techniques. Our\nexperimental evaluation across diverse domains,\nincluding programming, interactive questionanswering (QA), web navigation, and math, validates the effectiveness and generality of LATS\nin decision-making while maintaining competitive or improved reasoning performance. Notably, LATS achieves state-of-the-art pass@1 accuracy (92.7%) for programming on HumanEval\nwith GPT-4 and demonstrates gradient-free performance (average score of 75.9) comparable to\ngradient-based fine-tuning for web navigation on\nWebShop with GPT-3.5. Code can be found\nat [https://github.com/lapisrocks/](https://github.com/lapisrocks/LanguageAgentTreeSearch)\n[LanguageAgentTreeSearch.](https://github.com/lapisrocks/LanguageAgentTreeSearch)\n\n**1. Introduction**\n\n\n_Figure 1. Overview of LATS. Serving as a unified framework,_\nLATS leverages an external environment and an MCTS-based\nsearch algorithm to improve reasoning and decision-making.\n\nand Jennings, 1995) have been of longstanding interest in\nthe field of artificial intelligence. While this has traditionally been studied in reinforcement learning, the recent rise\nof language models (LMs) (Brown et al., 2020; Chowdhery et al., 2023; Touvron et al., 2023; OpenAI, 2023) with\nstrong reasoning and general adaptability offers an alternative paradigm. Not only have LMs excelled in standard\nnatural language processing (NLP) tasks such as summarization (Nallapati et al., 2016) and language inference (Bowman et al., 2015), but they have also been adapted to an\nincreasingly diverse set of tasks that often require advanced\ncommon-sense reasoning or quantitative skills (Cobbe et al.,\n2021; Saparov and He, 2023). In addition, LMs are capable\nof performing in complex environments that involve knowledge and reasoning, such as web navigation (Yao et al.,\n2022; Deng et al., 2023), tool-use (Schick et al., 2023), and\nopen-ended games (Fan et al., 2022).\n\nReasoning and acting abilities have been further improved\nby prompting techniques that augment LMs with feedback\nor observations from an external environment, as exemplified by ReAct (Yao et al., 2023b) and other work (Gao et al.,\n2023; Shinn et al., 2023). This eliminates the need to rely entirely on the base abilities of LMs, enhancing them through\nexternal tools or semantic feedback. Despite such strengths,\nthese methods are reflexive and fall short of humans’ deliberate and thoughtful decision-making characteristics to solve\nproblems (Sloman, 1996; Evans, 2010). In particular, they\nfail to consider multiple reasoning paths or to plan ahead.\nRecent search-guided LM work (Xie et al., 2023; Yao et al.,\n2023a; Hao et al., 2023) addresses this issue by searching\nover multiple reasoning chains. While enabling planning,\n\n\nGeneral autonomous agents capable of reasoning and\ndecision-making in a variety of environments (Wooldridge\n\n1University of Illinois Urbana-Champaign. 2Lapis Labs. Correspondence to: Andy Zhou <andyz3@illinois.edu>.\n\n_Proceedings of the 41_ _[st]_ _International Conference on Machine_\n_Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by_\nthe author(s).\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n\n**2. Related Work**\n**LMs for reasoning. For LMs, reasoning involves decom-**\nposing complex inputs into sequential intermediate steps\ntowards a final answer (Cobbe et al., 2021), demonstrated\nwith chain-of-thought (CoT) prompting (Wei et al., 2022)\nand its variants (Wei et al., 2022; Kojima et al., 2022; Wang\net al., 2022). However, these methods, which create chains\nautoregressively in a single step, often suffer from error\npropagation as the number of steps increases (Guo et al.,\n2018; Chen et al., 2023b), due to compound errors. Various\nadvancements aim to mitigate this issue; some approaches,\nsuch as self-consistency (Wang et al., 2022), employ majority voting over sampled chains, while others focus on\nmulti-step decomposition, such as least-to-most prompting (Zhou et al., 2022). Recently, CoT has been improved\nwith search algorithms (Yao et al., 2023a; Hao et al., 2023;\nBesta et al., 2023) that can sample trajectories more effectively. Tree-of-thought (ToT) prompting (Yao et al., 2023a)\nuses DFS or BFS-based (depth/breadth-first) search guided\nby an LM-generated heuristic, while reasoning via planning\n(RAP) (Hao et al., 2023) uses MCTS with rollouts simulated by LMs. However, they rely solely on LM internal\nknowledge and cannot adapt to useful external feedback.\n\n**LMs for acting. The strong reasoning and common-sense**\nabilities of LMs have been further adapted for decisionmaking or acting tasks as a policy model in interactive\nenvironments. In robotics, LMs have been employed as\nhigh-level controllers of control policies (Ahn et al., 2022;\nHuang et al., 2022; Driess et al., 2023). Similar work (Baker\net al., 2022; Wang et al., 2023) has also adapted LM agents\nto complex multimodal games such as Minecraft (Guss et al.,\n2019; Fan et al., 2022). LMs are particularly useful in textbased environments (Liu et al., 2018; Shridhar et al., 2020;\nLiu et al., 2024), where acting-based prompting techniques\nsuch as ReAct (Yao et al., 2023b) have seen success. Similar to CoT, ReAct is limited by its simplicity and cannot\neffectively adapt to environment conditions. Many extensions have been proposed to address this issue, including\nself-refine (Madaan et al., 2023) and Reflexion (Shinn et al.,\n2023), which use self-improvement to enhance reasoning\nand decision-making, and AdaPlanner (Sun et al., 2023),\nwhich incorporates both positive and negative feedback.\nHowever, these methods focus on refining an individual trajectory and do not consider alternative choices at each step.\nIn addition, recent work (Huang et al., 2024) has suggested\nthat LMs cannot self-correct their internal reasoning, making it critical to use external feedback. Alternatively, to pure\ndecision-making environments, the reasoning and practical\nabilities of LMs have been enhanced by providing access\nto external tools, such as APIs, search engines, calculators,\nand other models (Schick et al., 2023; Shen et al., 2023;\nSur´ıs et al., 2023). We summarize prior work in Tab. 1.\n\n**Tree-based search. Tree-based search, where multiple**\n\n\nsuch methods operate in isolation, lacking the incorporation\nof external feedback that can improve reasoning.\n\nTo overcome these challenges, we propose Language Agent\nTree Search (LATS) – a unified framework for decisionmaking and reasoning with language models. As illustrated\nin Fig. 1, LATS synergizes LM reasoning, acting, and plan_ning strategies by expanding ReAct (Yao et al., 2023b) into a_\nsearch over a combinatorial space of possible reasoning and\nacting steps. This effort is nontrivial – adapting search algorithms to language agents and shifting from non-interactive\ntasks to interactive ones requires a substantial novel design\non nodes, prompts, and search algorithms. In particular,\nnodes and prompts must effectively store and retrieve external feedback, with the search algorithm incorporating\nthis information into useful heuristics for value assignment.\nIndeed, our empirical evaluation, as demonstrated on HotPotQA (Yang et al., 2018) in Sec. 5.1, reveals that a simple\ncombination of existing methods is inadequate, even failing\nto surpass internal reasoning performance, despite having\naccess to the ground truth answer from the environment.\n\nOur key insight underpinning LATS is adapting Monte Carlo\nTree Search (MCTS), inspired by its success in model-based\nreinforcement learning (Silver et al., 2017) and the obser_vation that many LM tasks allow reverting to earlier steps,_\nto language agents, repurposing pretrained LMs as agents\nwith LM-powered value functions and self-reflections for\ncleverer exploration. Leveraging the general capabilities\nand in-context learning abilities of modern LMs, we use\nlanguage as an interface between each component, allowing\nLATS to adapt planning to environmental conditions with_out additional training. To the best of our knowledge, LATS_\nis the first framework that incorporates reasoning, acting,\nand planning to enhance LM performance. Notably, LATS\ndoubles the performance of ReAct (Yao et al., 2023b) on\nHotPotQA (Yang et al., 2018) and raises the average score\nby 22.1 on WebShop (Yao et al., 2022) with GPT-3.5. When\nused with GPT-4, LATS achieves a 92.7 Pass@1 rate on\nHumanEval (Chen et al., 2021), setting the state of the art.\n\nOur contributions are the following: 1) We introduce LATS,\na framework based on Monte Carlo Tree Search to construct\nthe best trajectory from sampled actions, enabling more flexible and adaptive problem-solving compared with reflexive\nprompting methods. 2) We propose a novel value function\nthat guides the search process and incorporates successful\nheuristics such as self-refinement and self-consistency. 3)\nBy integrating external feedback and self-reflection, LATS\nenhances model sensibility and enables agents to learn from\nexperience, surpassing reasoning-based search methods.\nThrough experiments across diverse domains, including programming, interactive question-answering (QA), web navigation, and math, we demonstrate the versatility of LATS\nfor enhancing autonomous reasoning and decision-making.\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n**Approach** **Reasoning** **Acting** **Planning** **Self-** **External**\n**Reflection** **Memory**\n\nCoT (Wei et al., 2022) ✓ _×_ _×_ _×_ _×_\n\nReAct (Yao et al., 2023b) ✓ ✓ _×_ _×_ _×_\n\nToT (Yao et al., 2023a) ✓ _×_ ✓ ✓ ✓\n\nRAP (Hao et al., 2023) ✓ _×_ ✓ _×_ ✓\n\nSelf-Refine (Madaan et al., 2023) ✓ _×_ _×_ ✓ _×_\n\nBeam Search (Xie et al., 2023) ✓ _×_ _×_ ✓ _×_\n\nReflexion (Shinn et al., 2023) ✓ ✓ _×_ ✓ ✓\n\n**LATS (Ours)** ✓ ✓ ✓ ✓ ✓\n\n_Table 1. Summary of related work on reasoning, acting, and planning. LATS is the first work incorporating designs from all three domains,_\nallowing broad applicability in all corresponding tasks. We refer to reasoning as LM internal reasoning, acting as external decision-making,\nplanning as the use of a search algorithm, self-reflection as the use of LM-generated feedback, and external memory as storing past text\ncontext for future updates of the solution.\n\n\nbranches of outcomes are explored during search, is widely\nused in many planning algorithms (Swiechowski et al., 2021;\nLaValle, 1998) and reinforcement learning (RL) (Hafner\net al., 2019; Du et al., 2023; Wu et al., 2023) algorithms for\nits good exploration-exploitation trade-off. Note that though\ntree-based search necessitates an environment model that\ncan expand from an arbitrary state (Vodopivec et al., 2017),\noften requiring extra training in RL (Hafner et al., 2023),\nsuch a problem does not exist for most LM tasks. This is\nbecause we can conveniently revert to any state by setting\nthe input to be the context and the corresponding previous\noutput from the LM for many tasks. Thus, we operate on the\ntree-based framework and use MCTS (Swiechowski et al.,\n2021) to fully unlock the potential of LMs. In addition, we\navoid the cost of training a value function over language\ndescriptions by leveraging the in-context learning (Brown\net al., 2020) abilities of LMs. Concurrent work (Liu et al.,\n2023) also explores combining search algorithms with LM\nagents but uses an off-the-shelf search algorithm, which\nmay not be optimal for LMs. Finally, following Yao et al.\n(2023a) and Hao et al. (2023), we note that we use planning\nand search algorithms interchangeably in this paper.\n\n**3. Preliminaries**\n\n**3.1. Problem Setting and Prompting**\n\nWe first define our problem and outline a few established\nmethods that leverage language models for reasoning or\ndecision-making. In LM reasoning or decision making,\nwe are given an input x in natural language and a pretrained language model pθ(x) parameterized by θ; our goal\nis to generate a final output y _pθ(x) that corresponds_\n_∼_\nto the answer (reasoning) or completes the task (decisionmaking). Both x and y are language sequences, which are\ncomprised of a list of tokens (the basic elements of natural\nlanguage, often words), denoted as x = (x[1], . . ., x[lx])\nand y = (y[1], . . ., y[ly]) where lx and ly are the length.\n\n\nThe LM decodes text autoregressively, i.e., without other\ninputs, the probability for an LM to generate a sequence y\nis given by pθ(x) = _i=1_ _[p][θ][(][x][[][i][]][|][x][[1][ . . . i][ −]_ [1])][. Usually,]\nto improve reasoning, prompts are provided along with the\ninput x, which are specific instructions or few-shot input-[Q][l][x]\noutput examples. We denote the generic process where an\ninput promptIO(x) is transformed into an output y by LM:\n_y_ _pθ(promptIO(x))._\n_∼_\n\n**Chain-of-thought (CoT) prompting (Wei et al., 2022)**\ncaters to scenarios where the direct mapping from x to y is\nintricate, e.g., when x is from a mathematical query or challenging question. It hinges on creating thoughts z1, . . ., zl\nthat act as stepping stones between x and y; each thought zi\nis a language sequence. To employ CoT prompting, thoughts\nare extracted sequentially as zi ∼ _p[CoT]θ_ (x, z1···i−1), with\nthe final output being y ∼ _p[CoT]θ_ (x, z1···l).\n\n**Tree-of-thought (ToT) prompting (Yao et al., 2023a) ex-**\ntends CoT prompting by exploring multiple reasoning paths\nover thoughts. It frames problems as a search over a tree,\nwhere each node s = [x, z1·i] represents a partial solution\nstate comprising the original input x and the thought sequence z1 _i. Thoughts zi are generated by proposal or_\n\n_···_\nsampling with CoT zi ∼ _p[CoT]θ_ (x, z1···i−1). Search algorithms like depth-first (DFS) or breadth-first (BFS) search\nare used to systematically explore the tree, guided by heuristics based on LM evaluations V (s) of each state.\n\n**ReAct (Yao et al., 2023b) extends language models to**\ntasks where the mapping from x to y is enhanced by or\nrequires interactions with an external environment, such\nas a game or API. This technique constructs an action\nspace _A[ˆ] = A ∪_ _Z that adds permissible actions a ∈_ _A_\nto the reasoning traces z ∈ _Z from CoT. Observations o_\nfrom the environment are used to improve both reasoning\nand acting. To solve problems with ReAct, after each observation, actions are generated from pθ sequentially as\n_ai ∼_ _p[ReAct]θ_ (x, o1···i−1, a1···i−1), with the final output be\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n\ning y ∼ _p[ReAct]θ_ (x, o1···l, a1···l). In this paper, consistent\nwith other LM agent methods such as ReAct and Reflexion\n(Shinn et al., 2023), we focus on decision-making tasks\n_where reverting between iterations is feasible._\n\nWhile the previously described prompting techniques improve LM performance on reasoning tasks, they falter on\ndifficult tasks that involve multifaceted decision-making\ndue to several shortcomings: 1) Flexibility: Base prompting\ndesigns (CoT or ReAct) autoregressively sample from the\nLM, neglecting potential alternative continuations from specific states. 2) Sensibility: Reasoning-based methods (CoT,\nRAP (Hao et al., 2023), or ToT) rely solely on the internal representations of the LM and cannot consider external\nobservations. This dependency risks fact hallucination and\nerror propagation while setting a performance ceiling. 3)\n_Adaptability: Current planning strategies (RAP or ToT) use_\nsimple search algorithms such as BFS or cannot leverage\nenvironmental feedback to improve planning. Additionally,\nthe agent is static and cannot reuse previous experience or\nlearn from trial and error. While RAP also adopts MCTS, it\nis constrained to tasks where the LM can become a world\nmodel and accurately predict states. These shortcomings\nlimit the ability of LMs to be deployed as general problemsolving agents and form the motivation for LATS.\n\n**3.2. Monte Carlo Tree Search (MCTS)**\n\nMonte Carlo Tree Search (MCTS) is a heuristic search algorithm that is proved successful on many decision-making\nenvironments, such as Atari (Ye et al., 2021) and Go (Silver\net al., 2016). MCTS builds a decision tree where every node\nin the tree is a state and edge is an action. MCTS runs for k\nepisodes; for each episode, it starts from the root (i.e., initial\nstate) and iteratively conducts two steps to expand the tree:\n1) Expansion, where multiple children states s are explored\nfrom the current parent state p by sampling n actions, and 2)\n_Selection, where the children with the highest UCT (Upper_\n_Confidence bounds applied to Trees) (Kocsis and Szepesvari´_,\n\n2006) value is selected for expansion by the next iteration.\nThe UCT of a child state s is calculated as follows:\n\n\nany step by simply copy-pasting historical text input. Such\na special property is the key motivation of our work.\n\n**4. Unifying Reasoning, Acting, and Planning**\n\n**4.1. LM Agent**\n\nDepending on the base prompting framework design, LATS\nsupports sequential reasoning or decision-making tasks. At\ntime step t, an agent receives an observation ot _O from_\n_∈_\nthe environment and takes an action at _A following some_\n_∈_\npolicy π(at|x, o1···t−1, a1···t−1). We initialize the agent\nwith pθ to leverage the useful language representations of\nan LM as a base decision-maker. We follow the ReAct instantiation, in which the action space _A[ˆ] = A ∪_ _Z consists_\nof both the space of permissible actions A and the language\nspace of reasoning traces Z. Actions directly affect the environment and result in observation, while thoughts are used\nto formalize decisions by organizing information, planning\nfuture actions, or injecting internal knowledge. The exact\ninstantiation of the action space depends on the particular\nenvironment – for decision-making tasks actions might consist of commands on a website, while for reasoning tasks\nthe action space might be limited to a few external tools or\nAPIs. In environments without feedback, such as reasoning\ntasks, we use CoT as the base prompting framework.\n\nInstead of greedily decoding one trajectory or solution, we\nsample n actions from pθ using the current state. This is\nbased on the intuition that for complex decision-making\ntasks, there is likely to be a range of potential trajectories or\nreasoning paths that are correct (Evans, 2010). Sampling a\ndiverse set of candidates at each step mitigates the stochastic\nnature of LM text generation and enables greater exploration\nin both the decision-making and reasoning space. We wrap\n_pθ within our proposed search algorithm to deliberately_\nconstruct the best trajectory from sampled actions.\n\n**4.2. LATS**\n\nThe main component of LATS is a search algorithm that\ncontrols the problem-solving process with planning. To find\nthe most promising trajectory and systemically balance exploration with exploitation, we adopt a variant of MCTS that\nframes decision-making as a tree search, in which each node\n_s = [x, a1···i, o1···i] represents a state comprising the origi-_\nnal input x, action sequence a1·i, and observation sequence\n_o1·i, where i is a token in the text sequence._\n\nOur main technical contribution is adapting MCTS to lan_guage agents. LATS repurposes pθ as an agent, state evalua-_\ntor, and feedback generator, leveraging the useful language\nrepresentations of modern LMs to facilitate planning. While\nstandard MCTS and RAP (Hao et al., 2023) rely on internal\ndynamics models to facilitate simulation, LATS uses environment interaction and does not require a world model. As\n\n\nln N (p)\n\n(1)\n_N_ (s) _[,]_\n\n\n_UCT_ (s) = V (s) + w\n\n\nwhere N (s) is the number of visits to a node s, V (s) is the\nvalue function (expected return) from the subtree of s, w is\nthe exploration weight, and p is the parent node of s. When\nthe end of an episode is reached, a backpropagation is carried out: the return r is used for updating every V (s) along\nthe path with the formula V (s) = _[V][old][(][s][)(]N[N]([(]s[s])[)][−][1)+][r]_, where\n\n_Vold(s) is the old value function. Normally, the major short-_\ncoming of MCTS is that it requires an environment model to\nundo previous steps and form a searching tree, which could\nbe a strong assumption. However, this limitation does not\nexist for many LM tasks, as we can conveniently reset to\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n\n– selection, expansion, evaluation, simulation, backpropa\n_Figure 2. Overview of the six operations in LATS. A node is selected, expanded, evaluated, then simulated until a terminal node is reached,_\nand then the resulting value is backpropagated. If the trajectory fails, a reflection is generated and used as additional context for future\ntrials. These operations are performed in succession until the budget is reached or the task is successful.\n\ndepicted in Fig. 2, LATS consists of a series of operations as it is difficult for LMs to improve their responses with\n_gation, and reflection – performed in succession until the_\ntask is successfully completed or a computational limit is\nreached after sampling k trajectories. The full pseudocode\nof LATS can be found in Sec. A in the Appendix.\n\n**Selection. In the first operation, the algorithm identifies**\na segment of the current tree most suitable for subsequent\nexpansion. Starting from the root node, denoted as the initial\nstate s0, a child node is selected at each tree level until a leaf\nnode is reached. To balance exploration and exploitation,\nwe use the UCT algorithm as shown in Eq. 1.\n\n**Expansion. After selecting a node, the second operation**\nexpands the tree by sampling n actions from pθ, as described\nin the prior section. The environment receives each action\nand returns corresponding feedback as an observation. This\nresults in n new child nodes added to the tree. This tree is\nstored in an external long-term memory structure.\n\n**Evaluation. The third operation assigns a scalar value to**\neach new child node for selection and backpropagation.\nThis value effectively quantifies the agent’s progress in task\ncompletion, serving as a heuristic to steer the search algorithm towards the most promising regions of the tree. As\nLATS does not involve training, we propose a novel value\nfunction for this setting based on two components: (1) a\n_self-generated LM score and (2) a self-consistency score._\n\nInspired by ToT, we repurpose pθ into a value function by\nprompting it to reason about a given state. To obtain a scalar\nvalue, we instruct pθ to end its reasoning trace with a score\nindicating the correctness of the trajectory. Our key distinction from ToT is that we obtain this value after obtaining\nthe environmental feedback, improving value assignment.\nThis also enables scaling to more challenging environments,\n\n\nas it is difficult for LMs to improve their responses without external feedback (Huang et al., 2024). Additionally,\nto further improve value assignment, we introduce an additional heuristic based on self-consistency (Wang et al.,\n2022), in which actions sampled multiple times at the same\nstate tend to be more accurate. This results in the overall\nvalue function:\n\n_V (s) = λ ∗_ LM(s) + (1 − _λ) ∗_ SC(s), (2)\n\nwhere λ is a hyperparameter. Notably, our method offers\nenhanced flexibility over programmed heuristics (Campbell\net al., 2002) and greater efficiency than learned heuristics\n(Silver et al., 2017).\n\n**Simulation. The fourth operation expands the currently se-**\nlected node until a terminal state is reached. At each depth\nlevel, we sample and evaluate nodes with the same operations but prioritize nodes of the highest value. Reaching a\nterminal state provides objective feedback on the correctness of a trajectory. If the task is completed successfully,\nthen LATS terminates the search. If the solution is partially\nsuccessful or unsuccessful, then we perform two additional\noperations as described below. The success of a trajectory is\ndetermined by the design of the specific environment, such\nas finalizing a purchase in web navigation environments.\n\n**Backpropagation. This operation updates the values of the**\ntree based on the outcome of a trajectory. For each node\n_s0, s1, . . ., sl in the trajectory from root (initial state s0)_\nof the searching tree to leaf (terminal state sl), its value is\nupdated to reflect the outcome of the simulation by N (si) =\n_N_ (si−1)+1 and V (si) = _[V][ (][s][i][−][1]N[)][N](s[(]i[s])[i][−][1][)+][r]_, where r is the\n\nreward. These updated values are used in the UCT formula\n(Eq. 1) to guide the selection of the next node.\n\n**Reflection. In addition to the environmental feedback, we**\nleverage self-reflection to further refine the decision-making\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n\n|Prompt Method|HotpotQA (EM) ↑|\n|---|---|\n\n\n|Base LM CoT (Wei et al., 2022) CoT - SC (Wang et al., 2022) ToT (Yao et al., 2023a) RAP (Hao et al., 2023) RAP (n = 10) LATS (CoT)|0.32 0.34 0.38 0.55 0.60 0.60 0.62|\n|---|---|\n\n\n_Table 2. GPT-3.5 reasoning-based prompting results on HotpotQA._\nLATS achieves the highest exact match (EM) for reasoning. We\nsample n = 5 nodes during expansion and k = 50 trajectories.\n\nprocess (Shinn et al., 2023; Madaan et al., 2023). Upon\nencountering an unsuccessful terminal node, pθ is prompted\nwith the trajectory and final reward to provide a verbal selfreflection that summarizes the errors in the reasoning or\nacting process and proposes superior alternatives. We store\nboth failed trajectories and corresponding reflections in the\nmemory. In subsequent iterations, these are integrated as\nadditional context to the agent and value function, refining\nboth through in-context learning. This imparts a semantic\ngradient signal more useful than a scalar value, enabling\nthe agent to learn from trial and error without the cost of\nexpensive optimization such as reinforcement learning.\n\n**Discussion. Conceptually, LATS has several notable advan-**\ntages as a general framework for reasoning and decisionmaking with LM agents. (1) Generality: LATS supports\nboth reasoning and decision-making tasks by defining a\nshared space of thoughts and actions. (2) Deliberation:\nLeveraging MCTS and LM value function in LATS ensures a principled search that selects options with high value\nwhile exploring promising alternatives. (3) Adaptability:\nIncorporating external feedback through observations and\nself-reflection in LATS enables greater adaptation during\nproblem-solving. (4) Flexibility: LATS can accommodate\ndifferent scenarios, environments, and resource stipulations\nby modifying state design and tree dimensions. (5) Modu_larity: The base LM agent, reflection generator, and value_\nfunction can be independently altered and adapted to individual LM properties.\n\n**5. Experiments**\n\nTo demonstrate the general applicability of LATS, we evaluate our method on a variety of domains that require reasoning and acting: programming (Chen et al., 2021; Austin\net al., 2022), HotPotQA (Yang et al., 2018), WebShop (Yao\net al., 2022), and Game of 24 (Yao et al., 2023a).\n\n\n|Prompt Method|HotpotQA (EM) ↑|\n|---|---|\n\n\n|ReAct (Yao et al., 2023b) ReAct (best of k) Reflexion (Shinn et al., 2023) ToT (ReAct) RAP (ReAct) LATS (ReAct) LATS (n = 3) LATS (n = 10) LATS (CoT + ReAct)|0.32 0.38 0.51 0.39 0.54 0.63 0.58 0.65 0.71|\n|---|---|\n\n\n_Table 3. GPT-3.5 acting-based prompting results on HotpotQA._\nLATS achieves the highest exact match (EM) for acting. We\nsample n = 5 nodes and use k = 50 trajectories. We also evaluate\nsampling ReAct k times and using both CoT and ReAct base\nprompting designs for LATS, which achieves the best performance.\nNote that LATS outperforms ToT and RAP with ReAct prompting,\nwhich are the simple adaptations of search algorithms to decisionmaking.\n\n**5.1. HotPotQA**\n\nFor a task that can be approached with both reasoning-based\nand acting-based strategies, we consider HotPotQA (Yang\net al., 2018), a multi-hop question-answering benchmark\nthat requires retrieval over two or more Wikipedia passages.\nFor the action space, in addition to LM thoughts, we follow\nthe setup from Yao et al. (2023b), which provides the agent\nwith API calls to search and retrieve information. The output\nof these API calls and self-generated reflections form the\nobservation space. Note that consistent with previous work\n(Yao et al., 2023b; Shinn et al., 2023), we use an oracle\nsetup for HotPotQA, in which the environment provides\nfeedback about the answer’s correctness upon receiving an\nanswer. This enables a fair comparison between our method\nand baselines in scenarios where the quality of feedback is\nhigh, allowing us to focus our evaluation on how well the\nagent incorporates external feedback. We use a subset of\n100 questions and three few-shot examples for each method.\nFor ToT, we use DFS as the base search algorithm. For all\nmethods that involve sampling, including LATS, we sample\n_k = 50 trajectories. More details are in Appendix Sec. D._\n\nWe evaluate internal reasoning strategies by removing actions and observations from the context, corresponding to\nCoT (Wei et al., 2022) and its variants, CoT-SC (Wang et al.,\n2022), ToT (Yao et al., 2023a), and RAP (Hao et al., 2023).\nThese methods rely solely on the agent’s existing knowledge\nto answer the question. We further consider acting-based\nmethods ReAct, Reflexion, and LATS, which augment the\nagent with the interactive API environment and primarily\nevaluate its information retrieval abilities. We also design\na simple integration of search algorithms with LM agents,\nextending ToT and RAP with ReAct prompting to handle\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n\n|Prompt Method Model|Pass@1 ↑|\n|---|---|\n\n\n|Prompt Method Model|Pass@1 ↑|\n|---|---|\n|CoT (Wei et al., 2022) GPT-3.5 ReAct (Yao et al., 2023b) GPT-3.5 Reflexion (Shinn et al., 2023) GPT-3.5 ToT (Yao et al., 2023a) GPT-3.5 RAP (Hao et al., 2023) GPT-3.5 LATS (ReAct) GPT-3.5|46.9 56.9 68.1 54.4 63.1 83.8|\n|Base LM GPT-4 Reflexion GPT-4 LATS (ReAct) GPT-4|80.1 91.0 92.7|\n\n\n_Table 4. GPT-3.5 and GPT-4 Pass@1 accuracy on HumanEval._\nPrompting with LATS achieves the best performance. We sample\n5 solutions during expansion for 8 iterations.\n\nexternal observations. In addition, while LATS is designed\nfor scenarios where external feedback can enhance reasoning, we also implement a reasoning-only version with CoT\nas the base prompting framework. Moreover, we combine\ninternal and external reasoning in LATS by first prompting\nwith a CoT-based prompt and then switching to a ReActbased prompt upon failure. This is closer to how humans\nmight approach this task by using tools to retrieve additional\ninformation only when the answer is not already known.\n\n**Results. We observe in Tab. 2 and Tab. 3 that both in-**\nternal reasoning and external retrieval strategies perform\nwell on HotPotQA. Due to their large-scale training corpus,\nmodern LMs already encode factual knowledge and can\noften directly answer the question correctly. While CoT can\nslightly enhance performance on questions requiring reasoning, larger gains are observed with search methods ToT\nand RAP (Tab. 2, Row 4, 5), which can sample and explore\nmore outputs. We observe similar results for acting-based\nmethods. LATS surpasses ReAct, even when sampling the\nsame number of trajectories, by expanding more nodes with\nprincipled search. This is demonstrated when modifying\n_n, the number of nodes expanded during each iteration. In-_\ncreasing n can consistently improve performance, although\nat greater computational and inference costs. LATS also\noutperforms RAP on internal reasoning, but has higher performance on the decision-making setting of HotPotQA than\nthe reasoning setting. Contrary to LATS, the ReAct versions\nof ToT and RAP (Tab. 3, Row 4, 5) perform even worse than\n_the reasoning-only setting of HotPotQA, which indicates_\nthat the acting-based setting is more challenging and adap_tation of search algorithms to decision-making scenarios_\n_is non-trivial. Combining internal and external reasoning_\nin LATS results in the highest performance, indicating the\nimportance of external feedback in augmenting reasoning\neven in tasks where the base LM can already perform.\n\n\n|Prompt Method|Pass@1 ↑|\n|---|---|\n\n\n|CoT (Wei et al., 2022) ReAct (Wei et al., 2022) Reflexion (Shinn et al., 2023) ToT (Yao et al., 2023a) RAP (Hao et al., 2023) LATS (ReAct)|54.9 67.0 70.0 65.8 71.4 81.1|\n|---|---|\n\n\n_Table 5. GPT-3.5 Pass@1 accuracy on MBPP. Prompting with_\nLATS achieves the highest performance. We sample 5 solutions\nduring expansion for 8 iterations.\n\n**5.2. Programming**\n\nTo demonstrate the importance of external observations\nfor complex reasoning tasks, we evaluate the baselines\nand LATS on programming with HumanEval (Chen et al.,\n2021)[1] and MBPP (Austin et al., 2022). Both datasets measure the correctness of synthesized programs in Python from\nnatural language docstrings. We use individual solutions\nas the action space and test suite and compiler feedback as\nthe external observation. We follow Chen et al. (2023a) and\nuse an LM to generate a synthetic test suite of syntactically\nvalid “assert” statements for each question. For each step,\nthe solution is evaluated on this test suite, and the results,\nincluding successful and failed tests and compiler output,\nare added to the context as an observation.\n\nFor this task, the reasoning and acting baselines share an\naction space, but acting methods are able to incorporate\nobservations as additional context. For LATS, since each\naction corresponds to a complete solution, we skip the simulation step of LATS and directly use the percentage of\npassed tests as the backpropagated reward. We use k = 8\niterations, set the number of generated tests at 4, and sample n = 5 solutions during expansion. After the search is\ncompleted, we select the solution with the highest value and\nevaluate it on the real test suite for the pass@1 accuracy\nevaluation. More details can be found in Appendix Sec. D.\n\n**Results. Tab. 4 and Tab. 5 show that both search and seman-**\ntic feedback are crucial for better performance. Despite not\nusing observations, ToT and RAP are competitive with Reflexion. LATS has the highest performance on both datasets.\nRAP uses a search algorithm similar to LATS, which reveals\nthe importance of external feedback for difficult reasoning\ntasks such as programming. With GPT-4, using LATS sets\nthe state of the art for HumanEval, validating that LATS can\nbe used with more advanced LMs for higher performance.\n\n1Some baselines use 161 questions from HumanEval. We\nuse all 164 questions for LATS and find minimal performance\ndifferences, so we report baselines for both settings.\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n\n|Method|Score SR ↑ ↑|\n|---|---|\n\n\n|ReAct (Yao et al., 2023b) ReAct (best of k) Reflexion (Shinn et al., 2023) LATS (ReAct)|53.8 28.0 59.1 32.0 64.2 35.0 75.9 38.0|\n|---|---|\n\n\n|IL (Yao et al., 2022) IL+RL (Yao et al., 2022) Fine-tuning (Furuta et al., 2024)|59.9 29.1 62.4 28.7 67.5 45.0|\n|---|---|\n\n\n|Expert|82.1 59.6|\n|---|---|\n\n\n_Table 6. Score and success rate (SR) on WebShop. Results are_\norganized into prompting, RL-based training, and human performance. For the same number of iterations, LATS improves both\nscore and SR and surpasses RL-based training.\n\n**5.3. WebShop**\n\nFor a complex decision-making environment with practical applications, we consider WebShop (Yao et al., 2022),\nan online shopping environment composed of a website\nwith 1.18M real-world products and 12k human instructions.\nAgents must navigate a website through a variety of commands to purchase an item matching a user specification.\nWe use the preconstructed action space of search and click\ncommands and browser feedback and reflections for the\nobservation. The performance is gauged using two metrics:\nan average score, reflecting the percentage of user-specified\nattributes met by the selected product, and a success rate,\nindicating the frequency with which the chosen product fulfills all given conditions. We compare against acting-based\nprompting methods and RL-based approaches. We evaluate\non 50 instructions, expand n = 5 children for LATS, and set\n_k = 30 for LATS, ReAct (best of k), and Reflexion. More_\ndetails and prompts are in Appendix Sec. D and Sec. G.\n\n**Results. We find in Tab. 6 that GPT-3.5 with ReAct is**\ncompetitive to imitation learning (IL) and can exceed reinforcement learning techniques with stronger prompting\nstrategies. Sampling k = 30 trajectories with ReAct and\nReflexion results in a similar performance, suggesting the semantic feedback is not as helpful in complex environments\nlike WebShop. Similar to Shinn et al. (2023), we find that\ngenerated reflections are often generic and do not provide\nuseful feedback, resulting in a tendency for the agent to\nbecome stuck in local minima. However, using LATS indeed results in a noticeable improvement, indicating a more\neffective exploration for the same number of iterations.\n\n**5.4. Ablation Study and Additional Analysis**\n\nWe further test the reasoning ability of LATS on Game of 24,\nand also conduct additional experiments on HotPotQA to\ndemonstrate the effect of each component of LATS (results\n\n\n|Prompt Method|Game of 24 (Success Rate) ↑|\n|---|---|\n\n\n|CoT (Wei et al., 2022) Reflexion (Shinn et al., 2023) ToT (Yao et al., 2023a) RAP (Hao et al., 2023) LATS (CoT)|0.08 0.12 0.20 0.40 0.44|\n|---|---|\n\n\n_Table 7. Results on Game of 24 with GPT-3.5. We sample n = 5_\nnodes and k = 30 trajectories.\n\n\n|Prompt Method|HotPotQA (EM) ↑|\n|---|---|\n\n\n|ToT (ReAct) RAP (ReAct) LATS (No LM Heuristic) LATS (DFS) LATS (No Reflection) LATS (ReAct)|0.39 0.54 0.37 0.42 0.58 0.63|\n|---|---|\n\n\n_Table 8. Ablation results on LATS and baseline variants in Hot-_\nPotQA. We use ReAct as the base prompt and sample n = 5\nchildren and k = 50 trajectories. LATS requires every component\nand operation for optimal performance.\n\nshown in Tab. 8). More ablations for token consumption on\nHotPotQA are in Tab. 9 in Appendix Sec. C.\n\n**Reasoning on Game of 24. To show how LATS can be**\napplied to purely internal reasoning tasks, we additionally\nevaluate on Game of 24 (Yao et al., 2023a), a mathematical\nreasoning task where the agent must construct 24 out of a\nset of numbers and basic operations. We use CoT as the\nbase prompting design and employ the same operations as\nin other settings. We find in Tab. 7 that LATS outperforms\nprevious methods proposed specifically for reasoning. This\nis due to our proposed value function, which incorporates\nself-consistency as an additional heuristic.\n\n**Self-reflection. LATS uses self-reflection to provide addi-**\ntional semantic signals for the agent. In Tab. 8 (Row 5, 6),\nwe observe a 0.05 performance drop when self-reflection\nis removed from LATS, validating its usefulness. This is a\nsmaller gain than the 0.19 gain that Reflexion has over ReAct as shown in Tab. 3, suggesting overlap between the questions where an answer can be improved by self-reflection\nand search. This variant outperforms RAP (ReAct), reflecting our improvements to MCTS.\n\n**Search algorithm. MCTS is a more principled search algo-**\nrithm than variants like A* (Zhuang et al., 2023) or DFS and\nis the basis for observed performance gains. We observe\nthe effects of using DFS, and incorporate the LM-based\nheuristic used in ToT in which branches with low values are\npruned. This removes the selection and backpropagation\noperations, and we observe a 0.21 drop in performance in\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n|Method|Performance ↑|Sample complexity ↓|Token Consumption ↓|\n|---|---|---|---|\n|ReAct (Best k = 250) CoT-SC (n = 1, k = 250) LATS (n = 1, k = 50) ToT (ReAct, n = 5, k = 50) RAP (ReAct, n = 5, k = 50) LATS (n = 5, k = 50)|0.42 0.40 0.48 0.49 0.54 0.63|O(k) O(k) O(k) O(kn) O(kn) O(kn)|- - - 210, 215 176, 500 173, 290|\n\n\n\n_Table 9. Performance, sample complexity of different methods, average number of nodes expanded, and token consumption upon success_\nby methods with tree-based search. n is the number of children nodes expanded at every step and k is the number of trajectories. LATS\nhas the same sample complexity as other methods with tree-based search and expands less nodes upon success, which indicates lower\ntoken cost.\n\n\nRAP and 12.12 fewer nodes than ToT. These findings underscore our improvements to MCTS and adaptation to LM\nagents, resulting in a more principled and efficient search\nmechanism.\n\n**6. Conclusion**\n\nThis work introduces Language Agent Tree Search (LATS),\nthe first framework to unify reasoning, acting, and planning for enhanced LM problem-solving. LATS addresses\nkey limitations of prior prompting techniques by deliberately constructing trajectories with search algorithms, incorporating external feedback, and enabling agents to learn\nfrom experience. Our evaluation demonstrates the ability\nof LATS to harness LM capabilities for various decisionmaking tasks while maintaining its reasoning ability without\n_additional training. The proposed synergies between search,_\ninteraction, and reflection offer a versatile approach to autonomous decision-making, highlighting the potential of\nLMs as generalist agents.\n\n**Limitations and future directions. LATS has two main**\nlimitations that should be considered before its application.\nFirst, it has a higher computational cost compared to simpler\nprompting methods like ReAct or Reflexion, which may\nlimit its practicality in certain situations. Second, LATS\nassumes the ability to revert to earlier states in decisionmaking environments, which may not be universally applicable in all possible environments. Despite these limitations, it is worth noting that LATS still achieves better\nperformance and efficiency compared to similar methods,\nand the number of nodes expanded at each step provides a\ntrade-off between performance and efficiency. Additionally,\nwe expect inference-time compute costs to decrease over\ntime, thereby increasing the usefulness of LATS and other\n“System-2” LM approaches. Finally, the reversion property is feasible in many real-world applications, opening up\nnew opportunities in the LM decision-making community.\nFuture directions include scaling LATS to more complex\nenvironments or multi-agent frameworks and improving efficiency to reduce costs. A more detailed discussion about\nthe limitations of LATS can be found in Appendix Sec. B.\n\n\n|Method|k HotPotQA # of Nodes ↑ ↓|\n|---|---|\n\n\n|ToT RAP LATS|10 0.34 33.97 10 0.44 31.53 10 0.44 28.42|\n|---|---|\n\n\n|ToT RAP LATS|30 0.39 47.54 30 0.50 37.71 30 0.52 34.12|\n|---|---|\n\n\n|ToT RAP LATS|50 0.49 84.05 50 0.54 70.60 50 0.61 66.65|\n|---|---|\n\n\n_Table 10. Comparison of the cost of different methods on Hot-_\nPotQA. LATS achieves the highest accuracy and the lowest average number of nodes/states required for success at various k\ntrajectories sampled.\n\nTab. 8 (Row 4) when sampling the same number of nodes\nbut outperforms ToT (ReAct). Despite also benefiting from\nground-truth feedback, LATS uses it better than ToT and\nRAP and can outperform these methods. We also find in\nTab. 8 (Row 3) that LM scoring, the main component of our\nvalue function, is crucial for leveraging external feedback\nand strong performance.\n\n**Sample complexity and token consumption. One pos-**\nsible concern of LATS is that the tree-structured search\nmight consume much more tokens than existing methods.\nTo further study the computational cost of LATS compared\nto prior methods, we examine the sample complexity (i.e.,\nasymptotic token cost) of all methods considered in this\npaper and count the average number of nodes expanded\nby our method and other tree-structured methods (ToT and\nRAP) upon successful search on HotPotQA. We present the\nresults in Tab. 9 and Tab. 10, which show that our method\nhas the same sample complexity as other tree-based search\nmethods and requires fewer overall tokens and states. The\ntoken cost gap will be even larger when taking failed trajectories into account, since our method has a higher success\nrate and reaches the computational budget limit less often.\nThis is also true when sampling a smaller number of trajectories; on average, LATS requires 3.55 fewer nodes than\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n\n**Impact Statement**\n\nLATS is a framework that enhances LM performance\nthrough interactions with an environment. This improvement in autonomous decision-making may facilitate harmful uses of LMs. On the other hand, LATS enhances interpretability and the potential for greater alignment, as it\ninvolves high-level linguistic reasoning and actions through\nseveral rounds of decision-making and reflection rather than\nrelying on autoregressive generation. Finally, enhancing the\ncapabilities of LM agents may raise security risks, such as\nexecuting malware. We encourage further research to fully\nunderstand and mitigate the risks of LMs.\n\n**Acknowledgements**\n\nWe thank Daniel Campos for useful feedback on earlier versions of this paper. This work was supported in part by NSF\nGrant 2106825, NIFA Award 2020-67021-32799, the Jump\nARCHES endowment through the Health Care Engineering\nSystems Center at Illinois and the OSF Foundation, and the\nIBM-Illinois Discovery Accelerator Institute. This work\nused NVIDIA GPUs at NCSA Delta through allocations\nCIS220014, CIS230012, and CIS230218 from the ACCESS\nprogram.\n\n**References**\n\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan\nFu, Keerthana Gopalakrishnan, Karol Hausman, Alex\nHerzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian\nIchter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano,\nKyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian,\nDmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,\nSergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter\nPastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers,\nClayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei\nXia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and\nAndy Zeng. Do as I can, not as I say: Grounding language\nin robotic affordances. In CoRL, 2022.\n\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen Jiang,\nCarrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models. In\n_NeurIPS, 2022._\n\nBowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga,\nJie Tang, Adrien Ecoffet, Brandon Houghton, Raul\nSampedro, and Jeff Clune. Video pretraining (VPT):\nLearning to act by watching unlabeled online videos. In\n_NeurIPS, 2022._\n\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Ger\n\nstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts:\nSolving elaborate problems with large language models.\n_arXiv:2308.09687, 2023._\n\nSamuel R Bowman, Gabor Angeli, Christopher Potts, and\nChristopher D Manning. A large annotated corpus for\nlearning natural language inference. In EMNLP, 2015.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,\nTom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,\nMark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam\nMcCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. Language models are few-shot learners. In\n_NeurIPS, 2020._\n\nMurray Campbell, A Joseph Hoane Jr, and Feng-hsiung\nHsu. Deep blue. Artificial intelligence, 2002.\n\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi\nLin, Jian-Guang Lou, and Weizhu Chen. CodeT: Code\ngeneration with generated tests. In ICLR, 2023a.\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde, Jared Kaplan, Harrison Edwards, Yura\nBurda, Nicholas Joseph, Greg Brockman, Alex Ray,\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy\nKhlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power,\nLukasz Kaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth\nBarnes, Ariel Herbert-Voss, William H. Guss, Alex\nNichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nAndrew Carr, Jan Leike, Joshua Achiam, Vedant Misra,\nEvan Morikawa, Alec Radford, Matthew M. Knight,\nMiles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv:2107.03374, 2021.\n\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W.\nCohen. Program of thoughts prompting: disentangling\ncomputation from reasoning for numerical reasoning\ntasks. TMLR, 2023b. ISSN 2835-8856.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao, Parker\n\n\n10\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n\nBarnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,\nEmily Reif, Nan Du, Ben Hutchinson, Reiner Pope,\nJames Bradbury, Jacob Austin, Michael Isard, Guy GurAri, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fedus,\nDenny Zhou, Daphne Ippolito, David Luan, Hyeontaek\nLim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai,\nMarie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon\nChild, Oleksandr Polozov, Katherine Lee, Zongwei Zhou,\nXuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat,\nMichele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM:\nScaling language modeling with pathways. JMLR, 24\n(240):1–113, 2023.\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark\nChen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to\nsolve math word problems. arXiv:2110.14168, 2021.\n\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel\nStevens, Boshi Wang, Huan Sun, and Yu Su. Mind2Web:\nTowards a generalist agent for the web. In NeurIPS\n_Datasets and Benchmarks Track, 2023._\n\nDanny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch,\nAakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. PaLM-E: An embodied multimodal language model. In ICML, 2023.\n\nYilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir\nNachum, Joshua B. Tenenbaum, Dale Schuurmans, and\nPieter Abbeel. Learning universal policies via text-guided\nvideo generation. In NeurIPS, 2023.\n\nJonathan St BT Evans. Intuition and reasoning: A dualprocess perspective. Psychological Inquiry, pages 313 –\n326, 2010.\n\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar,\nYuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang,\nYuke Zhu, and Anima Anandkumar. MineDojo: Building\nopen-ended embodied agents with internet-scale knowledge. In NeurIPS Datasets and Benchmarks Track, 2022.\n\nHiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo, Shixiang Shane Gu, and Izzeddin Gur. Multimodal\nweb navigation with instruction-finetuned foundation\nmodels. In ICLR, 2024.\n\n\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei\nLiu, Yiming Yang, Jamie Callan, and Graham Neubig.\nPAL: Program-aided language models. In ICML, 2023.\n\nJiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and\nJun Wang. Long text generation via adversarial training\nwith leaked information. In AAAI, 2018.\n\nWilliam H. Guss, Brandon Houghton, Nicholay Topin,\nPhillip Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. MineRL: A large-scale dataset of\nMinecraft demonstrations. In IJCAI, 2019.\n\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In ICML,\n2019.\n\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world\nmodels. arXiv:2301.04104, 2023.\n\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen\nWang, Daisy Zhe Wang, and Zhiting Hu. Reasoning\nwith language model is planning with world model. In\n_EMNLP, 2023._\n\nJie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven\nZheng, Adams Wei Yu, Xinying Song, and Denny Zhou.\nLarge language models cannot self-correct reasoning yet.\nIn ICLR, 2024.\n\nWenlong Huang, F. Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Peter R. Florence, Andy Zeng, Jonathan Tompson,\nIgor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah\nBrown, Tomas Jackson, Linda Luu, Sergey Levine, Karol\nHausman, and Brian Ichter. Inner monologue: Embodied\nreasoning through planning with language models. In\n_CoRL, 2022._\n\nLevente Kocsis and Csaba Szepesvari. Bandit based monte-´\ncarlo planning. In ECML, 2006.\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka\nMatsuo, and Yusuke Iwasawa. Large language models\nare zero-shot reasoners. In NeurIPS, 2022.\n\nSteven M. LaValle. Rapidly-exploring random trees : A\nnew tool for path planning. The Annual Research Report,\n1998.\n\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin\nShi, and Percy Liang. Reinforcement learning on web\ninterfaces using workflow-guided exploration. In ICLR,\n2018.\n\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu\nLei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men,\n\n\n11\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n\nKejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng,\nZhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun\nZhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong,\nand Jie Tang. AgentBench: Evaluating LLMs as agents.\nIn ICLR, 2024.\n\nZhihan Liu, Hao Hu, Shenao Zhang, Hongyi Guo, Shuqi\nKe, Boyi Liu, and Zhaoran Wang. Reason for future, act for now: A principled framework for autonomous LLM agents with provable sample efficiency.\n_arXiv:2309.17382, 2023._\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha\nDziri, Shrimai Prabhumoye, Yiming Yang, Shashank\nGupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter\nClark. Self-refine: Iterative refinement with self-feedback.\nIn NeurIPS, 2023.\n\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar\nGulcehre, and Bing Xiang. Abstractive text summarization using sequence-to-sequence RNNs and beyond. In\n_Special Interest Group on Natural Language Learning,_\n2016.\n\nOpenAI. GPT-4 technical report. arXiv:2303.08774, 2023.\n\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill\nQian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou,\nMark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun.\nToolLLM: Facilitating large language models to master\n16000+ real-world APIs. In ICLR, 2024.\n\nAbulhair Saparov and He He. Language models are greedy\nreasoners: A systematic formal analysis of chain-ofthought. In ICLR, 2023.\n\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language\nmodels can teach themselves to use tools. In NeurIPS,\n2023.\n\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. HuggingGPT: Solving\nAI tasks with ChatGPT and its friends in Hugging Face.\nIn NeurIPS, 2023.\n\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin\nGopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning.\nIn NeurIPS, 2023.\n\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Cotˆ e,´\nYonatan Bisk, Adam Trischler, and Matthew Hausknecht.\nALFWorld: Aligning text and embodied environments\nfor interactive learning. In ICLR, 2020.\n\n\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez,\nL. Sifre, George van den Driessche, Julian Schrittwieser,\nIoannis Antonoglou, Vedavyas Panneershelvam, Marc\nLanctot, Sander Dieleman, Dominik Grewe, John Nham,\nNal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,\nMadeleine Leach, Koray Kavukcuoglu, Thore Graepel,\nand Demis Hassabis. Mastering the game of Go with deep\nneural networks and tree search. Nature, 529:484–489,\n2016.\n\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez,\nL. Sifre, George van den Driessche, Julian Schrittwieser,\nIoannis Antonoglou, Vedavyas Panneershelvam, Marc\nLanctot, Sander Dieleman, Dominik Grewe, John Nham,\nNal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,\nMadeleine Leach, Koray Kavukcuoglu, Thore Graepel,\nand Demis Hassabis. Mastering chess and Shogi by selfplay with a general reinforcement learning algorithm.\n_arXiv:1712.01815, 2017._\n\nSteven A. Sloman. The empirical case for two systems of\nreasoning. Psychological Bulletin, 119:3–22, 1996.\n\nHaotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and\nChao Zhang. AdaPlanner: Adaptive planning from feedback with language models. In NeurIPS, 2023.\n\nD´ıdac Sur´ıs, Sachit Menon, and Carl Vondrick. ViperGPT:\nVisual inference via Python execution for reasoning. In\n_ICCV, 2023._\n\nMaciej Swiechowski, Konrad Godlewski, Bartosz Sawicki,\nand Jacek Ma’ndziuk. Monte Carlo tree search: A review of recent modifications and applications. Artificial\n_Intelligence Review, 56:2497–2562, 2021._\n\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Canton´\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana\nLiskovich, Yinghai Lu, Yuning Mao, Xavier Martinet,\nTodor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin\nNie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,\nKalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, R. Subramanian, Xia Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Rodriguez,\nRobert Stojnic, Sergey Edunov, and Thomas Scialom.\n\n\n12\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n\nLlama 2: Open foundation and fine-tuned chat models.\n_arXiv:2307.09288, 2023._\n\nTom Vodopivec, Spyridon Samothrakis, and Branko Ster.\nOn Monte Carlo tree search and reinforcement learning.\n_Journal of Artificial Intelligence Research, 60:881–936,_\n2017.\n\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,\nChaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with\nlarge language models. arXiv:2305.16291, 2023.\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. Self-consistency improves\nchain of thought reasoning in language models. In ICLR,\n2022.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of\nthought prompting elicits reasoning in large language\nmodels. In NeurIPS, 2022.\n\nMichael Wooldridge and Nicholas R Jennings. Intelligent\nagents: Theory and practice. The Knowledge Engineering\n_Review, 10:115 – 152, 1995._\n\nPhilipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter\nAbbeel, and Ken Goldberg. Daydreamer: World models\nfor physical robot learning. In CoRL, 2023.\n\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, MinYen Kan, Junxian He, and Qizhe Xie. Decomposition\nenhances reasoning via self-evaluation guided decoding.\n_arXiv:2305.00633, 2023._\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In EMNLP,\n2018.\n\nShunyu Yao, Howard Chen, John Yang, and Karthik R\nNarasimhan. WebShop: Towards scalable real-world web\ninteraction with grounded language agents. In NeurIPS,\n2022.\n\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths, Yuan Cao, and Karthik Narasimhan.\nTree of thoughts: deliberate problem solving with large\nlanguage models. In NeurIPS, 2023a.\n\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,\nKarthik Narasimhan, and Yuan Cao. ReAct: Synergizing\nreasoning and acting in language models. In ICLR, 2023b.\n\nWeirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel,\nand Yang Gao. Mastering Atari games with limited data.\nIn NeurIPS, 2021.\n\n\nDenny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan¨\nScales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting\nenables complex reasoning in large language models. In\n_ICLR, 2022._\n\nYuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor\nBursztyn, Ryan A. Rossi, Somdeb Sarkhel, and Chao\nZhang. ToolChain*: Efficient action space navigation in\nlarge language models with A* search. In ICLR, 2023.\n\n\n13\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n\n**Appendix of LATS**\n\nThe appendix is organized as follows. First in Sec. A, we\nshow the pseudocode of our proposed algorithm, LATS. In\nSec. B, we provide further discussion of the limitations of\nour method. In Sec. C, we present additional experimental\nresults. In Sec. D, we specify the environment details in our\nexperiments. Finally, we list our prompts used for the three\nenvironments in Sec. E (HotPotQA), Sec. F (Programming),\nand Sec. G (WebShop), respectively.\n\n**A. LATS Pseudocode**\n\nAlg. 1 shows the pseudocode of our algorithm LATS. Nodes\nare stored explicitly in the memory. Unless otherwise specified, in all experiments, we set the number of sampled nodes\nto n = 5 and the exploration weight to w = 1. We use\na self-consistency weight of λ = 0.5 for HotPotQA and\nGame of 24, and λ = 0.8 for Programming and WebShop.\n\n**B. More Discussion on Limitations**\n\nAs stated in Sec. 6, LATS has two main limitations:\n\n**Computational cost. Although LATS can improve rea-**\nsoning and decision-making, this arrives at a higher computational cost relative to simpler prompting methods like\nReAct or Reflexion. However, the following facts serve as\nmitigations to this issue:\n\n-  Asymptotically, our method has the same sample complexity as ToT (Yao et al., 2023a) and RAP (Hao et al.,\n2023), but achieves better performance, expands fewer\nnodes, and uses fewer tokens on average upon success.\nThis suggests that our method is not only stronger\nin problem-solving but also has higher efficiency. A\nfull analysis of the cost can be found in Tab. 9 in Appendix C.\n\n-  The number of nodes n expanded at every step provides\na natural trade-off between performance and efficiency.\nIn fact, setting n = 1 makes the method as efficient\nas ReAct (Yao et al., 2023b) with multiple trials or\nCoT-SC (Wang et al., 2022).\n\nIn general, we recommend using LATS for difficult tasks\nlike programming or for situations where performance is\nprioritized over efficiency in practice. We hope that continued advancements in LMs will reduce costs and increase\nthe applicability of LATS.\n\nAdditionally, there exists a minor cost from querying the environment, which we find to be trivial for the environments\nwe study. Most LM-based environments involve API-based\ntools, which are inexpensive and fast to use. It is also worth\n\n\nnoting that this is cheaper than the inference cost associated with using LMs as world models, as in previous search\napproaches (Hao et al., 2023; Liu et al., 2023).\n\n**Assumption of environment reversion in decision-**\n**making.** Since our method is based on Monte Carlo\nTree Search and is model-free, one limitation of LATS on\ndecision-making tasks is that it requires the agent to be\nable to revert to earlier states in the environments. However, this reversion property is feasible in many real-world\nenvironments and applications (despite being not universally applicable in all possible environments), including\nprogramming (HumanEval (Chen et al., 2021)), web search\n(WebShop (Yao et al., 2022)), text-based manipulation tasks\n(Alfworld (Shridhar et al., 2020)), and LMs with tool use\n(ToolBench (Qin et al., 2024)). Therefore, we believe that\nleveraging the reversion property is not a shortcoming but\nrather a feature that has not been explicitly given notice\n_by the LM decision-making community – it opens up new_\nopportunities in the emerging LM agent community.\n\nAdditionally, the benchmarks we use in this paper are relatively simple and focused on decision-making compared\nto the complexity of real-world interactive environments.\nMoreover, some environments might not easily support rollbacks to previous states. However, the design of LATS is\nflexible and can be adjusted to various resource constraints.\nUsing planning-based prompting methods like LATS in\nenvironments like Minecraft (Fan et al., 2022) and more reasoning benchmarks would be interesting avenues for future\nwork.\n\n**C. Additional Ablations**\n\nIn this section, we ablate various designs of LATS. Experiments are conducted on HotPotQA with a maximum\nof k = 50 trajectories and sampling size of n = 5 and\nHumanEval with a maximum of k = 8 trajectories and sampling size of n = 5. The result for HotPotQA is shown in\nTab. 8 and HumanEval in Fig. 3.\n\n**Exploration weight. We find that there is lower perfor-**\nmance on HotPotQA when the exploration weight w in the\nselection formula is decreased to 0.5, suggesting that this\nreduces the effectiveness of the search. Increasing w to 2.0\ndoes not lead to a performance improvement, but we tend\nto observe faster convergence. The optimal setting depends\non the particular environment and complexity of the state\nspace.\n\n**Depth. In our main experiments we use a maximum depth**\nof d = 7 on HotPotQA for all methods, following previous\nwork (Yao et al., 2023b). We ablate the effect on LATS after\nreducing it to d = 4. This results in only a slight drop in\nperformance. We find that most questions can be answered\nwithin four steps, and using a greater number of steps tends\n\n\n14\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n**Algorithm 1 LATS(s, pθ, pV, pref, d, k, n, w, a, b)**\n\n**Require: Initial state s, action generator pθ, value function pV, reflection generator pref, number of generated actions n,**\ndepth limit L, number of roll-outs K, context c, exploration weight w, and value function weight λ\nInitialize action space A, observation space O\nInitialize the state-action value function pV : S × A 7→ R and visit counter N : S 7→ N to one\n**for k ←** 0, . . ., K − 1 do\n\n**for t ←** 0, . . ., L − 1 do\n\n**if st not terminal then** _▷_ Expansion & Simulation\n\n**for i ←** 1, . . ., n do\n\nSample a[(]t[i][)] _pθ(st)_\n_∼_\n\nGet o[(]t[i][)] from environment, s[(]t+1[i][)] _[←]_ [(][c]t[(][i][)][, o]t[(][i][)][, a]t[(][i][)][)][,][ c]t[(]+1[i][)] _[←]_ [(][o]t[(][i][)][, a]t[(][i][)][)]\n\nEvaluate Vt[(][i][)] _∼_ _λ ∗_ _pV (s[(]t[i][)][) + (1][ −]_ _[λ][)][ ∗]_ [SC][(][s]t[(][i][)][)] _▷_ Evaluation\n\n_V (st)_ _Vt[(][i][)]_\n_←_\n\nAdd s[(]t[i][)] to children\n\n**end for**\n\n**end if**\n**if st is terminal then** _▷_ Reflection\n\nGet r from environment\n**if r not success then**\n\nreflection ← _pref(ct)_\n_c ←_ reflection\n\n**end if**\n\n**end if**\n_at ←_ arg maxa∈e(st) _V (st) + w_ _Nln( Nst(+1st))_ _▷_ Selection\n\nGet correspondingN (st+1) _N_ (st+1 ot) + 1 from memory,h q st+1 ← (ict, ot, at), ct+1 ← (ot, at)\n_←_\n**if at is an output action then break**\n\n**end for**\n_T ←_ the actual number of steps\n**for t ←** _T −_ 1, . . ., 0 do _▷_ Backpropagation\n\n_V (st)_ _N_ (st)\n_←_ _[V][ (][s][t][)(][N]_ [(][s][t][)][−][1)+][r]\n\n**end for**\n\n**end for**\n\n\nto force the agent into local minima and rarely improves\nsuccess.\n\n**LM value function. The LM value function scores states**\nbased on expected future reward. Without this heuristic,\nthe only signal to guide search would be from environment\nrewards for completed trajectories, which are scarce and\noften binary. When we remove the evaluation operation, we\nobserve a dramatic 0.26 drop in performance.\n\n**Performance over time. To see the effects of increasing**\nthe number of trajectories sampled, we change k to different\nvalues. We conduct this experiment on HumanEval, which\nhas a more noticeable difference due to sampling less trajectories. The results are shown in Fig. 3, in which LATS\nscales better with more iterations than Reflexion.\n\n**D. Environment Details**\n\n**D.1. HotPotQA**\n\nHotPotQA (Yang et al., 2018) is a question-answering\ndataset that requires reasoning over multiple supporting\ndocuments to answer questions. It contains 113k Wikipedia\n\nbased question-answer pairs crafted by crowdworkers to\nbe diverse, multi-hop, and explainable. Questions cover a\nrange of types like entities, locations, dates, and comparison\nof shared properties between two entities. Crowdworkers\nalso provide supporting facts from the documents that justify\nthe answer. We use the HotPotQA benchmark setting with\nall the Wikipedia paragraphs to test retrieval. We use a randomly selected subset of 100 questions for our experiments\nand a maximum depth limit of 6. Fig. 4 illustrates how\nReAct and LATS work on an example task of HotPotQA,\nand gives a qualitative example on how LATS outperforms\nReAct on the task. For value function hyperparameters, we\nuse λ = 0.5 for the LM score and self-consistency score.\n\n**Action Space. We adopt the Wikipedia web API proposed**\nin Yao et al. (2023b), with three types of actions to support\ninteractive information retrieval:\n\n(1) search[entity], which returns the first 5 sentences\nfrom the corresponding entity wiki page if it exists,\nor else suggests top-5 similar entities from the Wikipedia\nsearch engine,\n\n(2) lookup[string], which returns the next sentence in\n\n\n15\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n\nif any sample passes all tests. We use all 164 problems for\nour experiments and a maximum depth limit of 8. For the\nthree questions without sample test cases, we write our own.\nFor value function hyperparameters, we use λ = 0.8 for the\nLM score and self-consistency score. For GPT-3.5 we use\nsix internal tests, while for GPT-4 we use four internal tests.\n\nThe Mostly Basic Programming Problems (MBPP) (Austin\net al., 2022) benchmark contains 974 short Python functions\ndesigned to evaluate program synthesis techniques. The\ndataset was constructed by crowdsourcing from workers\nwith basic Python knowledge. Each data point consists of\na natural language description of a programming task, a\nreference solution implementation, and three test cases for\nfunctional correctness. The natural language prompts are\ntypically short, one-sentence descriptions. Solutions cover\ncommon programming constructs including mathematical\noperations, list processing, string manipulation, and usage\nof the Python standard library. On average, solutions are 6.8\nlines of code. The dataset is also supplemented with an additional set of 426 problems that were manually verified for\nunambiguous specifications, standard function signatures,\nand accurate test cases. We use a randomly selected subset\nof 397 problems for our experiments. For value function\nhyperparameters, we use λ = 0.8 for the LM score and\nself-consistency score.\n\n**D.3. WebShop**\n\nWebShop (Yao et al., 2022) is an interactive web-based\nenvironment designed to evaluate agents on grounded\nlanguage understanding and decision-making. It simulates\nan e-commerce shopping task by providing agents with\nover 1 million real-world products scraped from Amazon,\nspanning 5 categories and 113 subcategories. These\nproducts contain rich linguistic information, with an\naverage text length of 262 words and a vocabulary size\nof 224k. In addition, there are over 800k unique product\noptions available for customization. The environment\nrenders webpages in two modes: HTML mode provides\npixel-level observations with interactive elements, while\nsimple mode converts the raw HTML into a structured text\nobservation more amenable for training agents. The action\nspace consists of query searches and button clicks, which\ntransition between 4-page types: search, results, item, and\nitem detail. Instructions are crowdsourced natural language\nspecifying product attributes and options, with a total of 12k\ncollected. Automatic rewards are computed by comparing\nthe product purchased by the agent against the attributes\nand options specified in the instruction, using both lexical\nmatching and semantic similarity metrics.\n\nThere are two evaluation metrics used in WebShop: (1) Task\n**Score defined as (100 × avg. reward), which captures the**\n\n\n|Prompt Method|HotpotQA (EM) ↑|\n|---|---|\n\n\n|LATS (w = 0.5) LATS (w = 2.0) LATS (d = 4) LATS (CoT) LATS (No LM Heuristic) LATS (w = 1.0, d = 7)|0.55 0.63 0.58 0.62 0.37 0.63|\n|---|---|\n\n\n_Table 11. Ablation results on LATS and baseline variants in Hot-_\nPotQA measured by Exact Match (EM). We test different depth d,\nexploration factor w, and versions of LATS using CoT and without\nthe LM value function. We sample n = 5 and k = 50 trajectories.\n\n_Figure 3. Performance over successive iterations on HumanEval_\nwith GPT-3.5.\n\nthe page containing string,\n\n(3) finish[answer], which finishes the current task with\nanswer.\n\nThese API calls and free-form thoughts form the action\nspace for this environment.\n\n**D.2. Programming**\n\nThe HumanEval dataset (Chen et al., 2021) is a collection\nof 164 handwritten programming problems introduced to\nevaluate the functional correctness of models for synthesizing programs from natural language descriptions. Each\nproblem includes a function signature, docstring description, reference implementation, and multiple unit tests, with\nan average of 7.7 tests per problem. The programming\ntasks assess comprehension of natural language, reasoning,\nalgorithms, and basic mathematics, at a difficulty level comparable to simple software interview questions. Pass rates\nare evaluated with the pass@k metric, where k samples are\ngenerated per problem and a problem is considered solved\n\n\n16\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n_Figure 4. Example trajectories on HotPotQA for ReAct (left) and LATS (right). LATS can sample more actions and avoid failure from_\nprevious mistakes by evaluating states with an LM to guide the search toward promising areas of the tree.\n\n\nType Argument State → Next State\n\nsearch [Query] Search → Results\nchoose Back to search _∗→_ Search\nchoose Prev/Next page Results → Results\nchoose [Product title] Results → Item\nchoose [Option] Item → Item\nchoose Desc/Overview Item → Item-Detail\nchoose Previous Item-Detail → Item\nchoose Buy Item → Episode End\n\n_Table 12. Action space of WebShop._\n\naverage reward obtained across episodes; and (2) Success\n**Rate (SR) defined as the portion of instructions where r = 1.**\nThe reward is calculated based on the number of attributes\nsatisfied by the selected item. We use 50 environments for\nour experiments and a maximum depth limit of 15. For\nvalue function hyperparameters, we use λ = 0.8 for the LM\nscore and self-consistency score.\n\n**D.4. Game of 24**\n\nGame of 24 is a mathematical reasoning challenge where\nthe goal is to use basic arithmetic operations to construct\n24 out of 4 numbers. We follow the setup from Yao et al.\n(2023a), where we measure success if the agent produces a\n\n\n|Prompt Method|Game of 24 (Success Rate) ↑|\n|---|---|\n\n\n|LATS (CoT, λ = 1) LATS (CoT)|0.40 0.44|\n|---|---|\n\n\n_Table 13. Ablations on λ in Game of 24 with GPT-3.5. λ = 0.5_\nused in the main paper outperforms λ = 1, equivalent to removing\nself-consistency, which indicates that the self-consistency score\nimproves the performance of LATS.\n\ncorrect equation that equals 24 and uses each input number\nonly once. We report success rate over 50 games. We use\n_k = 30 iterations and a maximum depth limit of 5. For value_\nfunction hyperparameters, we use λ = 0.5 for the LM score\nand self-consistency score. Tab. 13 shows the performance\ncomparison between λ = 0.5 and λ = 1, which validates\nour design of self-consistency term.\n\n**E. HotPotQA Prompts**\n\n**E.1. Base Acting Prompt**\n\nSolve a question answering task with interleaving Thought,\nAction, Observation steps. Thought can reason about the\ncurrent situation, and Action can be three types:\n\n(1) Search[entity], which searches the exact entity on\nWikipedia and returns the first paragraph if it exists. If\nnot, it will return some similar entities to search.\n\n\n17\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n\n(2) Lookup[keyword], which returns the next sentence containing keyword in the current passage.\n\n(3) Finish[answer], which returns the answer and finishes\nthe task.\n\nAfter each observation, provide the next Thought and next\nAction.\n\nHere are some examples:\n\nQuestion: Which magazine was started first Arthur’s Magazine or First for Women?\n\nThought 1: I need to search Arthur’s Magazine and First for\nWomen, and find which was started first.\n\nAction 1: Search[Arthur’s Magazine]\n\nObservation 1: Arthur’s Magazine was an American literary\nperiodical published in Philadelphia in the 19th century.\nEdited by Timothy Shay Arthur, it featured work by Edgar\nA. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G.\nSpear, and others.[1][2] In May 1846 it was merged into\nGodey’s Lady’s Book.[3]\n\nThought 2: Arthur’s Magazine was started in 1844. I need\nto search First for Women next.\n\nAction 2: Search[First for Women]\n\nObservation 2: First for Women is a woman’s magazine\npublished by Bauer Media Group in the USA.[1] The magazine was started in 1989. It is based in Englewood Cliffs,\nNew Jersey.[2] In 2011 the circulation of the magazine was\n1,310,696 copies.[3]\n\nThought 3: First for Women was started in 1989. 1844\n(Arthur’s Magazine) < 1989 (First for Women), so Arthur’s\nMagazine was started first.\n\nAction 3: Finish[Arthur’s Magazine]\n\n(examples)\n\nYou have attempted to answer the following question before\nand failed. The following reflection(s) give a plan to avoid\nfailing to answer the question in the same way you did\npreviously. Use them to improve your strategy of correctly\nanswering the given question.\n\n(trajectories)\n\n(input)\n\n**E.2. Base Reasoning Prompt**\n\nSolve a question answering task by having a Thought, then\nFinish with your answer. Thought can reason about the\ncurrent situation. Finish[answer] returns the answer and\nfinishes the task. You will be given context that you should\nuse to help you answer the question. Start your response\n\n\nwith either Action or an indexed Thought\n\nHere are some examples:\n\nQuestion: What is the elevation range for the area that the\neastern sector of the Colorado orogeny extends into?\n\nLet’s think step by step.\n\nThought 1: The eastern sector of Colorado orogeny extends\ninto the High Plains.\n\nThought 2: High Plains rise in elevation from around 1,800\nto 7,000 ft\n\nThought 3: The answer is 1,800 to 7,000 ft.\n\nAction: Finish[1,800 to 7,000 ft]\n\n(examples)\n\nPrevious trial: (trajectories)\n\n(input)\n\n**E.3. Value Function Prompt**\n\nAnalyze the trajectories of a solution to a question answering\ntask. The trajectories are labeled by environmental Observations about the situation, Thoughts that can reason about\nthe current situation, and Actions that can be three types:\n\n(1) Search[entity], which searches the exact entity on\nWikipedia and returns the first paragraph if it exists. If\nnot, it will return some similar entities to search.\n\n(2) Lookup[keyword], which returns the next sentence containing keyword in the current passage.\n\n(3) Finish[answer], which returns the answer and finishes\nthe task.\n\nGiven a question and a trajectory, evaluate its correctness\nand provide your reasoning and analysis in detail. Focus\non the latest thought, action, and observation. Incomplete\ntrajectories can be correct if the thoughts and actions so\nfar are correct, even if the answer is not found yet. Do not\ngenerate additional thoughts or actions. Then at the last line\nconclude “Thus the correctness score is s”, where s is an\ninteger from 1 to 10.\n\nQuestion: Which magazine was started first Arthur’s Magazine or First for Women?\n\nThought 1: I need to search Arthur’s Magazine and First for\nWomen, and find which was started first.\n\nAction 1: Search[Arthur’s Magazine]\n\nObservation 1: Arthur’s Magazine was an American literary\nperiodical published in Philadelphia in the 19th century.\nEdited by Timothy Shay Arthur, it featured work by Edgar\nA. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G.\n\n\n18\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n\nSpear, and others.[1][2] In May 1846 it was merged into\nGodey’s Lady’s Book.[3]\n\nThis trajectory is correct as it is reasonable to search for\nthe first magazine provided in the question. It is also better\nto have simple searches corresponding to a single entity,\nmaking this the best action.\n\nThus the correctness score is 10\n\n(other examples)\n\n(failed trajectories)\n\n(context)\n\n**E.4. Reflection Prompt**\n\nAnalyze the trajectories of a solution to a questionanswering task. The trajectories are labeled by environmental Observations about the situation, Thoughts that can\nreason about the current situation, and Actions that can be\nthree types:\n\n(1) Search[entity], which searches the exact entity on\nWikipedia and returns the first paragraph if it exists. If\nnot, it will return some similar entities to search.\n\n(2) Lookup[keyword], which returns the next sentence containing keyword in the current passage.\n\n(3) Finish[answer], which returns the answer and finishes\nthe task.\n\nGiven a question and a trajectory, evaluate its correctness\nand provide your reasoning and analysis in detail. Focus\non the latest thought, action, and observation. Incomplete\ntrajectories can be correct if the thoughts and actions so\nfar are correct, even if the answer is not found yet. Do not\ngenerate additional thoughts or actions. Then at the last line\nconclude “Thus the correctness score is s”, where s is an\ninteger from 1 to 10.\n\nQuestion: Which magazine was started first Arthur’s Magazine or First for Women?\n\nThought 1: I need to search Arthur’s Magazine and First for\nWomen, and find which was started first.\n\nAction 1: Search[Arthur’s Magazine]\n\nObservation 1: Arthur’s Magazine was an American literary\nperiodical published in Philadelphia in the 19th century.\nEdited by Timothy Shay Arthur, it featured work by Edgar\nA. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G.\nSpear, and others.[1][2] In May 1846 it was merged into\nGodey’s Lady’s Book.[3]\n\nThis trajectory is correct as it is reasonable to search for\nthe first magazine provided in the question. It is also better\nto have simple searches corresponding to a single entity,\n\n\nmaking this the best action.\n\nThus the correctness score is 10\n\n(other examples)\n\n(failed trajectories)\n\n(context)\n\n**F. Programming Prompts**\n\n**F.1. HumanEval function implementation example**\n\nSample function signature:\n\ndef minSubArraySum ( nums ) :\nGiven an a r r a y of i n t e g e r s nums,\nf i n d the minimum sum of any\nnon−empty sub −a r r a y of nums .\nExample\nminSubArraySum ([ −1, −2, −3]) == −6\n\nSample function body implementation:\n\nmin sum = f l o a t ( ’ inf ’ )\nf o r i in range ( len ( nums ) ) :\ncurrent sum = 0\nf o r j in range ( i, len ( nums ) ) :\ncurrent sum += nums [ j ]\ni f current sum < min sum :\nmin sum = current sum\nr e t u r n min sum\n\n**F.2. Base Acting/Reasoning Prompt**\n\nYou are an AI Python assistant. You will be given your\nprevious implementation of a function, a series of unit tests\nresults, and your self-reflection on your previous implementation. Write your full implementation (restate the function\nsignature).\n\nExample 1:\n\n[previous impl]:\n\ndef add ( a : int, b : i n t ) −> i n t :\n‘ ‘ Given i n t e g e r s a and b,\nr e t u r n the t o t a l value of a and b . ’ ’\nr e t u r n a −b\n\n[unit test results from previous impl]:\n\nTested passed:\n\nTests failed:\n\nassert add(1, 2) == 3 # output: -1\n\n\n19\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n\nassert add(1, 2) == 4 # output: -1\n\n[reflection on previous impl]:\n\nThe implementation failed the test cases where the input\nintegers are 1 and 2. The issue arises because the code does\nnot add the two integers together, but instead subtracts the\nsecond integer from the first. To fix this issue, we should\nchange the operator from ‘-’ to ‘+’ in the return statement.\nThis will ensure that the function returns the correct output\nfor the given input.\n\n[improved impl]:\n\ndef add ( a : int, b : i n t ) −> i n t :\n‘ ‘\nGiven i n t e g e r s a and b,\nr e t u r n the t o t a l value of a and b .\n’ ’\nr e t u r n a + b\n\n**F.3. Reflection Prompt**\n\nYou are a Python programming assistant. You will be given\na function implementation and a series of unit test results.\nYour goal is to write a few sentences to explain why your\nimplementation is wrong, as indicated by the tests. You\nwill need this as guidance when you try again later. Only\nprovide the few sentence description in your answer, not the\nimplementation. You will be given a few examples by the\nuser.\n\nExample 1:\n\n[previous impl]:\n\ndef add ( a : int, b : i n t ) −> i n t :\n‘ ‘\nGiven i n t e g e r s a and b,\nr e t u r n the t o t a l value of a and b .\n’ ’\nr e t u r n a −b\n\n[unit test results from previous impl]:\n\nTested passed:\n\nTests failed:\n\nassert add(1, 2) == 3 # output: -1\n\nassert add(1, 2) == 4 # output: -1\n\n[reflection on previous impl]:\n\nThe implementation failed the test cases where the input\nintegers are 1 and 2. The issue arises because the code does\nnot add the two integers together, but instead subtracts the\nsecond integer from the first. To fix this issue, we should\nchange the operator from ‘-’ to ‘+’ in the return statement.\n\n20\n\n\nThis will ensure that the function returns the correct output\nfor the given input.\n\n**F.4. Test Case Generation Prompt**\n\nYou are an AI coding assistant that can write unique, diverse,\nand intuitive unit tests for functions given the signature and\ndocstring.\n\n(input)\n\n**G. WebShop Prompts**\n\n**G.1. Acting Prompt**\n\nWebshop\n\nInstruction:\n\ni would like a 3 ounce bottle of bright citrus deodorant for\nsensitive skin, and price lower than 50.00 dollars\n\n[Search]\n\nAction: search[3 ounce bright citrus deodorant sensitive\nskin]\n\nObservation:\n\n[Back to Search]\n\nPage 1 (Total results: 50)\n\n[Next ] [B078GWRC1J]\n\nBright Citrus Deodorant by Earth Mama — Natural and Safe\nfor Sensitive Skin, Pregnancy and Breastfeeding, Contains\nOrganic Calendula 3-Ounce\n\n$10.99\n\n[B078GTKVXY]\n\nGinger Fresh Deodorant by Earth Mama — Natural and Safe\nfor Sensitive Skin, Pregnancy and Breastfeeding, Contains\nOrganic Calendula 3-Ounce\n\n$10.99\n\n[B08KBVJ4XN]\n\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant\nfor Men, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar & Patchouli Blend, Gentle on Sensitive Skin\n(Mountain Sage, 2.7 oz, 2-Pack)\n\n$15.95\n\nAction: think[B078GWRC1J and B078GTKVXY are\nbright citrus deodorant less then 50 dollars. I can check\nB078GWRC1J first.]\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n\nObservation: OK.\n\nAction: click[B078GWRC1J]\n\nObservation:\n\n[Back to Search]\n\n[Prev]\n\nscent [assorted scents][bright citrus][calming lavender][ginger fresh][simply non-scents] size [travel set (4pack)][3 ounce (pack of 1)][3-ounce (2-pack)] Bright Citrus\nDeodorant by Earth Mama — Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\n\nPrice: $10.99\n\nRating: N.A.\n\n[Description]\n\n[Features]\n\n[Reviews]\n\n[Buy Now]\n\nAction: think[For 3 ounce bottle of bright citrus deodorant\nfor sensitive skin, the item has options ‘bright citrus’ and ‘3\nounce (pack of 1)’ and seems good to buy.]\n\nObservation: OK.\n\nAction: click[bright citrus]\n\nObservation: You have clicked bright citrus.\n\nAction: click[3 ounce (pack of 1)]\n\nObservation: You have clicked 3 ounce (pack of 1).\n\nAction: click[Buy Now]\n\n**G.2. Value Function Prompt**\n\nGiven an item to purchase and a trajectory that aims to buy\nan item that exactly matches the specification, which corresponds to the ideal score of 1.0, analyze the following\ntrajectory, then at the last line conclude, “Thus the correctness score is s”, where s is an integer from 1 to 10.\n\nHere are some examples\n\nWebshop\n\nInstruction:\n\n\ni am looking for dairy free and apple variety pack of chips,\nand price lower than 30.00 dollars\n\n[Search]\n\nAction: search[dairy free and apple variety pack of chips]\n\nObservation:\n\n[Back to Search]\n\nPage 1 (Total results: 50)\n\n[Next]\n\n[B07HRFJWP8]\n\nEnjoy Life Foods Soft Baked Ovals, Breakfast Bars, Nut\nFree Bars, Soy Free, Dairy Free, Non GMO, Gluten Free,\nVegan, Variety Pack, 4 Boxes (20 Bars Total)\n\n$100.0\n\n[B01KMHY5PG]\n\nEnjoy Life Soft Baked Chewy Bars, Variety Pack, Nut Free\nBars, Soy Free, Dairy Free, Gluten Free, 6 Boxes (30 Total\nBars)\n\n$21.49\n\n[B008D2X8C4]\n\nEnjoy Life Lentil Chips Variety Pack, Dairy Free Chips, Soy\nFree, Nut Free, Non GMO, Vegan, Gluten Free, 24 Bags\n(0.8 oz)\n\n$100.0\n\nAction: think[The first two items are not what I want. The\nthird item is the dairy free and apple variety pack of chips I\nwant.]\n\nObservation: OK.\n\nAction: click[B008D2X8C4]\n\nObservation:\n\n[Back to Search]\n\n[Prev]\n\nflavor name [dill and sour cream][garlic & parmesan][light\nsea salt][margherita pizza][thai chili lime][variety pack] size\n\n[0.8 ounce (pack of 24)][4 ounce (pack of 12)]\n\nPrice: $100.0\n\nRating: N.A.\n\n[Description]\n\n[Features]\n\n\n21\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\n\n\n[Reviews]\n\n[Buy Now]\n\nAction: think[The item has options ‘variety pack’ and ‘0.8\nounce (pack of 24)’ and seems good to buy.]\n\nObservation: OK.\n\nAction: click[variety pack]\n\nObservation: You have clicked variety pack.\n\nAction: click[0.8 ounce (pack of 24)]\n\nObservation: You have clicked 0.8 ounce (pack of 24).\n\nAction: click[Buy Now]\n\nReflection: In this attempt, I was unsuccessful. I accidentally bought a product that was $100, which is more than my\nbudget of $30. Either way, the initial search results were not\ngood. Next time, I will do search[“variety pack of chip”;]\nand then check if the results meet the dairy free and the $30\nbudget constraints. I will continue to refine my searches so\nthat I can find more products.\n\nThus the correctness score is 5\n\n(input)\n\n**G.3. Reflection Prompt**\n\nYou are an advanced reasoning agent that can improve based\non self-reflection. You will be given a previous reasoning\ntrial in which you were given access to a shopping website\nand a specific type of item to buy. You were given access\nto relevant context and an item to purchase. You were unsuccessful in buying the correct item either because you did\nnot find an item meeting all of the required specifications\nor because you did not select the correct item. The ideal\nscore is 1.0, and anything less is incorrect. In a few sentences, Diagnose a possible reason for failure and devise a\nnew, concise, high-level plan that aims to mitigate the same\nfailure. Use complete sentences. Here are some examples:\n\nPrevious Trial Instruction: i am looking for dairy free and\napple variety pack of chips, and price lower than 30.00\ndollars [Search]\n\nAction: search[dairy free and apple variety pack of chips]\nObservation: [Back to Search] Page 1 (Total results: 50)\n\n[Next >] [B07HRFJWP8] Enjoy Life Foods Soft Baked\nOvals, Breakfast Bars, Nut Free Bars, Soy Free, Dairy Free,\nNon GMO, Gluten Free, Vegan, Variety Pack, 4 Boxes\n\n\n(20 Bars Total) $100.0 [B01KMHY5PG] Enjoy Life Soft\nBaked Chewy Bars, Variety Pack, Nut Free Bars, Soy Free,\nDairy Free, Gluten Free, 6 Boxes (30 Total Bars) $21.49\n\n[B008D2X8C4] Enjoy Life Lentil Chips Variety Pack, Dairy\nFree Chips, Soy Free, Nut Free, Non GMO, Vegan, Gluten\nFree, 24 Bags (0.8 oz) $100.0\n\nAction: think[The first two items are not what I want. The\nthird item is the dairy free and apple variety pack of chips I\nwant.] Observation: OK.\n\nAction: click[B008D2X8C4] Observation: [Back to Search]\n\n[< Prev] flavor name [dill and sour cream][garlic & parmesan][light sea salt][margherita pizza][thai chili lime][variety\npack] size [0.8 ounce (pack of 24)][4 ounce (pack of 12)]\nPrice: $100.0 Rating: N.A. [Description] [Features] [Reviews] [Buy Now]\n\nAction: think[The item has options ‘variety pack’ and ‘0.8\nounce (pack of 24)’ and seems good to buy.] Observation:\nOK.\n\nAction: click[variety pack] Observation: You have clicked\nvariety pack.\n\nAction: click[0.8 ounce (pack of 24)] Observation: You\nhave clicked 0.8 ounce (pack of 24).\n\nAction: click[Buy Now]\n\nSTATUS: FAIL\n\nReflection: In this attempt, I was unsuccessful. I accidentally bought a product that was $100, which is more than my\nbudget of $30. Either way, the initial search results were not\ngood. Next time, I will do search[“variety pack of chips”]\nand then check if the results meet the dairy free and the $30\nbudget constraints. I will continue to refine my searches so\nthat I can find more products.\n\nPrevious Trial Instruction: i need gluten free vegetarian\nsmoked peppered bacon - 4 ounce (pack of 2), and price\nlower than 40.00 dollars [Search]\n\nAction: search[gluten free vegetarian smoked peppered bacon 4 ounce pack of 2] Observation: [Back to Search] Page\n1 (Total results: 50) [Next >] [B06Y96MXJV] Smoked\nBacon Sea Salt 3-Pack: Smoked Bacon Chipotle, Smoked\nBacon and Onion, Smoked Peppered Bacon - All-Natural\nSea Salts Slowly Smoked Over Alderwood - No Gluten, No\nMSG, Non-GMO (12 total oz.) $29.99 [B06Y96N1KG]\nSpicy Hot Pepper Sea Salt 3-Pack: Ghost Pepper, Jalapeno,\nHabanero - All Natural, Delicious Blends of Peppers & Sea\nSalt - Gluten-Free, Kosher, No MSG, Non-GMO (12 total\noz.) $29.99 [B07GJTKYJQ] Louisville Vegan Jerky - 5\nFlavor Variety Pack, Non-GMO Soy Protein, Gluten-Free\n(Black Pepper, Buffalo Dill, Pepperoni, Maple Bacon, and\nCarolina BBQ, 3 Ounces) $42.99\n\n\n22\n\n\n-----\n\n**Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models**\n\nAction: think[The first two items are not what I want. The\nthird item is not what I want, but it is the closest one.]\nObservation: OK.\n\nAction: click[B07GJTKYJQ] Observation: [Back to\nSearch] [< Prev] Louisville Vegan Jerky - 5 Flavor Variety\nPack, Non-GMO Soy Protein, Gluten-Free (Black Pepper,\nBuffalo Dill, Pepperoni, Maple Bacon, and Carolina BBQ,\n3 Ounces) Price: $42.99 Rating: N.A. [Description] [Features] [Reviews] [Buy Now]\n\nAction: think[The price is too high and it is not what I want.]\nObservation: OK.\n\nAction: click[< Prev] Observation: [Back to Search] Page\n1 (Total results: 50) [Next >] [B06Y96MXJV] Smoked\nBacon Sea Salt 3-Pack: Smoked Bacon Chipotle, Smoked\nBacon and Onion, Smoked Peppered Bacon - All-Natural\nSea Salts Slowly Smoked Over Alderwood - No Gluten,\nNo MSG, Non-GMO (12 total oz.) 29.99 [B06Y96N1KG]\nSpicy Hot Pepper Sea Salt 3-Pack: Ghost Pepper, Jalapeno,\nHabanero - All Natural, Delicious Blends of Peppers &\nSea Salt - Gluten-Free, Kosher, No MSG, Non-GMO (12\ntotal oz.) 29.99 [B07GJTKYJQ] Louisville Vegan Jerky - 5\nFlavor Variety Pack, Non-GMO Soy Protein, Gluten-Free\n(Black Pepper, Buffalo Dill, Pepperoni, Maple Bacon, and\nCarolina BBQ, 3 Ounces) 42.99\n\nAction: think[The first two items are not what I want.]\nObservation: OK.\n\nAction: click[Next >] Observation: Invalid action!\n\nAction: think[I need to refine my search.] Observation: OK.\n\nAction: click[< Back to Search] Observation: Invalid action!\n\nAction: click[Back to Search]\n\nSTATUS: FAIL\n\nReflection: In this attempt, I was unsuccessful. The initial\nsearch results were not the products that I was looking for.\nNext time, I will do search[“vegetarian bacon”] for a broader\nsearch. I will check that the new results will fulfill the gluten\nfree and 4 ounce pack of 2 constraints. I will continue to\nrefine my searches so that I can find more products.\n\nPrevious trial: trajectory Reflection:\n\n23\n\n\n-----\n\n"""

contextual_content = """# Companion Guide to "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models"

This guide is designed to help you navigate through the key concepts, methodologies, and references in the paper titled "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models". The guide is structured to follow the order and context of the original paper, providing definitions, explanations, related work, and citations at each relevant point.

## **1. Introduction**
The introduction sets the stage for the new framework, Language Agent Tree Search (LATS), highlighting its potential to enhance language models (LMs) in various decision-making tasks.

### **Keywords:**

1. **Language Agent Tree Search (LATS)**
   - **Definition:** A unified framework that integrates reasoning, acting, and planning to enhance decision-making and problem-solving with language models.
   - **Local Context:** Introduced as a framework to enhance decision-making and problem-solving with language models.

2. **Monte Carlo Tree Search (MCTS)**
   - **Definition:** A heuristic search algorithm that explores multiple branches of outcomes during search to find the best solution.
   - **Local Context:** Used as a key component in Language Agent Tree Search to guide the search process.

3. **Reasoning**
   - **Definition:** The process of drawing conclusions from available information.
   - **Local Context:** Improved through the use of Language Agent Tree Search, which enables agents to learn from experience and adapt to new situations.

4. **Acting**
   - **Definition:** The process of taking actions based on available information.
   - **Local Context:** Enhanced through the use of Language Agent Tree Search, which enables agents to make more informed decisions and adapt to new situations.

5. **Planning**
   - **Definition:** The process of selecting a sequence of actions to achieve a goal.
   - **Local Context:** Improved through the use of Language Agent Tree Search, which enables agents to plan more effectively and adapt to new situations.

### **Related Citations:**

1. **Codex and HumanEval:**
   - **Mark Chen et al., 2021:** Evaluates large language models trained on code, highlighting the importance of functional correctness and introducing datasets like HumanEval for programming tasks.
   - **Citation:** "Evaluating Large Language Models Trained on Code" by Mark Chen et al., 2021.

2. **Program Synthesis with Language Models:**
   - **Jacob Austin et al., 2022:** Explores the use of large language models for program synthesis, emphasizing the role of diverse language models and benchmarks.
   - **Citation:** "Program synthesis with large language models" by Jacob Austin et al., 2022.

## **2. Related Work**
This section discusses various advancements in LMs for reasoning, acting, and planning. It also covers shortcomings of existing methods and introduces improvements proposed by LATS.

### **Keywords:**

1. **Self-Reflection**
   - **Definition:** The process of evaluating and improving one's own performance and decision-making processes.
   - **Local Context:** Used in Language Agent Tree Search to improve the agent's performance and adapt to new situations.

2. **External Feedback**
   - **Definition:** Feedback provided by an external environment or source that can be used to improve decision-making and performance.
   - **Local Context:** Used in Language Agent Tree Search to improve the agent's performance and adapt to new situations.

3. **Value Function**
   - **Definition:** A function that assigns a value to each state or action to guide the decision-making process.
   - **Local Context:** Used in Language Agent Tree Search to guide the search process and evaluate the agent's performance.

4. **Self-Consistency**
   - **Definition:** The consistency of an agent's actions and decisions with its own knowledge and goals.
   - **Local Context:** Improved through the use of Language Agent Tree Search, which enables agents to evaluate their own performance and adapt to new situations.

### **Related Citations:**

1. **Chain-of-Thought Prompting:**
   - **Jason Wei et al., 2022:** Discusses how generating a chain of thought significantly improves the reasoning capabilities of large language models.
   - **Citation:** "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" by Jason Wei et al., 2022.

2. **Program of Thoughts Prompting:**
   - **Wenhu Chen et al., 2022:** Proposes 'Program of Thoughts' which expresses reasoning processes as a program, disentangling computation from reasoning.
   - **Citation:** "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks" by Wenhu Chen et al., 2022.

3. **Reasoning via Planning:**
   - **Shibo Hao et al., 2023:** Introduces RAP (Reasoning via Planning), which uses MCTS to perform deliberate planning for reasoning tasks.
   - **Citation:** "Reasoning with Language Model is Planning with World Model" by Shibo Hao et al., 2023.

## **3. Preliminaries**
This section defines the problem setting and outlines various established methods for leveraging LMs in reasoning or decision-making.

### **Keywords:**

1. **Deliberation**
   - **Definition:** The process of carefully considering and evaluating options before making a decision.
   - **Local Context:** Improved through the use of Language Agent Tree Search, which enables agents to make more informed decisions and adapt to new situations.

2. **Generality**
   - **Definition:** The ability of a framework or approach to be applied to a wide range of tasks and domains.
   - **Local Context:** Language Agent Tree Search is designed to be a general framework that can be applied to a wide range of tasks and domains.

### **Related Citations:**

1. **PaLM by Google Research:**
   - **Aakanksha Chowdhery et al., 2022:** Discusses the PaLM language model which scales language modeling with pathways and achieves state-of-the-art performance.
   - **Citation:** "PaLM: Scaling Language Modeling with Pathways" by Aakanksha Chowdhery et al., 2022.

2. **Monte Carlo Planning:**
   - **Steven Wooldridge and Nicholas Jennings, 1995:** Explains the theoretical basis and practical applications of intelligent agents in various environments.
   - **Citation:** "Intelligent agents: Theory and practice" by Steven Wooldridge and Nicholas Jennings, 1995.

## **4. Unifying Reasoning, Acting, and Planning**
This section provides a detailed explanation of the Language Agent Tree Search (LATS) framework and its components, including the selection, expansion, evaluation, simulation, and backpropagation operations in the MCTS algorithm.

### **Related Citations:**

1. **Monte Carlo Tree Search:**
   - **Maciej Świechowski et al., 2021:** Reviews recent modifications and applications of Monte Carlo Tree Search in planning and reinforcement learning.
   - **Citation:** "Monte Carlo Tree Search: A Review of Recent Modifications and Applications" by Maciej Świechowski et al., 2021.

2. **DeepMind's PlaNet:**
   - **Danijar Hafner et al., 2018:** Discusses the Deep Planning Network (PlaNet), which learns environment dynamics from images and uses them for planning.
   - **Citation:** "Learning Latent Dynamics for Planning from Pixels" by Danijar Hafner et al., 2018.

3. **DreamerV3:**
   - **Danijar Hafner et al., 2023:** Presents DreamerV3, an algorithm that uses a world model to solve tasks across diverse domains through imagination and robust learning techniques.
   - **Citation:** "Mastering Diverse Domains through World Models" by Danijar Hafner et al., 2023.

## **5. Experiments**
The experimental section evaluates LATS across various domains, showcasing its effectiveness and versatility in enhancing autonomous reasoning and decision-making.

### **Related Work and Usage:**

1. **ReAct:**
   - **Shunyu Yao et al., 2022:** Introduces ReAct, an approach for synergizing reasoning and acting in language models to improve performance in decision-making tasks.
   - **Citation:** "ReAct: Synergizing Reasoning and Acting in Language Models" by Shunyu Yao et al., 2022.

2. **HumanEval and MBPP Datasets:**
   - **Mark Chen et al., 2021; Jacob Austin et al., 2022:** Discusses benchmarks for evaluating code synthesis by language models.
   - **Citations:** "Evaluating Large Language Models Trained on Code" by Mark Chen et al., 2021 and "Program synthesis with large language models" by Jacob Austin et al., 2022.

3. **WebShop:**
   - **Shunyu Yao et al., 2022:** Evaluates LMs on interactive web-based environments, such as WebShop, to assess grounded language understanding and decision-making capabilities.
   - **Citation:** "WebShop: Towards scalable real-world web interaction with grounded language agents" by Shunyu Yao et al., 2022.

## **6. Conclusion**
Summarizes the contributions of LATS and its implications for the future of LM-based decision-making tasks.

### **Related Citations:**

1. **GPT-4 Development:**
   - **OpenAI, 2023:** Reports the development of GPT-4, a large-scale multimodal model designed to achieve human-level performance on various benchmarks.
   - **Citation:** "GPT-4 Technical Report" by OpenAI, 2023.

2. **HuggingGPT:**
   - **Yongliang Shen et al., 2023:** Discusses the integration of AI models using large language models to solve complex AI tasks across different domains and modalities.
   - **Citation:** "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face" by Yongliang Shen et al., 2023.

## **Impact Statement and Acknowledgements**
Discusses the potential impact, limitations, and future directions of LATS, along with acknowledgments of support and contributions.

### **Related Citations:**

1. **Toolformer:**
   - **Timo Schick et al., 2023:** Explores how language models can teach themselves to use external tools to improve performance on various tasks.
   - **Citation:** "Toolformer: Language Models Can Teach Themselves to Use Tools" by Timo Schick et al., 2023.

2. **Reflexion:**
   - **Noah Shinn et al., 2023:** Introduces Reflexion, a framework for enhancing language agents through verbal reinforcement learning.
   - **Citation:** "Reflexion: Language Agents with Verbal Reinforcement Learning" by Noah Shinn et al., 2023.

## **Appendix**
The appendix provides additional details on LATS, including pseudocode, further discussion on limitations, additional experimental results, and prompts used in various environments.

### **Related Citations:**

1. **ToolChain*:**
   - **Yuchen Zhuang et al., 2023:** Proposes ToolChain*, an efficient planning algorithm that leverages tree search for navigating expansive action spaces in language models.
   - **Citation:** "ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search" by Yuchen Zhuang et al., 2023.

2. **ALFWorld:**
   - **Mohit Shridhar et al., 2020:** Aligns text-based policies in TextWorld with visually grounded goals in ALFRED for better agent generalization.
   - **Citation:** "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning" by Mohit Shridhar et al., 2020.

Feel free to dive deep into each section and refer to the provided citations and definitions for a comprehensive understanding of the LATS framework and related work in improving decision-making and reasoning in language models."""